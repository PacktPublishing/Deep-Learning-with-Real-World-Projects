{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation using RNN - Character Level\n",
    "\n",
    "To generate text using RNN, we need a to convert raw text to a supervised learning problem format.\n",
    "\n",
    "Take, for example, the following corpus:\n",
    "\n",
    "\"Her brother shook his head incredulously\"\n",
    "\n",
    "First we need to divide the data into tabular format containing input (X) and output (y) sequences. In case of a character level model, the X and y will look like this:\n",
    "\n",
    "|      X     |  Y  |\n",
    "|------------|-----|\n",
    "|    Her b   |  r  |\n",
    "|    er br   |  o  |\n",
    "|    r bro   |  t  |\n",
    "|     brot   |  h  |\n",
    "|    broth   |  e  |\n",
    "|    .....   |  .  |\n",
    "|    .....   |  .  |\n",
    "|    ulous   |  l  |\n",
    "|    lousl   |  y  |\n",
    "\n",
    "Note that in the above problem, the sequence length of X is five characters and that of y is one character. Hence, this is a many-to-one architecture. We can, however, change the number of input characters to any number of characters depending on the type of problem.\n",
    "\n",
    "A model is trained on such data. To generate text, we simply give the model any five characters using which it predicts the next character. Then it appends the predicted character to the input sequence (on the extreme right of the sequence) and discards the first character (character on extreme left of the sequence). Then it predicts again using the new sequence and the cycle continues until a fix number of iterations. An example is shown below:\n",
    "\n",
    "Seed text: \"incre\"\n",
    "\n",
    "|      X                                            |  Y                       |\n",
    "|---------------------------------------------------|--------------------------|\n",
    "|                        incre                      |    < predicted char 1 >  |\n",
    "|               ncre < predicted char 1 >              |    < predicted char 2 >  |\n",
    "|       cre< predicted char 1 > < predicted char 2 >   |    < predicted char 3 >  |\n",
    "|       re< predicted char 1 >< predicted char 2 > < predicted char 3 >   |    < predicted char 4 >  |\n",
    "|                      ...                          |            ...           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "1. Preprocess data\n",
    "2. LSTM model\n",
    "3. Generate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4Sa28LJ6YqdD",
    "outputId": "ae782caf-f9d1-43fe-aa32-4f2ccf02cfe0"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build a C code generator by training an RNN on a huge corpus of C code (the linux kernel code). You can download the C code used as source text from the following link:\n",
    "https://github.com/torvalds/linux/tree/master/kernel\n",
    "\n",
    "We have already downloaded the entire kernel folder and stored in a local directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load C code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "hO1StR3rX72I",
    "outputId": "42079a21-0a71-4fba-c90a-b13e3acbb909"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'kernel/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-401e31f4068e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"kernel/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kernel/'"
     ]
    }
   ],
   "source": [
    "# set path where C files reside\n",
    "\n",
    "path = r\"kernel/\"\n",
    "\n",
    "os.chdir(path)\n",
    "\n",
    "file_names = os.listdir()\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "jg9HW8HwYlga",
    "outputId": "851307c1-b3f1-4fc6-c191-76962a66052b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tsacct.c', 'latencytop.c', 'iomem.c', 'kexec_core.c', 'async.c', 'utsname.c', 'rseq.c', 'module_signing.c', 'exec_domain.c', 'smpboot.c', 'bounds.c', 'sys.c', 'compat.c', 'audit_watch.c', 'resource.c', 'stop_machine.c', 'audit_tree.c', 'cpu.c', 'elfcore.c', 'delayacct.c', 'kcov.c', 'sysctl.c', 'ptrace.c', 'user.c', 'fork.c', 'backtracetest.c', 'user_namespace.c', 'kexec.c', 'module.c', 'extable.c', 'pid.c', 'padata.c', 'sysctl_binary.c', 'fail_function.c', 'exit.c', 'panic.c', 'test_kprobes.c', 'kexec_file.c', 'up.c', 'groups.c', 'watchdog_hld.c', 'auditsc.c', 'hung_task.c', 'crash_dump.c', 'ucount.c', 'acct.c', 'tracepoint.c', 'reboot.c', 'seccomp.c', 'kcmp.c', 'crash_core.c', 'range.c', 'watchdog.c', 'dma.c', 'softirq.c', 'stackleak.c', 'audit.c', 'notifier.c', 'user-return-notifier.c', 'jump_label.c', 'irq_work.c', 'freezer.c', 'utsname_sysctl.c', 'kthread.c', 'memremap.c', 'pid_namespace.c', 'ksysfs.c', 'stacktrace.c', 'profile.c', 'sys_ni.c', 'params.c', 'signal.c', 'workqueue.c', 'nsproxy.c', 'cpu_pm.c', 'audit_fsnotify.c', 'kallsyms.c', 'futex.c', 'kprobes.c', 'torture.c', 'uid16.c', 'auditfilter.c', 'capability.c', 'taskstats.c', 'umh.c', 'smp.c', 'context_tracking.c', 'relay.c', 'task_work.c', 'cred.c', 'kmod.c', 'configs.c']\n"
     ]
    }
   ],
   "source": [
    "# use regex to filter .c files\n",
    "import re\n",
    "c_names = \".*\\.c$\"\n",
    "\n",
    "c_files = list()\n",
    "\n",
    "for file in file_names:\n",
    "    if re.match(c_names, file):\n",
    "        c_files.append(file)\n",
    "\n",
    "print(c_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAbtuy5ZY87t"
   },
   "outputs": [],
   "source": [
    "# load all c code in a list\n",
    "full_code = list()\n",
    "for file in c_files:\n",
    "    code = open(file, \"r\", encoding='utf-8')\n",
    "    full_code.append(code.read())\n",
    "    code.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// SPDX-License-Identifier: GPL-2.0\n",
      "#define pr_fmt(fmt) \"kcov: \" fmt\n",
      "\n",
      "#define DISABLE_BRANCH_PROFILING\n",
      "#include <linux/atomic.h>\n",
      "#include <linux/compiler.h>\n",
      "#include <linux/errno.h>\n",
      "#include <linux/export.h>\n",
      "#include <linux/types.h>\n",
      "#include <linux/file.h>\n",
      "#include <linux/fs.h>\n",
      "#include <linux/init.h>\n",
      "#include <linux/mm.h>\n",
      "#include <linux/preempt.h>\n",
      "#include <linux/printk.h>\n",
      "#include <linux/sched.h>\n",
      "#include <linux/slab.h>\n",
      "#include <linux/spinlock.h>\n",
      "#include <linux/vmalloc.h>\n",
      "#include <linux/debugfs.h>\n",
      "#include <linux/uaccess.h>\n",
      "#include <linux/kcov.h>\n",
      "#include <asm/setup.h>\n",
      "\n",
      "/* Number of 64-bit words written per one comparison: */\n",
      "#define KCOV_WORDS_PER_CMP 4\n",
      "\n",
      "/*\n",
      " * kcov descriptor (one per opened debugfs file).\n",
      " * State transitions of the descriptor:\n",
      " *  - initial state after open()\n",
      " *  - then there must be a single ioctl(KCOV_INIT_TRACE) call\n",
      " *  - then, mmap() call (several calls are allowed but not useful)\n",
      " *  - then, ioctl(KCOV_ENABLE, arg), where arg is\n",
      " *\tKCOV_TRACE_PC - to trace only the PCs\n",
      " *\tor\n",
      " *\tKCOV_TRACE_CMP - to trace only the comparison operands\n",
      " *  - then, ioctl(KCOV_DISABLE) to disable the task.\n",
      " * Enabling/disabling ioctls can be repeated (only one task a time allowed).\n",
      " */\n",
      "struct kcov {\n",
      "\t/*\n",
      "\t * Reference counter. We keep one for:\n",
      "\t *  - opened file descriptor\n",
      "\t *  - task with enabled coverage (we can't unwire it from another task)\n",
      "\t */\n",
      "\tatomic_t\t\trefcount;\n",
      "\t/* The lock protects mode, size, area and t. */\n",
      "\tspinlock_t\t\tlock;\n",
      "\tenum kcov_mode\t\tmode;\n",
      "\t/* Size of arena (in long's for KCOV_MODE_TRACE). */\n",
      "\tunsigned\t\tsize;\n",
      "\t/* Coverage buffer shared with user space. */\n",
      "\tvoid\t\t\t*area;\n",
      "\t/* Task for which we collect coverage, or NULL. */\n",
      "\tstruct task_struct\t*t;\n",
      "};\n",
      "\n",
      "static notrace bool check_kcov_mode(enum kcov_mode needed_mode, struct task_struct *t)\n",
      "{\n",
      "\tunsigned int mode;\n",
      "\n",
      "\t/*\n",
      "\t * We are interested in code coverage as a function of a syscall inputs,\n",
      "\t * so we ignore code executed in interrupts.\n",
      "\t */\n",
      "\tif (!in_task())\n",
      "\t\treturn false;\n",
      "\tmode = READ_ONCE(t->kcov_mode);\n",
      "\t/*\n",
      "\t * There is some code that runs in interrupts but for which\n",
      "\t * in_interrupt() returns false (e.g. preempt_schedule_irq()).\n",
      "\t * READ_ONCE()/barrier() effectively provides load-acquire wrt\n",
      "\t * interrupts, there are paired barrier()/WRITE_ONCE() in\n",
      "\t * kcov_ioctl_locked().\n",
      "\t */\n",
      "\tbarrier();\n",
      "\treturn mode == needed_mode;\n",
      "}\n",
      "\n",
      "static notrace unsigned long canonicalize_ip(unsigned long ip)\n",
      "{\n",
      "#ifdef CONFIG_RANDOMIZE_BASE\n",
      "\tip -= kaslr_offset();\n",
      "#endif\n",
      "\treturn ip;\n",
      "}\n",
      "\n",
      "/*\n",
      " * Entry point from instrumented code.\n",
      " * This is called once per basic-block/edge.\n",
      " */\n",
      "void notrace __sanitizer_cov_trace_pc(void)\n",
      "{\n",
      "\tstruct task_struct *t;\n",
      "\tunsigned long *area;\n",
      "\tunsigned long ip = canonicalize_ip(_RET_IP_);\n",
      "\tunsigned long pos;\n",
      "\n",
      "\tt = current;\n",
      "\tif (!check_kcov_mode(KCOV_MODE_TRACE_PC, t))\n",
      "\t\treturn;\n",
      "\n",
      "\tarea = t->kcov_area;\n",
      "\t/* The first 64-bit word is the number of subsequent PCs. */\n",
      "\tpos = READ_ONCE(area[0]) + 1;\n",
      "\tif (likely(pos < t->kcov_size)) {\n",
      "\t\tarea[pos] = ip;\n",
      "\t\tWRITE_ONCE(area[0], pos);\n",
      "\t}\n",
      "}\n",
      "EXPORT_SYMBOL(__sanitizer_cov_trace_pc);\n",
      "\n",
      "#ifdef CONFIG_KCOV_ENABLE_COMPARISONS\n",
      "static void notrace write_comp_data(u64 type, u64 arg1, u64 arg2, u64 ip)\n",
      "{\n",
      "\tstruct task_struct *t;\n",
      "\tu64 *area;\n",
      "\tu64 count, start_index, end_pos, max_pos;\n",
      "\n",
      "\tt = current;\n",
      "\tif (!check_kcov_mode(KCOV_MODE_TRACE_CMP, t))\n",
      "\t\treturn;\n",
      "\n",
      "\tip = canonicalize_ip(ip);\n",
      "\n",
      "\t/*\n",
      "\t * We write all comparison arguments and types as u64.\n",
      "\t * The buffer was allocated for t->kcov_size unsigned longs.\n",
      "\t */\n",
      "\tarea = (u64 *)t->kcov_area;\n",
      "\tmax_pos = t->kcov_size * sizeof(unsigned long);\n",
      "\n",
      "\tcount = READ_ONCE(area[0]);\n",
      "\n",
      "\t/* Every record is KCOV_WORDS_PER_CMP 64-bit words. */\n",
      "\tstart_index = 1 + count * KCOV_WORDS_PER_CMP;\n",
      "\tend_pos = (start_index + KCOV_WORDS_PER_CMP) * sizeof(u64);\n",
      "\tif (likely(end_pos <= max_pos)) {\n",
      "\t\tarea[start_index] = type;\n",
      "\t\tarea[start_index + 1] = arg1;\n",
      "\t\tarea[start_index + 2] = arg2;\n",
      "\t\tarea[start_index + 3] = ip;\n",
      "\t\tWRITE_ONCE(area[0], count + 1);\n",
      "\t}\n",
      "}\n",
      "\n",
      "void notrace __sanitizer_cov_trace_cmp1(u8 arg1, u8 arg2)\n",
      "{\n",
      "\twrite_comp_data(KCOV_CMP_SIZE(0), arg1, arg2, _RET_IP_);\n",
      "}\n",
      "EXPORT_SYMBOL(__sanitizer_cov_trace_cmp1);\n",
      "\n",
      "void notrace __sanitizer_cov_trace_cmp2(u16 arg1, u16 arg2)\n",
      "{\n",
      "\twrite_comp_data(KCOV_CMP_SIZE(1), arg1, arg2, _RET_IP_);\n",
      "}\n",
      "EXPORT_SYMBOL(__sanitizer_cov_trace_cmp2);\n",
      "\n",
      "void notrace __sanitizer_cov_trace_cmp4(u32 arg1, u32 arg2)\n",
      "{\n",
      "\twrite_comp_data(KCOV_CMP_SIZE(2), arg1, arg2, _RET_IP_);\n",
      "}\n",
      "EXPORT_SYMBOL(__sanitizer_cov_trace_cmp4);\n",
      "\n",
      "void notrace __sanitizer_cov_trace_cmp8(u64 arg1, u64 arg2)\n",
      "{\n",
      "\twrite_comp_data(KCOV_CMP_SIZE(3), arg1, arg2, _RET_IP_);\n",
      "}\n",
      "EXPORT_SYMBOL(__sanitizer_cov_trace_cmp8);\n",
      "\n",
      "void notrace __sanitizer_cov_trace_const_cmp1(u8 arg1, u8 arg2)\n",
      "{\n",
      "\twrite_comp_data(KCOV_CMP_SIZE(0) | KCOV_CMP_CONST, arg1, arg2,\n",
      "\t\t\t_RET_IP_);\n",
      "}\n",
      "EXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp1);\n",
      "\n",
      "void notrace __sanitizer_cov_trace_const_cmp2(u16 arg1, u16 arg2)\n",
      "{\n",
      "\twrite_comp_data(KCOV_CMP_SIZE(1) | KCOV_CMP_CONST, arg1, arg2,\n",
      "\t\t\t_RET_IP_);\n",
      "}\n",
      "EXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp2);\n",
      "\n",
      "void notrace __sanitizer_cov_trace_const_cmp4(u32 arg1, u32 arg2)\n",
      "{\n",
      "\twrite_comp_data(KCOV_CMP_SIZE(2) | KCOV_CMP_CONST, arg1, arg2,\n",
      "\t\t\t_RET_IP_);\n",
      "}\n",
      "EXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp4);\n",
      "\n",
      "void notrace __sanitizer_cov_trace_const_cmp8(u64 arg1, u64 arg2)\n",
      "{\n",
      "\twrite_comp_data(KCOV_CMP_SIZE(3) | KCOV_CMP_CONST, arg1, arg2,\n",
      "\t\t\t_RET_IP_);\n",
      "}\n",
      "EXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp8);\n",
      "\n",
      "void notrace __sanitizer_cov_trace_switch(u64 val, u64 *cases)\n",
      "{\n",
      "\tu64 i;\n",
      "\tu64 count = cases[0];\n",
      "\tu64 size = cases[1];\n",
      "\tu64 type = KCOV_CMP_CONST;\n",
      "\n",
      "\tswitch (size) {\n",
      "\tcase 8:\n",
      "\t\ttype |= KCOV_CMP_SIZE(0);\n",
      "\t\tbreak;\n",
      "\tcase 16:\n",
      "\t\ttype |= KCOV_CMP_SIZE(1);\n",
      "\t\tbreak;\n",
      "\tcase 32:\n",
      "\t\ttype |= KCOV_CMP_SIZE(2);\n",
      "\t\tbreak;\n",
      "\tcase 64:\n",
      "\t\ttype |= KCOV_CMP_SIZE(3);\n",
      "\t\tbreak;\n",
      "\tdefault:\n",
      "\t\treturn;\n",
      "\t}\n",
      "\tfor (i = 0; i < count; i++)\n",
      "\t\twrite_comp_data(type, cases[i + 2], val, _RET_IP_);\n",
      "}\n",
      "EXPORT_SYMBOL(__sanitizer_cov_trace_switch);\n",
      "#endif /* ifdef CONFIG_KCOV_ENABLE_COMPARISONS */\n",
      "\n",
      "static void kcov_get(struct kcov *kcov)\n",
      "{\n",
      "\tatomic_inc(&kcov->refcount);\n",
      "}\n",
      "\n",
      "static void kcov_put(struct kcov *kcov)\n",
      "{\n",
      "\tif (atomic_dec_and_test(&kcov->refcount)) {\n",
      "\t\tvfree(kcov->area);\n",
      "\t\tkfree(kcov);\n",
      "\t}\n",
      "}\n",
      "\n",
      "void kcov_task_init(struct task_struct *t)\n",
      "{\n",
      "\tWRITE_ONCE(t->kcov_mode, KCOV_MODE_DISABLED);\n",
      "\tbarrier();\n",
      "\tt->kcov_size = 0;\n",
      "\tt->kcov_area = NULL;\n",
      "\tt->kcov = NULL;\n",
      "}\n",
      "\n",
      "void kcov_task_exit(struct task_struct *t)\n",
      "{\n",
      "\tstruct kcov *kcov;\n",
      "\n",
      "\tkcov = t->kcov;\n",
      "\tif (kcov == NULL)\n",
      "\t\treturn;\n",
      "\tspin_lock(&kcov->lock);\n",
      "\tif (WARN_ON(kcov->t != t)) {\n",
      "\t\tspin_unlock(&kcov->lock);\n",
      "\t\treturn;\n",
      "\t}\n",
      "\t/* Just to not leave dangling references behind. */\n",
      "\tkcov_task_init(t);\n",
      "\tkcov->t = NULL;\n",
      "\tkcov->mode = KCOV_MODE_INIT;\n",
      "\tspin_unlock(&kcov->lock);\n",
      "\tkcov_put(kcov);\n",
      "}\n",
      "\n",
      "static int kcov_mmap(struct file *filep, struct vm_area_struct *vma)\n",
      "{\n",
      "\tint res = 0;\n",
      "\tvoid *area;\n",
      "\tstruct kcov *kcov = vma->vm_file->private_data;\n",
      "\tunsigned long size, off;\n",
      "\tstruct page *page;\n",
      "\n",
      "\tarea = vmalloc_user(vma->vm_end - vma->vm_start);\n",
      "\tif (!area)\n",
      "\t\treturn -ENOMEM;\n",
      "\n",
      "\tspin_lock(&kcov->lock);\n",
      "\tsize = kcov->size * sizeof(unsigned long);\n",
      "\tif (kcov->mode != KCOV_MODE_INIT || vma->vm_pgoff != 0 ||\n",
      "\t    vma->vm_end - vma->vm_start != size) {\n",
      "\t\tres = -EINVAL;\n",
      "\t\tgoto exit;\n",
      "\t}\n",
      "\tif (!kcov->area) {\n",
      "\t\tkcov->area = area;\n",
      "\t\tvma->vm_flags |= VM_DONTEXPAND;\n",
      "\t\tspin_unlock(&kcov->lock);\n",
      "\t\tfor (off = 0; off < size; off += PAGE_SIZE) {\n",
      "\t\t\tpage = vmalloc_to_page(kcov->area + off);\n",
      "\t\t\tif (vm_insert_page(vma, vma->vm_start + off, page))\n",
      "\t\t\t\tWARN_ONCE(1, \"vm_insert_page() failed\");\n",
      "\t\t}\n",
      "\t\treturn 0;\n",
      "\t}\n",
      "exit:\n",
      "\tspin_unlock(&kcov->lock);\n",
      "\tvfree(area);\n",
      "\treturn res;\n",
      "}\n",
      "\n",
      "static int kcov_open(struct inode *inode, struct file *filep)\n",
      "{\n",
      "\tstruct kcov *kcov;\n",
      "\n",
      "\tkcov = kzalloc(sizeof(*kcov), GFP_KERNEL);\n",
      "\tif (!kcov)\n",
      "\t\treturn -ENOMEM;\n",
      "\tkcov->mode = KCOV_MODE_DISABLED;\n",
      "\tatomic_set(&kcov->refcount, 1);\n",
      "\tspin_lock_init(&kcov->lock);\n",
      "\tfilep->private_data = kcov;\n",
      "\treturn nonseekable_open(inode, filep);\n",
      "}\n",
      "\n",
      "static int kcov_close(struct inode *inode, struct file *filep)\n",
      "{\n",
      "\tkcov_put(filep->private_data);\n",
      "\treturn 0;\n",
      "}\n",
      "\n",
      "/*\n",
      " * Fault in a lazily-faulted vmalloc area before it can be used by\n",
      " * __santizer_cov_trace_pc(), to avoid recursion issues if any code on the\n",
      " * vmalloc fault handling path is instrumented.\n",
      " */\n",
      "static void kcov_fault_in_area(struct kcov *kcov)\n",
      "{\n",
      "\tunsigned long stride = PAGE_SIZE / sizeof(unsigned long);\n",
      "\tunsigned long *area = kcov->area;\n",
      "\tunsigned long offset;\n",
      "\n",
      "\tfor (offset = 0; offset < kcov->size; offset += stride)\n",
      "\t\tREAD_ONCE(area[offset]);\n",
      "}\n",
      "\n",
      "static int kcov_ioctl_locked(struct kcov *kcov, unsigned int cmd,\n",
      "\t\t\t     unsigned long arg)\n",
      "{\n",
      "\tstruct task_struct *t;\n",
      "\tunsigned long size, unused;\n",
      "\n",
      "\tswitch (cmd) {\n",
      "\tcase KCOV_INIT_TRACE:\n",
      "\t\t/*\n",
      "\t\t * Enable kcov in trace mode and setup buffer size.\n",
      "\t\t * Must happen before anything else.\n",
      "\t\t */\n",
      "\t\tif (kcov->mode != KCOV_MODE_DISABLED)\n",
      "\t\t\treturn -EBUSY;\n",
      "\t\t/*\n",
      "\t\t * Size must be at least 2 to hold current position and one PC.\n",
      "\t\t * Later we allocate size * sizeof(unsigned long) memory,\n",
      "\t\t * that must not overflow.\n",
      "\t\t */\n",
      "\t\tsize = arg;\n",
      "\t\tif (size < 2 || size > INT_MAX / sizeof(unsigned long))\n",
      "\t\t\treturn -EINVAL;\n",
      "\t\tkcov->size = size;\n",
      "\t\tkcov->mode = KCOV_MODE_INIT;\n",
      "\t\treturn 0;\n",
      "\tcase KCOV_ENABLE:\n",
      "\t\t/*\n",
      "\t\t * Enable coverage for the current task.\n",
      "\t\t * At this point user must have been enabled trace mode,\n",
      "\t\t * and mmapped the file. Coverage collection is disabled only\n",
      "\t\t * at task exit or voluntary by KCOV_DISABLE. After that it can\n",
      "\t\t * be enabled for another task.\n",
      "\t\t */\n",
      "\t\tif (kcov->mode != KCOV_MODE_INIT || !kcov->area)\n",
      "\t\t\treturn -EINVAL;\n",
      "\t\tt = current;\n",
      "\t\tif (kcov->t != NULL || t->kcov != NULL)\n",
      "\t\t\treturn -EBUSY;\n",
      "\t\tif (arg == KCOV_TRACE_PC)\n",
      "\t\t\tkcov->mode = KCOV_MODE_TRACE_PC;\n",
      "\t\telse if (arg == KCOV_TRACE_CMP)\n",
      "#ifdef CONFIG_KCOV_ENABLE_COMPARISONS\n",
      "\t\t\tkcov->mode = KCOV_MODE_TRACE_CMP;\n",
      "#else\n",
      "\t\treturn -ENOTSUPP;\n",
      "#endif\n",
      "\t\telse\n",
      "\t\t\treturn -EINVAL;\n",
      "\t\tkcov_fault_in_area(kcov);\n",
      "\t\t/* Cache in task struct for performance. */\n",
      "\t\tt->kcov_size = kcov->size;\n",
      "\t\tt->kcov_area = kcov->area;\n",
      "\t\t/* See comment in check_kcov_mode(). */\n",
      "\t\tbarrier();\n",
      "\t\tWRITE_ONCE(t->kcov_mode, kcov->mode);\n",
      "\t\tt->kcov = kcov;\n",
      "\t\tkcov->t = t;\n",
      "\t\t/* This is put either in kcov_task_exit() or in KCOV_DISABLE. */\n",
      "\t\tkcov_get(kcov);\n",
      "\t\treturn 0;\n",
      "\tcase KCOV_DISABLE:\n",
      "\t\t/* Disable coverage for the current task. */\n",
      "\t\tunused = arg;\n",
      "\t\tif (unused != 0 || current->kcov != kcov)\n",
      "\t\t\treturn -EINVAL;\n",
      "\t\tt = current;\n",
      "\t\tif (WARN_ON(kcov->t != t))\n",
      "\t\t\treturn -EINVAL;\n",
      "\t\tkcov_task_init(t);\n",
      "\t\tkcov->t = NULL;\n",
      "\t\tkcov->mode = KCOV_MODE_INIT;\n",
      "\t\tkcov_put(kcov);\n",
      "\t\treturn 0;\n",
      "\tdefault:\n",
      "\t\treturn -ENOTTY;\n",
      "\t}\n",
      "}\n",
      "\n",
      "static long kcov_ioctl(struct file *filep, unsigned int cmd, unsigned long arg)\n",
      "{\n",
      "\tstruct kcov *kcov;\n",
      "\tint res;\n",
      "\n",
      "\tkcov = filep->private_data;\n",
      "\tspin_lock(&kcov->lock);\n",
      "\tres = kcov_ioctl_locked(kcov, cmd, arg);\n",
      "\tspin_unlock(&kcov->lock);\n",
      "\treturn res;\n",
      "}\n",
      "\n",
      "static const struct file_operations kcov_fops = {\n",
      "\t.open\t\t= kcov_open,\n",
      "\t.unlocked_ioctl\t= kcov_ioctl,\n",
      "\t.compat_ioctl\t= kcov_ioctl,\n",
      "\t.mmap\t\t= kcov_mmap,\n",
      "\t.release        = kcov_close,\n",
      "};\n",
      "\n",
      "static int __init kcov_init(void)\n",
      "{\n",
      "\t/*\n",
      "\t * The kcov debugfs file won't ever get removed and thus,\n",
      "\t * there is no need to protect it against removal races. The\n",
      "\t * use of debugfs_create_file_unsafe() is actually safe here.\n",
      "\t */\n",
      "\tif (!debugfs_create_file_unsafe(\"kcov\", 0600, NULL, NULL, &kcov_fops)) {\n",
      "\t\tpr_err(\"failed to create kcov in debugfs\\n\");\n",
      "\t\treturn -ENOMEM;\n",
      "\t}\n",
      "\treturn 0;\n",
      "}\n",
      "\n",
      "device_initcall(kcov_init);\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at how a typical C code looks like\n",
    "print(full_code[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4PvwiZVwY__A",
    "outputId": "8363fd91-2706-4d2d-c18d-fa275ff631ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in entire code: 2065442\n"
     ]
    }
   ],
   "source": [
    "# merge different c codes into one big c code\n",
    "text = \"\\n\".join(full_code)\n",
    "print(\"Total number of characters in entire code: {}\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GxDf0tsBb6Pq"
   },
   "outputs": [],
   "source": [
    "# top_n: only consider first top_n characters and discard the rest for memory and computational efficiency\n",
    "top_n = 400000\n",
    "text = text[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/*\\n * tsacct.c - System accounting over taskstats interface\\n *\\n * Copyright (C) Jay Lan,\\t<jlan@sgi.com>\\n *\\n *\\n * This program is free software; you can redistribute it and/or modify\\n * it under the terms of the GNU General Public License as published by\\n * the Free Software Foundation; either version 2 of the License, or\\n * (at your option) any later version.\\n *\\n * This program is distributed in the hope that it will be useful,\\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n * GNU General Public License for more details.\\n *\\n */\\n\\n#include <linux/kernel.h>\\n#include <linux/sched/signal.h>\\n#include <linux/sched/mm.h>\\n#include <linux/sched/cputime.h>\\n#include <linux/tsacct_kern.h>\\n#include <linux/acct.h>\\n#include <linux/jiffies.h>\\n#include <linux/mm.h>\\n\\n/*\\n * fill in basic accounting fields\\n */\\nvoid bacct_add_tsk(struct user_namespace *user_ns,\\n\\t\\t   struct pid_namespace *pid_ns,\\n\\t\\t   struct taskstats *stats, struct task_struct *tsk)\\n{\\n\\tconst struct cred *tcred;\\n\\tu64 utime, stime, utimescaled, stimescaled;\\n\\tu64 delta;\\n\\n\\tBUILD_BUG_ON(TS_COMM_LEN < TASK_COMM_LEN);\\n\\n\\t/* calculate task elapsed time in nsec */\\n\\tdelta = ktime_get_ns() - tsk->start_time;\\n\\t/* Convert to micro seconds */\\n\\tdo_div(delta, NSEC_PER_USEC);\\n\\tstats->ac_etime = delta;\\n\\t/* Convert to seconds for btime */\\n\\tdo_div(delta, USEC_PER_SEC);\\n\\tstats->ac_btime = get_seconds() - delta;\\n\\tif (thread_group_leader(tsk)) {\\n\\t\\tstats->ac_exitcode = tsk->exit_code;\\n\\t\\tif (tsk->flags & PF_FORKNOEXEC)\\n\\t\\t\\tstats->ac_flag |= AFORK;\\n\\t}\\n\\tif (tsk->flags & PF_SUPERPRIV)\\n\\t\\tstats->ac_flag |= ASU;\\n\\tif (tsk->flags & PF_DUMPCORE)\\n\\t\\tstats->ac_flag |= ACORE;\\n\\tif (tsk->flags & PF_SIGNALED)\\n\\t\\tstats->ac_flag |= AXSIG;\\n\\tstats->ac_nice\\t = task_nice(tsk);\\n\\tstats->ac_sched\\t = tsk->policy;\\n\\tstats->ac_pid\\t = task_pid_nr_ns(tsk, pid_ns);\\n\\trcu_read_lock();\\n\\ttcred = __task_cred(tsk);\\n\\tstats->ac_uid\\t = from_kuid_munged(user_ns, tcred->uid);\\n\\tstats->ac_gid\\t = from_kgid_munged(user_ns, tcred->gid);\\n\\tstats->ac_ppid\\t = pid_alive(tsk) ?\\n\\t\\ttask_tgid_nr_ns(rcu_dereference(tsk->real_parent), pid_ns) : 0;\\n\\trcu_read_unlock();\\n\\n\\ttask_cputime(tsk, &utime, &stime);\\n\\tstats->ac_utime = div_u64(utime, NSEC_PER_USEC);\\n\\tstats->ac_stime = div_u64(stime, NSEC_PER_USEC);\\n\\n\\ttask_cputime_scaled(tsk, &utimescaled, &stimescaled);\\n\\tstats->ac_utimescaled = div_u64(utimescaled, NSEC_PER_USEC);\\n\\tstats->ac_stimescaled = div_u64(stimescaled, NSEC_PER_USEC);\\n\\n\\tstats->ac_minflt = tsk->min_flt;\\n\\tstats->ac_majflt = tsk->maj_flt;\\n\\n\\tstrncpy(stats->ac_comm, tsk->comm, sizeof(stats->ac_comm));\\n}\\n\\n\\n#ifdef CONFIG_TASK_XACCT\\n\\n#define KB 1024\\n#define MB (1024*KB)\\n#define KB_MASK (~(KB-1))\\n/*\\n * fill in extended accounting fields\\n */\\nvoid xacct_add_tsk(struct taskstats *stats, struct task_struct *p)\\n{\\n\\tstruct mm_struct *mm;\\n\\n\\t/* convert pages-nsec/1024 to Mbyte-usec, see __acct_update_integrals */\\n\\tstats->coremem = p->acct_rss_mem1 * PAGE_SIZE;\\n\\tdo_div(stats->coremem, 1000 * KB);\\n\\tstats->virtmem = p->acct_vm_mem1 * PAGE_SIZE;\\n\\tdo_div(stats->virtmem, 1000 * KB);\\n\\tmm = get_task_mm(p);\\n\\tif (mm) {\\n\\t\\t/* adjust to KB unit */\\n\\t\\tstats->hiwater_rss   = get_mm_hiwater_rss(mm) * PAGE_SIZE / KB;\\n\\t\\tstats->hiwater_vm    = get_mm_hiwater_vm(mm)  * PAGE_SIZE / KB;\\n\\t\\tmmput(mm);\\n\\t}\\n\\tstats->read_char\\t= p->ioac.rchar & KB_MASK;\\n\\tstats->write_char\\t= p->ioac.wchar & KB_MASK;\\n\\tstats->read_syscalls\\t= p->ioac.syscr & KB_MASK;\\n\\tstats->write_syscalls\\t= p->ioac.syscw & KB_MASK;\\n#ifdef CONFIG_TASK_IO_ACCOUNTING\\n\\tstats->read_bytes\\t= p->ioac.read_bytes & KB_MASK;\\n\\tstats->write_bytes\\t= p->ioac.write_bytes & KB_MASK;\\n\\tstats->cancelled_write_bytes = p->ioac.cancelled_write_bytes & KB_MASK;\\n#else\\n\\tstats->read_bytes\\t= 0;\\n\\tstats->write_bytes\\t= 0;\\n\\tstats->cancelled_write_bytes = 0;\\n#endif\\n}\\n#undef KB\\n#undef MB\\n\\nstatic void __acct_update_integrals(struct task_struct *tsk,\\n\\t\\t\\t\\t    u64 utime, u64 stime)\\n{\\n\\tu64 time, delta;\\n\\n\\tif (!likely(tsk->mm))\\n\\t\\treturn;\\n\\n\\ttime = stime + utime;\\n\\tdelta = time - tsk->acct_timexpd;\\n\\n\\tif (delta < TICK_NSEC)\\n\\t\\treturn;\\n\\n\\ttsk->acct_timexpd = time;\\n\\t/*\\n\\t * Divide by 1024 to avoid overflow, and to avoid division.\\n\\t * The final unit reported to userspace is Mbyte-usecs,\\n\\t * the rest of the math is done in xacct_add_tsk.\\n\\t */\\n\\ttsk->acct_rss_mem1 += delta * get_mm_rss(tsk->mm) >> 10;\\n\\ttsk->acct_vm_mem1 += delta * tsk->mm->total_vm >> 10;\\n}\\n\\n/**\\n * acct_update_integrals - update mm integral fields in task_struct\\n * @tsk: task_struct for accounting\\n */\\nvoid acct_update_integrals(struct task_struct *tsk)\\n{\\n\\tu64 utime, stime;\\n\\tunsigned long flags;\\n\\n\\tlocal_irq_save(flags);\\n\\ttask_cputime(tsk, &utime, &stime);\\n\\t__acct_update_integrals(tsk, utime, stime);\\n\\tlocal_irq_restore(flags);\\n}\\n\\n/**\\n * acct_account_cputime - update mm integral after cputime update\\n * @tsk: task_struct for accounting\\n */\\nvoid acct_account_cputime(struct task_struct *tsk)\\n{\\n\\t__acct_update_integrals(tsk, tsk->utime, tsk->stime);\\n}\\n\\n/**\\n * acct_clear_integrals - clear the mm integral fields in task_struct\\n * @tsk: task_struct whose accounting fields are cleared\\n */\\nvoid acct_clear_integrals(struct task_struct *tsk)\\n{\\n\\ttsk->acct_timexpd = 0;\\n\\ttsk->acct_rss_mem1 = 0;\\n\\ttsk->acct_vm_mem1 = 0;\\n}\\n#endif\\n\\n/*\\n * latencytop.c: Latency display infrastructure\\n *\\n * (C) Copyright 2008 Intel Corporation\\n * Author: Arjan van de Ven <arjan@linux.intel.com>\\n *\\n * This program is free software; you can redistribute it and/or\\n * modify it under the terms of the GNU General Public License\\n * as published by the Free Software Foundation; version 2\\n * of the License.\\n */\\n\\n/*\\n * CONFIG_LATENCYTOP enables a kernel latency tracking infrastructure that is\\n * used by the \"latencytop\" userspace tool. The latency that is tracked is not\\n * the \\'traditional\\' interrupt latency (which is primarily caused by something\\n * else consuming CPU), but instead, it is the latency an application encounters\\n * because the kernel sleeps on its behalf for various reasons.\\n *\\n * This code tracks 2 levels of statistics:\\n * 1) System level latency\\n * 2) Per process latency\\n *\\n * The latency is stored in fixed sized data structures in an accumulated form;\\n * if the \"same\" latency cause is hit twice, this will be tracked as one entry\\n * in the data structure. Both the count, total accumulated latency and maximum\\n * latency are tracked in this data structure. When the fixed size structure is\\n * full, no new causes are tracked until the buffer is flushed by writing to\\n * the /proc file; the userspace tool does this on a regular basis.\\n *\\n * A latency cause is identified by a stringified backtrace at the point that\\n * the scheduler gets invoked. The userland tool will use this string to\\n * identify the cause of the latency in human readable form.\\n *\\n * The information is exported via /proc/latency_stats and /proc/<pid>/latency.\\n * These files look like this:\\n *\\n * Latency Top version : v0.1\\n * 70 59433 4897 i915_irq_wait drm_ioctl vfs_ioctl do_vfs_ioctl sys_ioctl\\n * |    |    |    |\\n * |    |    |    +----> the stringified backtrace\\n * |    |    +---------> The maximum latency for this entry in microseconds\\n * |    +--------------> The accumulated latency for this entry (microseconds)\\n * +-------------------> The number of times this entry is hit\\n *\\n * (note: the average latency is the accumulated latency divided by the number\\n * of times)\\n */\\n\\n#include <linux/kallsyms.h>\\n#include <linux/seq_file.h>\\n#include <linux/notifier.h>\\n#include <linux/spinlock.h>\\n#include <linux/proc_fs.h>\\n#include <linux/latencytop.h>\\n#include <linux/export.h>\\n#include <linux/sched.h>\\n#include <linux/sched/debug.h>\\n#include <linux/sched/stat.h>\\n#include <linux/list.h>\\n#include <linux/stacktrace.h>\\n\\nstatic DEFINE_RAW_SPINLOCK(latency_lock);\\n\\n#define MAXLR 128\\nstatic struct latency_record latency_record[MAXLR];\\n\\nint latencytop_enabled;\\n\\nvoid clear_all_latency_tracing(struct task_struct *p)\\n{\\n\\tunsigned long flags;\\n\\n\\tif (!latencytop_enabled)\\n\\t\\treturn;\\n\\n\\traw_spin_lock_irqsave(&latency_lock, flags);\\n\\tmemset(&p->latency_record, 0, sizeof(p->latency_record));\\n\\tp->latency_record_count = 0;\\n\\traw_spin_unlock_irqrestore(&latency_lock, flags);\\n}\\n\\nstatic void clear_global_latency_tracing(void)\\n{\\n\\tunsigned long flags;\\n\\n\\traw_spin_lock_irqsave(&latency_lock, flags);\\n\\tmemset(&latency_record, 0, sizeof(latency_record));\\n\\traw_spin_unlock_irqrestore(&latency_lock, flags);\\n}\\n\\nstatic void __sched\\naccount_global_scheduler_latency(struct task_struct *tsk,\\n\\t\\t\\t\\t struct latency_record *lat)\\n{\\n\\tint firstnonnull = MAXLR + 1;\\n\\tint i;\\n\\n\\tif (!latencytop_enabled)\\n\\t\\treturn;\\n\\n\\t/* skip kernel threads for now */\\n\\tif (!tsk->mm)\\n\\t\\treturn;\\n\\n\\tfor (i = 0; i < MAXLR; i++) {\\n\\t\\tint q, same = 1;\\n\\n\\t\\t/* Nothing stored: */\\n\\t\\tif (!latency_record[i].backtrace[0]) {\\n\\t\\t\\tif (firstnonnull > i)\\n\\t\\t\\t\\tfirstnonnull = i;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\tfor (q = 0; q < LT_BACKTRACEDEPTH; q++) {\\n\\t\\t\\tunsigned long record = lat->backtrace[q];\\n\\n\\t\\t\\tif (latency_record[i].backtrace[q] != record) {\\n\\t\\t\\t\\tsame = 0;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\n\\t\\t\\t/* 0 and ULONG_MAX entries mean end of backtrace: */\\n\\t\\t\\tif (record == 0 || record == ULONG_MAX)\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tif (same) {\\n\\t\\t\\tlatency_record[i].count++;\\n\\t\\t\\tlatency_record[i].time += lat->time;\\n\\t\\t\\tif (lat->time > latency_record[i].max)\\n\\t\\t\\t\\tlatency_record[i].max = lat->time;\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\t}\\n\\n\\ti = firstnonnull;\\n\\tif (i >= MAXLR - 1)\\n\\t\\treturn;\\n\\n\\t/* Allocted a new one: */\\n\\tmemcpy(&latency_record[i], lat, sizeof(struct latency_record));\\n}\\n\\n/*\\n * Iterator to store a backtrace into a latency record entry\\n */\\nstatic inline void store_stacktrace(struct task_struct *tsk,\\n\\t\\t\\t\\t\\tstruct latency_record *lat)\\n{\\n\\tstruct stack_trace trace;\\n\\n\\tmemset(&trace, 0, sizeof(trace));\\n\\ttrace.max_entries = LT_BACKTRACEDEPTH;\\n\\ttrace.entries = &lat->backtrace[0];\\n\\tsave_stack_trace_tsk(tsk, &trace);\\n}\\n\\n/**\\n * __account_scheduler_latency - record an occurred latency\\n * @tsk - the task struct of the task hitting the latency\\n * @usecs - the duration of the latency in microseconds\\n * @inter - 1 if the sleep was interruptible, 0 if uninterruptible\\n *\\n * This function is the main entry point for recording latency entries\\n * as called by the scheduler.\\n *\\n * This function has a few special cases to deal with normal \\'non-latency\\'\\n * sleeps: specifically, interruptible sleep longer than 5 msec is skipped\\n * since this usually is caused by waiting for events via select() and co.\\n *\\n * Negative latencies (caused by time going backwards) are also explicitly\\n * skipped.\\n */\\nvoid __sched\\n__account_scheduler_latency(struct task_struct *tsk, int usecs, int inter)\\n{\\n\\tunsigned long flags;\\n\\tint i, q;\\n\\tstruct latency_record lat;\\n\\n\\t/* Long interruptible waits are generally user requested... */\\n\\tif (inter && usecs > 5000)\\n\\t\\treturn;\\n\\n\\t/* Negative sleeps are time going backwards */\\n\\t/* Zero-time sleeps are non-interesting */\\n\\tif (usecs <= 0)\\n\\t\\treturn;\\n\\n\\tmemset(&lat, 0, sizeof(lat));\\n\\tlat.count = 1;\\n\\tlat.time = usecs;\\n\\tlat.max = usecs;\\n\\tstore_stacktrace(tsk, &lat);\\n\\n\\traw_spin_lock_irqsave(&latency_lock, flags);\\n\\n\\taccount_global_scheduler_latency(tsk, &lat);\\n\\n\\tfor (i = 0; i < tsk->latency_record_count; i++) {\\n\\t\\tstruct latency_record *mylat;\\n\\t\\tint same = 1;\\n\\n\\t\\tmylat = &tsk->latency_record[i];\\n\\t\\tfor (q = 0; q < LT_BACKTRACEDEPTH; q++) {\\n\\t\\t\\tunsigned long record = lat.backtrace[q];\\n\\n\\t\\t\\tif (mylat->backtrace[q] != record) {\\n\\t\\t\\t\\tsame = 0;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\n\\t\\t\\t/* 0 and ULONG_MAX entries mean end of backtrace: */\\n\\t\\t\\tif (record == 0 || record == ULONG_MAX)\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tif (same) {\\n\\t\\t\\tmylat->count++;\\n\\t\\t\\tmylat->time += lat.time;\\n\\t\\t\\tif (lat.time > mylat->max)\\n\\t\\t\\t\\tmylat->max = lat.time;\\n\\t\\t\\tgoto out_unlock;\\n\\t\\t}\\n\\t}\\n\\n\\t/*\\n\\t * short term hack; if we\\'re > 32 we stop; future we recycle:\\n\\t */\\n\\tif (tsk->latency_record_count >= LT_SAVECOUNT)\\n\\t\\tgoto out_unlock;\\n\\n\\t/* Allocated a new one: */\\n\\ti = tsk->latency_record_count++;\\n\\tmemcpy(&tsk->latency_record[i], &lat, sizeof(struct latency_record));\\n\\nout_unlock:\\n\\traw_spin_unlock_irqrestore(&latency_lock, flags);\\n}\\n\\nstatic int lstats_show(struct seq_file *m, void *v)\\n{\\n\\tint i;\\n\\n\\tseq_puts(m, \"Latency Top version : v0.1\\\\n\");\\n\\n\\tfor (i = 0; i < MAXLR; i++) {\\n\\t\\tstruct latency_record *lr = &latency_record[i];\\n\\n\\t\\tif (lr->backtrace[0]) {\\n\\t\\t\\tint q;\\n\\t\\t\\tseq_printf(m, \"%i %lu %lu\",\\n\\t\\t\\t\\t   lr->count, lr->time, lr->max);\\n\\t\\t\\tfor (q = 0; q < LT_BACKTRACEDEPTH; q++) {\\n\\t\\t\\t\\tunsigned long bt = lr->backtrace[q];\\n\\t\\t\\t\\tif (!bt)\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\tif (bt == ULONG_MAX)\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\tseq_printf(m, \" %ps\", (void *)bt);\\n\\t\\t\\t}\\n\\t\\t\\tseq_puts(m, \"\\\\n\");\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic ssize_t\\nlstats_write(struct file *file, const char __user *buf, size_t count,\\n\\t     loff_t *offs)\\n{\\n\\tclear_global_latency_tracing();\\n\\n\\treturn count;\\n}\\n\\nstatic int lstats_open(struct inode *inode, struct file *filp)\\n{\\n\\treturn single_open(filp, lstats_show, NULL);\\n}\\n\\nstatic const struct file_operations lstats_fops = {\\n\\t.open\\t\\t= lstats_open,\\n\\t.read\\t\\t= seq_read,\\n\\t.write\\t\\t= lstats_write,\\n\\t.llseek\\t\\t= seq_lseek,\\n\\t.release\\t= single_release,\\n};\\n\\nstatic int __init init_lstats_procfs(void)\\n{\\n\\tproc_create(\"latency_stats\", 0644, NULL, &lstats_fops);\\n\\treturn 0;\\n}\\n\\nint sysctl_latencytop(struct ctl_table *table, int write,\\n\\t\\t\\tvoid __user *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tint err;\\n\\n\\terr = proc_dointvec(table, write, buffer, lenp, ppos);\\n\\tif (latencytop_enabled)\\n\\t\\tforce_schedstat_enabled();\\n\\n\\treturn err;\\n}\\ndevice_initcall(init_lstats_procfs);\\n\\n/* SPDX-License-Identifier: GPL-2.0 */\\n#include <linux/device.h>\\n#include <linux/types.h>\\n#include <linux/io.h>\\n#include <linux/mm.h>\\n\\n#ifndef ioremap_cache\\n/* temporary while we convert existing ioremap_cache users to memremap */\\n__weak void __iomem *ioremap_cache(resource_size_t offset, unsigned long size)\\n{\\n\\treturn ioremap(offset, size);\\n}\\n#endif\\n\\n#ifndef arch_memremap_wb\\nstatic void *arch_memremap_wb(resource_size_t offset, unsigned long size)\\n{\\n\\treturn (__force void *)ioremap_cache(offset, size);\\n}\\n#endif\\n\\n#ifndef arch_memremap_can_ram_remap\\nstatic bool arch_memremap_can_ram_remap(resource_size_t offset, size_t size,\\n\\t\\t\\t\\t\\tunsigned long flags)\\n{\\n\\treturn true;\\n}\\n#endif\\n\\nstatic void *try_ram_remap(resource_size_t offset, size_t size,\\n\\t\\t\\t   unsigned long flags)\\n{\\n\\tunsigned long pfn = PHYS_PFN(offset);\\n\\n\\t/* In the simple case just return the existing linear address */\\n\\tif (pfn_valid(pfn) && !PageHighMem(pfn_to_page(pfn)) &&\\n\\t    arch_memremap_can_ram_remap(offset, size, flags))\\n\\t\\treturn __va(offset);\\n\\n\\treturn NULL; /* fallback to arch_memremap_wb */\\n}\\n\\n/**\\n * memremap() - remap an iomem_resource as cacheable memory\\n * @offset: iomem resource start address\\n * @size: size of remap\\n * @flags: any of MEMREMAP_WB, MEMREMAP_WT, MEMREMAP_WC,\\n *\\t\\t  MEMREMAP_ENC, MEMREMAP_DEC\\n *\\n * memremap() is \"ioremap\" for cases where it is known that the resource\\n * being mapped does not have i/o side effects and the __iomem\\n * annotation is not applicable. In the case of multiple flags, the different\\n * mapping types will be attempted in the order listed below until one of\\n * them succeeds.\\n *\\n * MEMREMAP_WB - matches the default mapping for System RAM on\\n * the architecture.  This is usually a read-allocate write-back cache.\\n * Morever, if MEMREMAP_WB is specified and the requested remap region is RAM\\n * memremap() will bypass establishing a new mapping and instead return\\n * a pointer into the direct map.\\n *\\n * MEMREMAP_WT - establish a mapping whereby writes either bypass the\\n * cache or are written through to memory and never exist in a\\n * cache-dirty state with respect to program visibility.  Attempts to\\n * map System RAM with this mapping type will fail.\\n *\\n * MEMREMAP_WC - establish a writecombine mapping, whereby writes may\\n * be coalesced together (e.g. in the CPU\\'s write buffers), but is otherwise\\n * uncached. Attempts to map System RAM with this mapping type will fail.\\n */\\nvoid *memremap(resource_size_t offset, size_t size, unsigned long flags)\\n{\\n\\tint is_ram = region_intersects(offset, size,\\n\\t\\t\\t\\t       IORESOURCE_SYSTEM_RAM, IORES_DESC_NONE);\\n\\tvoid *addr = NULL;\\n\\n\\tif (!flags)\\n\\t\\treturn NULL;\\n\\n\\tif (is_ram == REGION_MIXED) {\\n\\t\\tWARN_ONCE(1, \"memremap attempted on mixed range %pa size: %#lx\\\\n\",\\n\\t\\t\\t\\t&offset, (unsigned long) size);\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\t/* Try all mapping types requested until one returns non-NULL */\\n\\tif (flags & MEMREMAP_WB) {\\n\\t\\t/*\\n\\t\\t * MEMREMAP_WB is special in that it can be satisifed\\n\\t\\t * from the direct map.  Some archs depend on the\\n\\t\\t * capability of memremap() to autodetect cases where\\n\\t\\t * the requested range is potentially in System RAM.\\n\\t\\t */\\n\\t\\tif (is_ram == REGION_INTERSECTS)\\n\\t\\t\\taddr = try_ram_remap(offset, size, flags);\\n\\t\\tif (!addr)\\n\\t\\t\\taddr = arch_memremap_wb(offset, size);\\n\\t}\\n\\n\\t/*\\n\\t * If we don\\'t have a mapping yet and other request flags are\\n\\t * present then we will be attempting to establish a new virtual\\n\\t * address mapping.  Enforce that this mapping is not aliasing\\n\\t * System RAM.\\n\\t */\\n\\tif (!addr && is_ram == REGION_INTERSECTS && flags != MEMREMAP_WB) {\\n\\t\\tWARN_ONCE(1, \"memremap attempted on ram %pa size: %#lx\\\\n\",\\n\\t\\t\\t\\t&offset, (unsigned long) size);\\n\\t\\treturn NULL;\\n\\t}\\n\\n\\tif (!addr && (flags & MEMREMAP_WT))\\n\\t\\taddr = ioremap_wt(offset, size);\\n\\n\\tif (!addr && (flags & MEMREMAP_WC))\\n\\t\\taddr = ioremap_wc(offset, size);\\n\\n\\treturn addr;\\n}\\nEXPORT_SYMBOL(memremap);\\n\\nvoid memunmap(void *addr)\\n{\\n\\tif (is_vmalloc_addr(addr))\\n\\t\\tiounmap((void __iomem *) addr);\\n}\\nEXPORT_SYMBOL(memunmap);\\n\\nstatic void devm_memremap_release(struct device *dev, void *res)\\n{\\n\\tmemunmap(*(void **)res);\\n}\\n\\nstatic int devm_memremap_match(struct device *dev, void *res, void *match_data)\\n{\\n\\treturn *(void **)res == match_data;\\n}\\n\\nvoid *devm_memremap(struct device *dev, resource_size_t offset,\\n\\t\\tsize_t size, unsigned long flags)\\n{\\n\\tvoid **ptr, *addr;\\n\\n\\tptr = devres_alloc_node(devm_memremap_release, sizeof(*ptr), GFP_KERNEL,\\n\\t\\t\\tdev_to_node(dev));\\n\\tif (!ptr)\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\taddr = memremap(offset, size, flags);\\n\\tif (addr) {\\n\\t\\t*ptr = addr;\\n\\t\\tdevres_add(dev, ptr);\\n\\t} else {\\n\\t\\tdevres_free(ptr);\\n\\t\\treturn ERR_PTR(-ENXIO);\\n\\t}\\n\\n\\treturn addr;\\n}\\nEXPORT_SYMBOL(devm_memremap);\\n\\nvoid devm_memunmap(struct device *dev, void *addr)\\n{\\n\\tWARN_ON(devres_release(dev, devm_memremap_release,\\n\\t\\t\\t\\tdevm_memremap_match, addr));\\n}\\nEXPORT_SYMBOL(devm_memunmap);\\n\\n/*\\n * kexec.c - kexec system call core code.\\n * Copyright (C) 2002-2004 Eric Biederman  <ebiederm@xmission.com>\\n *\\n * This source code is licensed under the GNU General Public License,\\n * Version 2.  See the file COPYING for more details.\\n */\\n\\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\\n\\n#include <linux/capability.h>\\n#include <linux/mm.h>\\n#include <linux/file.h>\\n#include <linux/slab.h>\\n#include <linux/fs.h>\\n#include <linux/kexec.h>\\n#include <linux/mutex.h>\\n#include <linux/list.h>\\n#include <linux/highmem.h>\\n#include <linux/syscalls.h>\\n#include <linux/reboot.h>\\n#include <linux/ioport.h>\\n#include <linux/hardirq.h>\\n#include <linux/elf.h>\\n#include <linux/elfcore.h>\\n#include <linux/utsname.h>\\n#include <linux/numa.h>\\n#include <linux/suspend.h>\\n#include <linux/device.h>\\n#include <linux/freezer.h>\\n#include <linux/pm.h>\\n#include <linux/cpu.h>\\n#include <linux/uaccess.h>\\n#include <linux/io.h>\\n#include <linux/console.h>\\n#include <linux/vmalloc.h>\\n#include <linux/swap.h>\\n#include <linux/syscore_ops.h>\\n#include <linux/compiler.h>\\n#include <linux/hugetlb.h>\\n#include <linux/frame.h>\\n\\n#include <asm/page.h>\\n#include <asm/sections.h>\\n\\n#include <crypto/hash.h>\\n#include <crypto/sha.h>\\n#include \"kexec_internal.h\"\\n\\nDEFINE_MUTEX(kexec_mutex);\\n\\n/* Per cpu memory for storing cpu states in case of system crash. */\\nnote_buf_t __percpu *crash_notes;\\n\\n/* Flag to indicate we are going to kexec a new kernel */\\nbool kexec_in_progress = false;\\n\\n\\n/* Location of the reserved area for the crash kernel */\\nstruct resource crashk_res = {\\n\\t.name  = \"Crash kernel\",\\n\\t.start = 0,\\n\\t.end   = 0,\\n\\t.flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM,\\n\\t.desc  = IORES_DESC_CRASH_KERNEL\\n};\\nstruct resource crashk_low_res = {\\n\\t.name  = \"Crash kernel\",\\n\\t.start = 0,\\n\\t.end   = 0,\\n\\t.flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM,\\n\\t.desc  = IORES_DESC_CRASH_KERNEL\\n};\\n\\nint kexec_should_crash(struct task_struct *p)\\n{\\n\\t/*\\n\\t * If crash_kexec_post_notifiers is enabled, don\\'t run\\n\\t * crash_kexec() here yet, which must be run after panic\\n\\t * notifiers in panic().\\n\\t */\\n\\tif (crash_kexec_post_notifiers)\\n\\t\\treturn 0;\\n\\t/*\\n\\t * There are 4 panic() calls in do_exit() path, each of which\\n\\t * corresponds to each of these 4 conditions.\\n\\t */\\n\\tif (in_interrupt() || !p->pid || is_global_init(p) || panic_on_oops)\\n\\t\\treturn 1;\\n\\treturn 0;\\n}\\n\\nint kexec_crash_loaded(void)\\n{\\n\\treturn !!kexec_crash_image;\\n}\\nEXPORT_SYMBOL_GPL(kexec_crash_loaded);\\n\\n/*\\n * When kexec transitions to the new kernel there is a one-to-one\\n * mapping between physical and virtual addresses.  On processors\\n * where you can disable the MMU this is trivial, and easy.  For\\n * others it is still a simple predictable page table to setup.\\n *\\n * In that environment kexec copies the new kernel to its final\\n * resting place.  This means I can only support memory whose\\n * physical address can fit in an unsigned long.  In particular\\n * addresses where (pfn << PAGE_SHIFT) > ULONG_MAX cannot be handled.\\n * If the assembly stub has more restrictive requirements\\n * KEXEC_SOURCE_MEMORY_LIMIT and KEXEC_DEST_MEMORY_LIMIT can be\\n * defined more restrictively in <asm/kexec.h>.\\n *\\n * The code for the transition from the current kernel to the\\n * the new kernel is placed in the control_code_buffer, whose size\\n * is given by KEXEC_CONTROL_PAGE_SIZE.  In the best case only a single\\n * page of memory is necessary, but some architectures require more.\\n * Because this memory must be identity mapped in the transition from\\n * virtual to physical addresses it must live in the range\\n * 0 - TASK_SIZE, as only the user space mappings are arbitrarily\\n * modifiable.\\n *\\n * The assembly stub in the control code buffer is passed a linked list\\n * of descriptor pages detailing the source pages of the new kernel,\\n * and the destination addresses of those source pages.  As this data\\n * structure is not used in the context of the current OS, it must\\n * be self-contained.\\n *\\n * The code has been made to work with highmem pages and will use a\\n * destination page in its final resting place (if it happens\\n * to allocate it).  The end product of this is that most of the\\n * physical address space, and most of RAM can be used.\\n *\\n * Future directions include:\\n *  - allocating a page table with the control code buffer identity\\n *    mapped, to simplify machine_kexec and make kexec_on_panic more\\n *    reliable.\\n */\\n\\n/*\\n * KIMAGE_NO_DEST is an impossible destination address..., for\\n * allocating pages whose destination address we do not care about.\\n */\\n#define KIMAGE_NO_DEST (-1UL)\\n#define PAGE_COUNT(x) (((x) + PAGE_SIZE - 1) >> PAGE_SHIFT)\\n\\nstatic struct page *kimage_alloc_page(struct kimage *image,\\n\\t\\t\\t\\t       gfp_t gfp_mask,\\n\\t\\t\\t\\t       unsigned long dest);\\n\\nint sanity_check_segment_list(struct kimage *image)\\n{\\n\\tint i;\\n\\tunsigned long nr_segments = image->nr_segments;\\n\\tunsigned long total_pages = 0;\\n\\tunsigned long nr_pages = totalram_pages();\\n\\n\\t/*\\n\\t * Verify we have good destination addresses.  The caller is\\n\\t * responsible for making certain we don\\'t attempt to load\\n\\t * the new image into invalid or reserved areas of RAM.  This\\n\\t * just verifies it is an address we can use.\\n\\t *\\n\\t * Since the kernel does everything in page size chunks ensure\\n\\t * the destination addresses are page aligned.  Too many\\n\\t * special cases crop of when we don\\'t do this.  The most\\n\\t * insidious is getting overlapping destination addresses\\n\\t * simply because addresses are changed to page size\\n\\t * granularity.\\n\\t */\\n\\tfor (i = 0; i < nr_segments; i++) {\\n\\t\\tunsigned long mstart, mend;\\n\\n\\t\\tmstart = image->segment[i].mem;\\n\\t\\tmend   = mstart + image->segment[i].memsz;\\n\\t\\tif (mstart > mend)\\n\\t\\t\\treturn -EADDRNOTAVAIL;\\n\\t\\tif ((mstart & ~PAGE_MASK) || (mend & ~PAGE_MASK))\\n\\t\\t\\treturn -EADDRNOTAVAIL;\\n\\t\\tif (mend >= KEXEC_DESTINATION_MEMORY_LIMIT)\\n\\t\\t\\treturn -EADDRNOTAVAIL;\\n\\t}\\n\\n\\t/* Verify our destination addresses do not overlap.\\n\\t * If we alloed overlapping destination addresses\\n\\t * through very weird things can happen with no\\n\\t * easy explanation as one segment stops on another.\\n\\t */\\n\\tfor (i = 0; i < nr_segments; i++) {\\n\\t\\tunsigned long mstart, mend;\\n\\t\\tunsigned long j;\\n\\n\\t\\tmstart = image->segment[i].mem;\\n\\t\\tmend   = mstart + image->segment[i].memsz;\\n\\t\\tfor (j = 0; j < i; j++) {\\n\\t\\t\\tunsigned long pstart, pend;\\n\\n\\t\\t\\tpstart = image->segment[j].mem;\\n\\t\\t\\tpend   = pstart + image->segment[j].memsz;\\n\\t\\t\\t/* Do the segments overlap ? */\\n\\t\\t\\tif ((mend > pstart) && (mstart < pend))\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t}\\n\\n\\t/* Ensure our buffer sizes are strictly less than\\n\\t * our memory sizes.  This should always be the case,\\n\\t * and it is easier to check up front than to be surprised\\n\\t * later on.\\n\\t */\\n\\tfor (i = 0; i < nr_segments; i++) {\\n\\t\\tif (image->segment[i].bufsz > image->segment[i].memsz)\\n\\t\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\t/*\\n\\t * Verify that no more than half of memory will be consumed. If the\\n\\t * request from userspace is too large, a large amount of time will be\\n\\t * wasted allocating pages, which can cause a soft lockup.\\n\\t */\\n\\tfor (i = 0; i < nr_segments; i++) {\\n\\t\\tif (PAGE_COUNT(image->segment[i].memsz) > nr_pages / 2)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\ttotal_pages += PAGE_COUNT(image->segment[i].memsz);\\n\\t}\\n\\n\\tif (total_pages > nr_pages / 2)\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * Verify we have good destination addresses.  Normally\\n\\t * the caller is responsible for making certain we don\\'t\\n\\t * attempt to load the new image into invalid or reserved\\n\\t * areas of RAM.  But crash kernels are preloaded into a\\n\\t * reserved area of ram.  We must ensure the addresses\\n\\t * are in the reserved area otherwise preloading the\\n\\t * kernel could corrupt things.\\n\\t */\\n\\n\\tif (image->type == KEXEC_TYPE_CRASH) {\\n\\t\\tfor (i = 0; i < nr_segments; i++) {\\n\\t\\t\\tunsigned long mstart, mend;\\n\\n\\t\\t\\tmstart = image->segment[i].mem;\\n\\t\\t\\tmend = mstart + image->segment[i].memsz - 1;\\n\\t\\t\\t/* Ensure we are within the crash kernel limits */\\n\\t\\t\\tif ((mstart < phys_to_boot_phys(crashk_res.start)) ||\\n\\t\\t\\t    (mend > phys_to_boot_phys(crashk_res.end)))\\n\\t\\t\\t\\treturn -EADDRNOTAVAIL;\\n\\t\\t}\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstruct kimage *do_kimage_alloc_init(void)\\n{\\n\\tstruct kimage *image;\\n\\n\\t/* Allocate a controlling structure */\\n\\timage = kzalloc(sizeof(*image), GFP_KERNEL);\\n\\tif (!image)\\n\\t\\treturn NULL;\\n\\n\\timage->head = 0;\\n\\timage->entry = &image->head;\\n\\timage->last_entry = &image->head;\\n\\timage->control_page = ~0; /* By default this does not apply */\\n\\timage->type = KEXEC_TYPE_DEFAULT;\\n\\n\\t/* Initialize the list of control pages */\\n\\tINIT_LIST_HEAD(&image->control_pages);\\n\\n\\t/* Initialize the list of destination pages */\\n\\tINIT_LIST_HEAD(&image->dest_pages);\\n\\n\\t/* Initialize the list of unusable pages */\\n\\tINIT_LIST_HEAD(&image->unusable_pages);\\n\\n\\treturn image;\\n}\\n\\nint kimage_is_destination_range(struct kimage *image,\\n\\t\\t\\t\\t\\tunsigned long start,\\n\\t\\t\\t\\t\\tunsigned long end)\\n{\\n\\tunsigned long i;\\n\\n\\tfor (i = 0; i < image->nr_segments; i++) {\\n\\t\\tunsigned long mstart, mend;\\n\\n\\t\\tmstart = image->segment[i].mem;\\n\\t\\tmend = mstart + image->segment[i].memsz;\\n\\t\\tif ((end > mstart) && (start < mend))\\n\\t\\t\\treturn 1;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nstatic struct page *kimage_alloc_pages(gfp_t gfp_mask, unsigned int order)\\n{\\n\\tstruct page *pages;\\n\\n\\tpages = alloc_pages(gfp_mask & ~__GFP_ZERO, order);\\n\\tif (pages) {\\n\\t\\tunsigned int count, i;\\n\\n\\t\\tpages->mapping = NULL;\\n\\t\\tset_page_private(pages, order);\\n\\t\\tcount = 1 << order;\\n\\t\\tfor (i = 0; i < count; i++)\\n\\t\\t\\tSetPageReserved(pages + i);\\n\\n\\t\\tarch_kexec_post_alloc_pages(page_address(pages), count,\\n\\t\\t\\t\\t\\t    gfp_mask);\\n\\n\\t\\tif (gfp_mask & __GFP_ZERO)\\n\\t\\t\\tfor (i = 0; i < count; i++)\\n\\t\\t\\t\\tclear_highpage(pages + i);\\n\\t}\\n\\n\\treturn pages;\\n}\\n\\nstatic void kimage_free_pages(struct page *page)\\n{\\n\\tunsigned int order, count, i;\\n\\n\\torder = page_private(page);\\n\\tcount = 1 << order;\\n\\n\\tarch_kexec_pre_free_pages(page_address(page), count);\\n\\n\\tfor (i = 0; i < count; i++)\\n\\t\\tClearPageReserved(page + i);\\n\\t__free_pages(page, order);\\n}\\n\\nvoid kimage_free_page_list(struct list_head *list)\\n{\\n\\tstruct page *page, *next;\\n\\n\\tlist_for_each_entry_safe(page, next, list, lru) {\\n\\t\\tlist_del(&page->lru);\\n\\t\\tkimage_free_pages(page);\\n\\t}\\n}\\n\\nstatic struct page *kimage_alloc_normal_control_pages(struct kimage *image,\\n\\t\\t\\t\\t\\t\\t\\tunsigned int order)\\n{\\n\\t/* Control pages are special, they are the intermediaries\\n\\t * that are needed while we copy the rest of the pages\\n\\t * to their final resting place.  As such they must\\n\\t * not conflict with either the destination addresses\\n\\t * or memory the kernel is already using.\\n\\t *\\n\\t * The only case where we really need more than one of\\n\\t * these are for architectures where we cannot disable\\n\\t * the MMU and must instead generate an identity mapped\\n\\t * page table for all of the memory.\\n\\t *\\n\\t * At worst this runs in O(N) of the image size.\\n\\t */\\n\\tstruct list_head extra_pages;\\n\\tstruct page *pages;\\n\\tunsigned int count;\\n\\n\\tcount = 1 << order;\\n\\tINIT_LIST_HEAD(&extra_pages);\\n\\n\\t/* Loop while I can allocate a page and the page allocated\\n\\t * is a destination page.\\n\\t */\\n\\tdo {\\n\\t\\tunsigned long pfn, epfn, addr, eaddr;\\n\\n\\t\\tpages = kimage_alloc_pages(KEXEC_CONTROL_MEMORY_GFP, order);\\n\\t\\tif (!pages)\\n\\t\\t\\tbreak;\\n\\t\\tpfn   = page_to_boot_pfn(pages);\\n\\t\\tepfn  = pfn + count;\\n\\t\\taddr  = pfn << PAGE_SHIFT;\\n\\t\\teaddr = epfn << PAGE_SHIFT;\\n\\t\\tif ((epfn >= (KEXEC_CONTROL_MEMORY_LIMIT >> PAGE_SHIFT)) ||\\n\\t\\t\\t      kimage_is_destination_range(image, addr, eaddr)) {\\n\\t\\t\\tlist_add(&pages->lru, &extra_pages);\\n\\t\\t\\tpages = NULL;\\n\\t\\t}\\n\\t} while (!pages);\\n\\n\\tif (pages) {\\n\\t\\t/* Remember the allocated page... */\\n\\t\\tlist_add(&pages->lru, &image->control_pages);\\n\\n\\t\\t/* Because the page is already in it\\'s destination\\n\\t\\t * location we will never allocate another page at\\n\\t\\t * that address.  Therefore kimage_alloc_pages\\n\\t\\t * will not return it (again) and we don\\'t need\\n\\t\\t * to give it an entry in image->segment[].\\n\\t\\t */\\n\\t}\\n\\t/* Deal with the destination pages I have inadvertently allocated.\\n\\t *\\n\\t * Ideally I would convert multi-page allocations into single\\n\\t * page allocations, and add everything to image->dest_pages.\\n\\t *\\n\\t * For now it is simpler to just free the pages.\\n\\t */\\n\\tkimage_free_page_list(&extra_pages);\\n\\n\\treturn pages;\\n}\\n\\nstatic struct page *kimage_alloc_crash_control_pages(struct kimage *image,\\n\\t\\t\\t\\t\\t\\t      unsigned int order)\\n{\\n\\t/* Control pages are special, they are the intermediaries\\n\\t * that are needed while we copy the rest of the pages\\n\\t * to their final resting place.  As such they must\\n\\t * not conflict with either the destination addresses\\n\\t * or memory the kernel is already using.\\n\\t *\\n\\t * Control pages are also the only pags we must allocate\\n\\t * when loading a crash kernel.  All of the other pages\\n\\t * are specified by the segments and we just memcpy\\n\\t * into them directly.\\n\\t *\\n\\t * The only case where we really need more than one of\\n\\t * these are for architectures where we cannot disable\\n\\t * the MMU and must instead generate an identity mapped\\n\\t * page table for all of the memory.\\n\\t *\\n\\t * Given the low demand this implements a very simple\\n\\t * allocator that finds the first hole of the appropriate\\n\\t * size in the reserved memory region, and allocates all\\n\\t * of the memory up to and including the hole.\\n\\t */\\n\\tunsigned long hole_start, hole_end, size;\\n\\tstruct page *pages;\\n\\n\\tpages = NULL;\\n\\tsize = (1 << order) << PAGE_SHIFT;\\n\\thole_start = (image->control_page + (size - 1)) & ~(size - 1);\\n\\thole_end   = hole_start + size - 1;\\n\\twhile (hole_end <= crashk_res.end) {\\n\\t\\tunsigned long i;\\n\\n\\t\\tcond_resched();\\n\\n\\t\\tif (hole_end > KEXEC_CRASH_CONTROL_MEMORY_LIMIT)\\n\\t\\t\\tbreak;\\n\\t\\t/* See if I overlap any of the segments */\\n\\t\\tfor (i = 0; i < image->nr_segments; i++) {\\n\\t\\t\\tunsigned long mstart, mend;\\n\\n\\t\\t\\tmstart = image->segment[i].mem;\\n\\t\\t\\tmend   = mstart + image->segment[i].memsz - 1;\\n\\t\\t\\tif ((hole_end >= mstart) && (hole_start <= mend)) {\\n\\t\\t\\t\\t/* Advance the hole to the end of the segment */\\n\\t\\t\\t\\thole_start = (mend + (size - 1)) & ~(size - 1);\\n\\t\\t\\t\\thole_end   = hole_start + size - 1;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\t/* If I don\\'t overlap any segments I have found my hole! */\\n\\t\\tif (i == image->nr_segments) {\\n\\t\\t\\tpages = pfn_to_page(hole_start >> PAGE_SHIFT);\\n\\t\\t\\timage->control_page = hole_end;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\t/* Ensure that these pages are decrypted if SME is enabled. */\\n\\tif (pages)\\n\\t\\tarch_kexec_post_alloc_pages(page_address(pages), 1 << order, 0);\\n\\n\\treturn pages;\\n}\\n\\n\\nstruct page *kimage_alloc_control_pages(struct kimage *image,\\n\\t\\t\\t\\t\\t unsigned int order)\\n{\\n\\tstruct page *pages = NULL;\\n\\n\\tswitch (image->type) {\\n\\tcase KEXEC_TYPE_DEFAULT:\\n\\t\\tpages = kimage_alloc_normal_control_pages(image, order);\\n\\t\\tbreak;\\n\\tcase KEXEC_TYPE_CRASH:\\n\\t\\tpages = kimage_alloc_crash_control_pages(image, order);\\n\\t\\tbreak;\\n\\t}\\n\\n\\treturn pages;\\n}\\n\\nint kimage_crash_copy_vmcoreinfo(struct kimage *image)\\n{\\n\\tstruct page *vmcoreinfo_page;\\n\\tvoid *safecopy;\\n\\n\\tif (image->type != KEXEC_TYPE_CRASH)\\n\\t\\treturn 0;\\n\\n\\t/*\\n\\t * For kdump, allocate one vmcoreinfo safe copy from the\\n\\t * crash memory. as we have arch_kexec_protect_crashkres()\\n\\t * after kexec syscall, we naturally protect it from write\\n\\t * (even read) access under kernel direct mapping. But on\\n\\t * the other hand, we still need to operate it when crash\\n\\t * happens to generate vmcoreinfo note, hereby we rely on\\n\\t * vmap for this purpose.\\n\\t */\\n\\tvmcoreinfo_page = kimage_alloc_control_pages(image, 0);\\n\\tif (!vmcoreinfo_page) {\\n\\t\\tpr_warn(\"Could not allocate vmcoreinfo buffer\\\\n\");\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\tsafecopy = vmap(&vmcoreinfo_page, 1, VM_MAP, PAGE_KERNEL);\\n\\tif (!safecopy) {\\n\\t\\tpr_warn(\"Could not vmap vmcoreinfo buffer\\\\n\");\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\timage->vmcoreinfo_data_copy = safecopy;\\n\\tcrash_update_vmcoreinfo_safecopy(safecopy);\\n\\n\\treturn 0;\\n}\\n\\nstatic int kimage_add_entry(struct kimage *image, kimage_entry_t entry)\\n{\\n\\tif (*image->entry != 0)\\n\\t\\timage->entry++;\\n\\n\\tif (image->entry == image->last_entry) {\\n\\t\\tkimage_entry_t *ind_page;\\n\\t\\tstruct page *page;\\n\\n\\t\\tpage = kimage_alloc_page(image, GFP_KERNEL, KIMAGE_NO_DEST);\\n\\t\\tif (!page)\\n\\t\\t\\treturn -ENOMEM;\\n\\n\\t\\tind_page = page_address(page);\\n\\t\\t*image->entry = virt_to_boot_phys(ind_page) | IND_INDIRECTION;\\n\\t\\timage->entry = ind_page;\\n\\t\\timage->last_entry = ind_page +\\n\\t\\t\\t\\t      ((PAGE_SIZE/sizeof(kimage_entry_t)) - 1);\\n\\t}\\n\\t*image->entry = entry;\\n\\timage->entry++;\\n\\t*image->entry = 0;\\n\\n\\treturn 0;\\n}\\n\\nstatic int kimage_set_destination(struct kimage *image,\\n\\t\\t\\t\\t   unsigned long destination)\\n{\\n\\tint result;\\n\\n\\tdestination &= PAGE_MASK;\\n\\tresult = kimage_add_entry(image, destination | IND_DESTINATION);\\n\\n\\treturn result;\\n}\\n\\n\\nstatic int kimage_add_page(struct kimage *image, unsigned long page)\\n{\\n\\tint result;\\n\\n\\tpage &= PAGE_MASK;\\n\\tresult = kimage_add_entry(image, page | IND_SOURCE);\\n\\n\\treturn result;\\n}\\n\\n\\nstatic void kimage_free_extra_pages(struct kimage *image)\\n{\\n\\t/* Walk through and free any extra destination pages I may have */\\n\\tkimage_free_page_list(&image->dest_pages);\\n\\n\\t/* Walk through and free any unusable pages I have cached */\\n\\tkimage_free_page_list(&image->unusable_pages);\\n\\n}\\nvoid kimage_terminate(struct kimage *image)\\n{\\n\\tif (*image->entry != 0)\\n\\t\\timage->entry++;\\n\\n\\t*image->entry = IND_DONE;\\n}\\n\\n#define for_each_kimage_entry(image, ptr, entry) \\\\\\n\\tfor (ptr = &image->head; (entry = *ptr) && !(entry & IND_DONE); \\\\\\n\\t\\tptr = (entry & IND_INDIRECTION) ? \\\\\\n\\t\\t\\tboot_phys_to_virt((entry & PAGE_MASK)) : ptr + 1)\\n\\nstatic void kimage_free_entry(kimage_entry_t entry)\\n{\\n\\tstruct page *page;\\n\\n\\tpage = boot_pfn_to_page(entry >> PAGE_SHIFT);\\n\\tkimage_free_pages(page);\\n}\\n\\nvoid kimage_free(struct kimage *image)\\n{\\n\\tkimage_entry_t *ptr, entry;\\n\\tkimage_entry_t ind = 0;\\n\\n\\tif (!image)\\n\\t\\treturn;\\n\\n\\tif (image->vmcoreinfo_data_copy) {\\n\\t\\tcrash_update_vmcoreinfo_safecopy(NULL);\\n\\t\\tvunmap(image->vmcoreinfo_data_copy);\\n\\t}\\n\\n\\tkimage_free_extra_pages(image);\\n\\tfor_each_kimage_entry(image, ptr, entry) {\\n\\t\\tif (entry & IND_INDIRECTION) {\\n\\t\\t\\t/* Free the previous indirection page */\\n\\t\\t\\tif (ind & IND_INDIRECTION)\\n\\t\\t\\t\\tkimage_free_entry(ind);\\n\\t\\t\\t/* Save this indirection page until we are\\n\\t\\t\\t * done with it.\\n\\t\\t\\t */\\n\\t\\t\\tind = entry;\\n\\t\\t} else if (entry & IND_SOURCE)\\n\\t\\t\\tkimage_free_entry(entry);\\n\\t}\\n\\t/* Free the final indirection page */\\n\\tif (ind & IND_INDIRECTION)\\n\\t\\tkimage_free_entry(ind);\\n\\n\\t/* Handle any machine specific cleanup */\\n\\tmachine_kexec_cleanup(image);\\n\\n\\t/* Free the kexec control pages... */\\n\\tkimage_free_page_list(&image->control_pages);\\n\\n\\t/*\\n\\t * Free up any temporary buffers allocated. This might hit if\\n\\t * error occurred much later after buffer allocation.\\n\\t */\\n\\tif (image->file_mode)\\n\\t\\tkimage_file_post_load_cleanup(image);\\n\\n\\tkfree(image);\\n}\\n\\nstatic kimage_entry_t *kimage_dst_used(struct kimage *image,\\n\\t\\t\\t\\t\\tunsigned long page)\\n{\\n\\tkimage_entry_t *ptr, entry;\\n\\tunsigned long destination = 0;\\n\\n\\tfor_each_kimage_entry(image, ptr, entry) {\\n\\t\\tif (entry & IND_DESTINATION)\\n\\t\\t\\tdestination = entry & PAGE_MASK;\\n\\t\\telse if (entry & IND_SOURCE) {\\n\\t\\t\\tif (page == destination)\\n\\t\\t\\t\\treturn ptr;\\n\\t\\t\\tdestination += PAGE_SIZE;\\n\\t\\t}\\n\\t}\\n\\n\\treturn NULL;\\n}\\n\\nstatic struct page *kimage_alloc_page(struct kimage *image,\\n\\t\\t\\t\\t\\tgfp_t gfp_mask,\\n\\t\\t\\t\\t\\tunsigned long destination)\\n{\\n\\t/*\\n\\t * Here we implement safeguards to ensure that a source page\\n\\t * is not copied to its destination page before the data on\\n\\t * the destination page is no longer useful.\\n\\t *\\n\\t * To do this we maintain the invariant that a source page is\\n\\t * either its own destination page, or it is not a\\n\\t * destination page at all.\\n\\t *\\n\\t * That is slightly stronger than required, but the proof\\n\\t * that no problems will not occur is trivial, and the\\n\\t * implementation is simply to verify.\\n\\t *\\n\\t * When allocating all pages normally this algorithm will run\\n\\t * in O(N) time, but in the worst case it will run in O(N^2)\\n\\t * time.   If the runtime is a problem the data structures can\\n\\t * be fixed.\\n\\t */\\n\\tstruct page *page;\\n\\tunsigned long addr;\\n\\n\\t/*\\n\\t * Walk through the list of destination pages, and see if I\\n\\t * have a match.\\n\\t */\\n\\tlist_for_each_entry(page, &image->dest_pages, lru) {\\n\\t\\taddr = page_to_boot_pfn(page) << PAGE_SHIFT;\\n\\t\\tif (addr == destination) {\\n\\t\\t\\tlist_del(&page->lru);\\n\\t\\t\\treturn page;\\n\\t\\t}\\n\\t}\\n\\tpage = NULL;\\n\\twhile (1) {\\n\\t\\tkimage_entry_t *old;\\n\\n\\t\\t/* Allocate a page, if we run out of memory give up */\\n\\t\\tpage = kimage_alloc_pages(gfp_mask, 0);\\n\\t\\tif (!page)\\n\\t\\t\\treturn NULL;\\n\\t\\t/* If the page cannot be used file it away */\\n\\t\\tif (page_to_boot_pfn(page) >\\n\\t\\t\\t\\t(KEXEC_SOURCE_MEMORY_LIMIT >> PAGE_SHIFT)) {\\n\\t\\t\\tlist_add(&page->lru, &image->unusable_pages);\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\taddr = page_to_boot_pfn(page) << PAGE_SHIFT;\\n\\n\\t\\t/* If it is the destination page we want use it */\\n\\t\\tif (addr == destination)\\n\\t\\t\\tbreak;\\n\\n\\t\\t/* If the page is not a destination page use it */\\n\\t\\tif (!kimage_is_destination_range(image, addr,\\n\\t\\t\\t\\t\\t\\t  addr + PAGE_SIZE))\\n\\t\\t\\tbreak;\\n\\n\\t\\t/*\\n\\t\\t * I know that the page is someones destination page.\\n\\t\\t * See if there is already a source page for this\\n\\t\\t * destination page.  And if so swap the source pages.\\n\\t\\t */\\n\\t\\told = kimage_dst_used(image, addr);\\n\\t\\tif (old) {\\n\\t\\t\\t/* If so move it */\\n\\t\\t\\tunsigned long old_addr;\\n\\t\\t\\tstruct page *old_page;\\n\\n\\t\\t\\told_addr = *old & PAGE_MASK;\\n\\t\\t\\told_page = boot_pfn_to_page(old_addr >> PAGE_SHIFT);\\n\\t\\t\\tcopy_highpage(page, old_page);\\n\\t\\t\\t*old = addr | (*old & ~PAGE_MASK);\\n\\n\\t\\t\\t/* The old page I have found cannot be a\\n\\t\\t\\t * destination page, so return it if it\\'s\\n\\t\\t\\t * gfp_flags honor the ones passed in.\\n\\t\\t\\t */\\n\\t\\t\\tif (!(gfp_mask & __GFP_HIGHMEM) &&\\n\\t\\t\\t    PageHighMem(old_page)) {\\n\\t\\t\\t\\tkimage_free_pages(old_page);\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\t}\\n\\t\\t\\taddr = old_addr;\\n\\t\\t\\tpage = old_page;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\t/* Place the page on the destination list, to be used later */\\n\\t\\tlist_add(&page->lru, &image->dest_pages);\\n\\t}\\n\\n\\treturn page;\\n}\\n\\nstatic int kimage_load_normal_segment(struct kimage *image,\\n\\t\\t\\t\\t\\t struct kexec_segment *segment)\\n{\\n\\tunsigned long maddr;\\n\\tsize_t ubytes, mbytes;\\n\\tint result;\\n\\tunsigned char __user *buf = NULL;\\n\\tunsigned char *kbuf = NULL;\\n\\n\\tresult = 0;\\n\\tif (image->file_mode)\\n\\t\\tkbuf = segment->kbuf;\\n\\telse\\n\\t\\tbuf = segment->buf;\\n\\tubytes = segment->bufsz;\\n\\tmbytes = segment->memsz;\\n\\tmaddr = segment->mem;\\n\\n\\tresult = kimage_set_destination(image, maddr);\\n\\tif (result < 0)\\n\\t\\tgoto out;\\n\\n\\twhile (mbytes) {\\n\\t\\tstruct page *page;\\n\\t\\tchar *ptr;\\n\\t\\tsize_t uchunk, mchunk;\\n\\n\\t\\tpage = kimage_alloc_page(image, GFP_HIGHUSER, maddr);\\n\\t\\tif (!page) {\\n\\t\\t\\tresult  = -ENOMEM;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tresult = kimage_add_page(image, page_to_boot_pfn(page)\\n\\t\\t\\t\\t\\t\\t\\t\\t<< PAGE_SHIFT);\\n\\t\\tif (result < 0)\\n\\t\\t\\tgoto out;\\n\\n\\t\\tptr = kmap(page);\\n\\t\\t/* Start with a clear page */\\n\\t\\tclear_page(ptr);\\n\\t\\tptr += maddr & ~PAGE_MASK;\\n\\t\\tmchunk = min_t(size_t, mbytes,\\n\\t\\t\\t\\tPAGE_SIZE - (maddr & ~PAGE_MASK));\\n\\t\\tuchunk = min(ubytes, mchunk);\\n\\n\\t\\t/* For file based kexec, source pages are in kernel memory */\\n\\t\\tif (image->file_mode)\\n\\t\\t\\tmemcpy(ptr, kbuf, uchunk);\\n\\t\\telse\\n\\t\\t\\tresult = copy_from_user(ptr, buf, uchunk);\\n\\t\\tkunmap(page);\\n\\t\\tif (result) {\\n\\t\\t\\tresult = -EFAULT;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tubytes -= uchunk;\\n\\t\\tmaddr  += mchunk;\\n\\t\\tif (image->file_mode)\\n\\t\\t\\tkbuf += mchunk;\\n\\t\\telse\\n\\t\\t\\tbuf += mchunk;\\n\\t\\tmbytes -= mchunk;\\n\\n\\t\\tcond_resched();\\n\\t}\\nout:\\n\\treturn result;\\n}\\n\\nstatic int kimage_load_crash_segment(struct kimage *image,\\n\\t\\t\\t\\t\\tstruct kexec_segment *segment)\\n{\\n\\t/* For crash dumps kernels we simply copy the data from\\n\\t * user space to it\\'s destination.\\n\\t * We do things a page at a time for the sake of kmap.\\n\\t */\\n\\tunsigned long maddr;\\n\\tsize_t ubytes, mbytes;\\n\\tint result;\\n\\tunsigned char __user *buf = NULL;\\n\\tunsigned char *kbuf = NULL;\\n\\n\\tresult = 0;\\n\\tif (image->file_mode)\\n\\t\\tkbuf = segment->kbuf;\\n\\telse\\n\\t\\tbuf = segment->buf;\\n\\tubytes = segment->bufsz;\\n\\tmbytes = segment->memsz;\\n\\tmaddr = segment->mem;\\n\\twhile (mbytes) {\\n\\t\\tstruct page *page;\\n\\t\\tchar *ptr;\\n\\t\\tsize_t uchunk, mchunk;\\n\\n\\t\\tpage = boot_pfn_to_page(maddr >> PAGE_SHIFT);\\n\\t\\tif (!page) {\\n\\t\\t\\tresult  = -ENOMEM;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tarch_kexec_post_alloc_pages(page_address(page), 1, 0);\\n\\t\\tptr = kmap(page);\\n\\t\\tptr += maddr & ~PAGE_MASK;\\n\\t\\tmchunk = min_t(size_t, mbytes,\\n\\t\\t\\t\\tPAGE_SIZE - (maddr & ~PAGE_MASK));\\n\\t\\tuchunk = min(ubytes, mchunk);\\n\\t\\tif (mchunk > uchunk) {\\n\\t\\t\\t/* Zero the trailing part of the page */\\n\\t\\t\\tmemset(ptr + uchunk, 0, mchunk - uchunk);\\n\\t\\t}\\n\\n\\t\\t/* For file based kexec, source pages are in kernel memory */\\n\\t\\tif (image->file_mode)\\n\\t\\t\\tmemcpy(ptr, kbuf, uchunk);\\n\\t\\telse\\n\\t\\t\\tresult = copy_from_user(ptr, buf, uchunk);\\n\\t\\tkexec_flush_icache_page(page);\\n\\t\\tkunmap(page);\\n\\t\\tarch_kexec_pre_free_pages(page_address(page), 1);\\n\\t\\tif (result) {\\n\\t\\t\\tresult = -EFAULT;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tubytes -= uchunk;\\n\\t\\tmaddr  += mchunk;\\n\\t\\tif (image->file_mode)\\n\\t\\t\\tkbuf += mchunk;\\n\\t\\telse\\n\\t\\t\\tbuf += mchunk;\\n\\t\\tmbytes -= mchunk;\\n\\n\\t\\tcond_resched();\\n\\t}\\nout:\\n\\treturn result;\\n}\\n\\nint kimage_load_segment(struct kimage *image,\\n\\t\\t\\t\\tstruct kexec_segment *segment)\\n{\\n\\tint result = -ENOMEM;\\n\\n\\tswitch (image->type) {\\n\\tcase KEXEC_TYPE_DEFAULT:\\n\\t\\tresult = kimage_load_normal_segment(image, segment);\\n\\t\\tbreak;\\n\\tcase KEXEC_TYPE_CRASH:\\n\\t\\tresult = kimage_load_crash_segment(image, segment);\\n\\t\\tbreak;\\n\\t}\\n\\n\\treturn result;\\n}\\n\\nstruct kimage *kexec_image;\\nstruct kimage *kexec_crash_image;\\nint kexec_load_disabled;\\n\\n/*\\n * No panic_cpu check version of crash_kexec().  This function is called\\n * only when panic_cpu holds the current CPU number; this is the only CPU\\n * which processes crash_kexec routines.\\n */\\nvoid __noclone __crash_kexec(struct pt_regs *regs)\\n{\\n\\t/* Take the kexec_mutex here to prevent sys_kexec_load\\n\\t * running on one cpu from replacing the crash kernel\\n\\t * we are using after a panic on a different cpu.\\n\\t *\\n\\t * If the crash kernel was not located in a fixed area\\n\\t * of memory the xchg(&kexec_crash_image) would be\\n\\t * sufficient.  But since I reuse the memory...\\n\\t */\\n\\tif (mutex_trylock(&kexec_mutex)) {\\n\\t\\tif (kexec_crash_image) {\\n\\t\\t\\tstruct pt_regs fixed_regs;\\n\\n\\t\\t\\tcrash_setup_regs(&fixed_regs, regs);\\n\\t\\t\\tcrash_save_vmcoreinfo();\\n\\t\\t\\tmachine_crash_shutdown(&fixed_regs);\\n\\t\\t\\tmachine_kexec(kexec_crash_image);\\n\\t\\t}\\n\\t\\tmutex_unlock(&kexec_mutex);\\n\\t}\\n}\\nSTACK_FRAME_NON_STANDARD(__crash_kexec);\\n\\nvoid crash_kexec(struct pt_regs *regs)\\n{\\n\\tint old_cpu, this_cpu;\\n\\n\\t/*\\n\\t * Only one CPU is allowed to execute the crash_kexec() code as with\\n\\t * panic().  Otherwise parallel calls of panic() and crash_kexec()\\n\\t * may stop each other.  To exclude them, we use panic_cpu here too.\\n\\t */\\n\\tthis_cpu = raw_smp_processor_id();\\n\\told_cpu = atomic_cmpxchg(&panic_cpu, PANIC_CPU_INVALID, this_cpu);\\n\\tif (old_cpu == PANIC_CPU_INVALID) {\\n\\t\\t/* This is the 1st CPU which comes here, so go ahead. */\\n\\t\\tprintk_safe_flush_on_panic();\\n\\t\\t__crash_kexec(regs);\\n\\n\\t\\t/*\\n\\t\\t * Reset panic_cpu to allow another panic()/crash_kexec()\\n\\t\\t * call.\\n\\t\\t */\\n\\t\\tatomic_set(&panic_cpu, PANIC_CPU_INVALID);\\n\\t}\\n}\\n\\nsize_t crash_get_memory_size(void)\\n{\\n\\tsize_t size = 0;\\n\\n\\tmutex_lock(&kexec_mutex);\\n\\tif (crashk_res.end != crashk_res.start)\\n\\t\\tsize = resource_size(&crashk_res);\\n\\tmutex_unlock(&kexec_mutex);\\n\\treturn size;\\n}\\n\\nvoid __weak crash_free_reserved_phys_range(unsigned long begin,\\n\\t\\t\\t\\t\\t   unsigned long end)\\n{\\n\\tunsigned long addr;\\n\\n\\tfor (addr = begin; addr < end; addr += PAGE_SIZE)\\n\\t\\tfree_reserved_page(boot_pfn_to_page(addr >> PAGE_SHIFT));\\n}\\n\\nint crash_shrink_memory(unsigned long new_size)\\n{\\n\\tint ret = 0;\\n\\tunsigned long start, end;\\n\\tunsigned long old_size;\\n\\tstruct resource *ram_res;\\n\\n\\tmutex_lock(&kexec_mutex);\\n\\n\\tif (kexec_crash_image) {\\n\\t\\tret = -ENOENT;\\n\\t\\tgoto unlock;\\n\\t}\\n\\tstart = crashk_res.start;\\n\\tend = crashk_res.end;\\n\\told_size = (end == 0) ? 0 : end - start + 1;\\n\\tif (new_size >= old_size) {\\n\\t\\tret = (new_size == old_size) ? 0 : -EINVAL;\\n\\t\\tgoto unlock;\\n\\t}\\n\\n\\tram_res = kzalloc(sizeof(*ram_res), GFP_KERNEL);\\n\\tif (!ram_res) {\\n\\t\\tret = -ENOMEM;\\n\\t\\tgoto unlock;\\n\\t}\\n\\n\\tstart = roundup(start, KEXEC_CRASH_MEM_ALIGN);\\n\\tend = roundup(start + new_size, KEXEC_CRASH_MEM_ALIGN);\\n\\n\\tcrash_free_reserved_phys_range(end, crashk_res.end);\\n\\n\\tif ((start == end) && (crashk_res.parent != NULL))\\n\\t\\trelease_resource(&crashk_res);\\n\\n\\tram_res->start = end;\\n\\tram_res->end = crashk_res.end;\\n\\tram_res->flags = IORESOURCE_BUSY | IORESOURCE_SYSTEM_RAM;\\n\\tram_res->name = \"System RAM\";\\n\\n\\tcrashk_res.end = end - 1;\\n\\n\\tinsert_resource(&iomem_resource, ram_res);\\n\\nunlock:\\n\\tmutex_unlock(&kexec_mutex);\\n\\treturn ret;\\n}\\n\\nvoid crash_save_cpu(struct pt_regs *regs, int cpu)\\n{\\n\\tstruct elf_prstatus prstatus;\\n\\tu32 *buf;\\n\\n\\tif ((cpu < 0) || (cpu >= nr_cpu_ids))\\n\\t\\treturn;\\n\\n\\t/* Using ELF notes here is opportunistic.\\n\\t * I need a well defined structure format\\n\\t * for the data I pass, and I need tags\\n\\t * on the data to indicate what information I have\\n\\t * squirrelled away.  ELF notes happen to provide\\n\\t * all of that, so there is no need to invent something new.\\n\\t */\\n\\tbuf = (u32 *)per_cpu_ptr(crash_notes, cpu);\\n\\tif (!buf)\\n\\t\\treturn;\\n\\tmemset(&prstatus, 0, sizeof(prstatus));\\n\\tprstatus.pr_pid = current->pid;\\n\\telf_core_copy_kernel_regs(&prstatus.pr_reg, regs);\\n\\tbuf = append_elf_note(buf, KEXEC_CORE_NOTE_NAME, NT_PRSTATUS,\\n\\t\\t\\t      &prstatus, sizeof(prstatus));\\n\\tfinal_note(buf);\\n}\\n\\nstatic int __init crash_notes_memory_init(void)\\n{\\n\\t/* Allocate memory for saving cpu registers. */\\n\\tsize_t size, align;\\n\\n\\t/*\\n\\t * crash_notes could be allocated across 2 vmalloc pages when percpu\\n\\t * is vmalloc based . vmalloc doesn\\'t guarantee 2 continuous vmalloc\\n\\t * pages are also on 2 continuous physical pages. In this case the\\n\\t * 2nd part of crash_notes in 2nd page could be lost since only the\\n\\t * starting address and size of crash_notes are exported through sysfs.\\n\\t * Here round up the size of crash_notes to the nearest power of two\\n\\t * and pass it to __alloc_percpu as align value. This can make sure\\n\\t * crash_notes is allocated inside one physical page.\\n\\t */\\n\\tsize = sizeof(note_buf_t);\\n\\talign = min(roundup_pow_of_two(sizeof(note_buf_t)), PAGE_SIZE);\\n\\n\\t/*\\n\\t * Break compile if size is bigger than PAGE_SIZE since crash_notes\\n\\t * definitely will be in 2 pages with that.\\n\\t */\\n\\tBUILD_BUG_ON(size > PAGE_SIZE);\\n\\n\\tcrash_notes = __alloc_percpu(size, align);\\n\\tif (!crash_notes) {\\n\\t\\tpr_warn(\"Memory allocation for saving cpu register states failed\\\\n\");\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\treturn 0;\\n}\\nsubsys_initcall(crash_notes_memory_init);\\n\\n\\n/*\\n * Move into place and start executing a preloaded standalone\\n * executable.  If nothing was preloaded return an error.\\n */\\nint kernel_kexec(void)\\n{\\n\\tint error = 0;\\n\\n\\tif (!mutex_trylock(&kexec_mutex))\\n\\t\\treturn -EBUSY;\\n\\tif (!kexec_image) {\\n\\t\\terror = -EINVAL;\\n\\t\\tgoto Unlock;\\n\\t}\\n\\n#ifdef CONFIG_KEXEC_JUMP\\n\\tif (kexec_image->preserve_context) {\\n\\t\\tlock_system_sleep();\\n\\t\\tpm_prepare_console();\\n\\t\\terror = freeze_processes();\\n\\t\\tif (error) {\\n\\t\\t\\terror = -EBUSY;\\n\\t\\t\\tgoto Restore_console;\\n\\t\\t}\\n\\t\\tsuspend_console();\\n\\t\\terror = dpm_suspend_start(PMSG_FREEZE);\\n\\t\\tif (error)\\n\\t\\t\\tgoto Resume_console;\\n\\t\\t/* At this point, dpm_suspend_start() has been called,\\n\\t\\t * but *not* dpm_suspend_end(). We *must* call\\n\\t\\t * dpm_suspend_end() now.  Otherwise, drivers for\\n\\t\\t * some devices (e.g. interrupt controllers) become\\n\\t\\t * desynchronized with the actual state of the\\n\\t\\t * hardware at resume time, and evil weirdness ensues.\\n\\t\\t */\\n\\t\\terror = dpm_suspend_end(PMSG_FREEZE);\\n\\t\\tif (error)\\n\\t\\t\\tgoto Resume_devices;\\n\\t\\terror = disable_nonboot_cpus();\\n\\t\\tif (error)\\n\\t\\t\\tgoto Enable_cpus;\\n\\t\\tlocal_irq_disable();\\n\\t\\terror = syscore_suspend();\\n\\t\\tif (error)\\n\\t\\t\\tgoto Enable_irqs;\\n\\t} else\\n#endif\\n\\t{\\n\\t\\tkexec_in_progress = true;\\n\\t\\tkernel_restart_prepare(NULL);\\n\\t\\tmigrate_to_reboot_cpu();\\n\\n\\t\\t/*\\n\\t\\t * migrate_to_reboot_cpu() disables CPU hotplug assuming that\\n\\t\\t * no further code needs to use CPU hotplug (which is true in\\n\\t\\t * the reboot case). However, the kexec path depends on using\\n\\t\\t * CPU hotplug again; so re-enable it here.\\n\\t\\t */\\n\\t\\tcpu_hotplug_enable();\\n\\t\\tpr_emerg(\"Starting new kernel\\\\n\");\\n\\t\\tmachine_shutdown();\\n\\t}\\n\\n\\tmachine_kexec(kexec_image);\\n\\n#ifdef CONFIG_KEXEC_JUMP\\n\\tif (kexec_image->preserve_context) {\\n\\t\\tsyscore_resume();\\n Enable_irqs:\\n\\t\\tlocal_irq_enable();\\n Enable_cpus:\\n\\t\\tenable_nonboot_cpus();\\n\\t\\tdpm_resume_start(PMSG_RESTORE);\\n Resume_devices:\\n\\t\\tdpm_resume_end(PMSG_RESTORE);\\n Resume_console:\\n\\t\\tresume_console();\\n\\t\\tthaw_processes();\\n Restore_console:\\n\\t\\tpm_restore_console();\\n\\t\\tunlock_system_sleep();\\n\\t}\\n#endif\\n\\n Unlock:\\n\\tmutex_unlock(&kexec_mutex);\\n\\treturn error;\\n}\\n\\n/*\\n * Protection mechanism for crashkernel reserved memory after\\n * the kdump kernel is loaded.\\n *\\n * Provide an empty default implementation here -- architecture\\n * code may override this\\n */\\nvoid __weak arch_kexec_protect_crashkres(void)\\n{}\\n\\nvoid __weak arch_kexec_unprotect_crashkres(void)\\n{}\\n\\n/*\\n * async.c: Asynchronous function calls for boot performance\\n *\\n * (C) Copyright 2009 Intel Corporation\\n * Author: Arjan van de Ven <arjan@linux.intel.com>\\n *\\n * This program is free software; you can redistribute it and/or\\n * modify it under the terms of the GNU General Public License\\n * as published by the Free Software Foundation; version 2\\n * of the License.\\n */\\n\\n\\n/*\\n\\nGoals and Theory of Operation\\n\\nThe primary goal of this feature is to reduce the kernel boot time,\\nby doing various independent hardware delays and discovery operations\\ndecoupled and not strictly serialized.\\n\\nMore specifically, the asynchronous function call concept allows\\ncertain operations (primarily during system boot) to happen\\nasynchronously, out of order, while these operations still\\nhave their externally visible parts happen sequentially and in-order.\\n(not unlike how out-of-order CPUs retire their instructions in order)\\n\\nKey to the asynchronous function call implementation is the concept of\\na \"sequence cookie\" (which, although it has an abstracted type, can be\\nthought of as a monotonically incrementing number).\\n\\nThe async core will assign each scheduled event such a sequence cookie and\\npass this to the called functions.\\n\\nThe asynchronously called function should before doing a globally visible\\noperation, such as registering device numbers, call the\\nasync_synchronize_cookie() function and pass in its own cookie. The\\nasync_synchronize_cookie() function will make sure that all asynchronous\\noperations that were scheduled prior to the operation corresponding with the\\ncookie have completed.\\n\\nSubsystem/driver initialization code that scheduled asynchronous probe\\nfunctions, but which shares global resources with other drivers/subsystems\\nthat do not use the asynchronous call feature, need to do a full\\nsynchronization with the async_synchronize_full() function, before returning\\nfrom their init function. This is to maintain strict ordering between the\\nasynchronous and synchronous parts of the kernel.\\n\\n*/\\n\\n#include <linux/async.h>\\n#include <linux/atomic.h>\\n#include <linux/ktime.h>\\n#include <linux/export.h>\\n#include <linux/wait.h>\\n#include <linux/sched.h>\\n#include <linux/slab.h>\\n#include <linux/workqueue.h>\\n\\n#include \"workqueue_internal.h\"\\n\\nstatic async_cookie_t next_cookie = 1;\\n\\n#define MAX_WORK\\t\\t32768\\n#define ASYNC_COOKIE_MAX\\tULLONG_MAX\\t/* infinity cookie */\\n\\nstatic LIST_HEAD(async_global_pending);\\t/* pending from all registered doms */\\nstatic ASYNC_DOMAIN(async_dfl_domain);\\nstatic DEFINE_SPINLOCK(async_lock);\\n\\nstruct async_entry {\\n\\tstruct list_head\\tdomain_list;\\n\\tstruct list_head\\tglobal_list;\\n\\tstruct work_struct\\twork;\\n\\tasync_cookie_t\\t\\tcookie;\\n\\tasync_func_t\\t\\tfunc;\\n\\tvoid\\t\\t\\t*data;\\n\\tstruct async_domain\\t*domain;\\n};\\n\\nstatic DECLARE_WAIT_QUEUE_HEAD(async_done);\\n\\nstatic atomic_t entry_count;\\n\\nstatic async_cookie_t lowest_in_progress(struct async_domain *domain)\\n{\\n\\tstruct async_entry *first = NULL;\\n\\tasync_cookie_t ret = ASYNC_COOKIE_MAX;\\n\\tunsigned long flags;\\n\\n\\tspin_lock_irqsave(&async_lock, flags);\\n\\n\\tif (domain) {\\n\\t\\tif (!list_empty(&domain->pending))\\n\\t\\t\\tfirst = list_first_entry(&domain->pending,\\n\\t\\t\\t\\t\\tstruct async_entry, domain_list);\\n\\t} else {\\n\\t\\tif (!list_empty(&async_global_pending))\\n\\t\\t\\tfirst = list_first_entry(&async_global_pending,\\n\\t\\t\\t\\t\\tstruct async_entry, global_list);\\n\\t}\\n\\n\\tif (first)\\n\\t\\tret = first->cookie;\\n\\n\\tspin_unlock_irqrestore(&async_lock, flags);\\n\\treturn ret;\\n}\\n\\n/*\\n * pick the first pending entry and run it\\n */\\nstatic void async_run_entry_fn(struct work_struct *work)\\n{\\n\\tstruct async_entry *entry =\\n\\t\\tcontainer_of(work, struct async_entry, work);\\n\\tunsigned long flags;\\n\\tktime_t uninitialized_var(calltime), delta, rettime;\\n\\n\\t/* 1) run (and print duration) */\\n\\tif (initcall_debug && system_state < SYSTEM_RUNNING) {\\n\\t\\tpr_debug(\"calling  %lli_%pF @ %i\\\\n\",\\n\\t\\t\\t(long long)entry->cookie,\\n\\t\\t\\tentry->func, task_pid_nr(current));\\n\\t\\tcalltime = ktime_get();\\n\\t}\\n\\tentry->func(entry->data, entry->cookie);\\n\\tif (initcall_debug && system_state < SYSTEM_RUNNING) {\\n\\t\\trettime = ktime_get();\\n\\t\\tdelta = ktime_sub(rettime, calltime);\\n\\t\\tpr_debug(\"initcall %lli_%pF returned 0 after %lld usecs\\\\n\",\\n\\t\\t\\t(long long)entry->cookie,\\n\\t\\t\\tentry->func,\\n\\t\\t\\t(long long)ktime_to_ns(delta) >> 10);\\n\\t}\\n\\n\\t/* 2) remove self from the pending queues */\\n\\tspin_lock_irqsave(&async_lock, flags);\\n\\tlist_del_init(&entry->domain_list);\\n\\tlist_del_init(&entry->global_list);\\n\\n\\t/* 3) free the entry */\\n\\tkfree(entry);\\n\\tatomic_dec(&entry_count);\\n\\n\\tspin_unlock_irqrestore(&async_lock, flags);\\n\\n\\t/* 4) wake up any waiters */\\n\\twake_up(&async_done);\\n}\\n\\nstatic async_cookie_t __async_schedule(async_func_t func, void *data, struct async_domain *domain)\\n{\\n\\tstruct async_entry *entry;\\n\\tunsigned long flags;\\n\\tasync_cookie_t newcookie;\\n\\n\\t/* allow irq-off callers */\\n\\tentry = kzalloc(sizeof(struct async_entry), GFP_ATOMIC);\\n\\n\\t/*\\n\\t * If we\\'re out of memory or if there\\'s too much work\\n\\t * pending already, we execute synchronously.\\n\\t */\\n\\tif (!entry || atomic_read(&entry_count) > MAX_WORK) {\\n\\t\\tkfree(entry);\\n\\t\\tspin_lock_irqsave(&async_lock, flags);\\n\\t\\tnewcookie = next_cookie++;\\n\\t\\tspin_unlock_irqrestore(&async_lock, flags);\\n\\n\\t\\t/* low on memory.. run synchronously */\\n\\t\\tfunc(data, newcookie);\\n\\t\\treturn newcookie;\\n\\t}\\n\\tINIT_LIST_HEAD(&entry->domain_list);\\n\\tINIT_LIST_HEAD(&entry->global_list);\\n\\tINIT_WORK(&entry->work, async_run_entry_fn);\\n\\tentry->func = func;\\n\\tentry->data = data;\\n\\tentry->domain = domain;\\n\\n\\tspin_lock_irqsave(&async_lock, flags);\\n\\n\\t/* allocate cookie and queue */\\n\\tnewcookie = entry->cookie = next_cookie++;\\n\\n\\tlist_add_tail(&entry->domain_list, &domain->pending);\\n\\tif (domain->registered)\\n\\t\\tlist_add_tail(&entry->global_list, &async_global_pending);\\n\\n\\tatomic_inc(&entry_count);\\n\\tspin_unlock_irqrestore(&async_lock, flags);\\n\\n\\t/* mark that this task has queued an async job, used by module init */\\n\\tcurrent->flags |= PF_USED_ASYNC;\\n\\n\\t/* schedule for execution */\\n\\tqueue_work(system_unbound_wq, &entry->work);\\n\\n\\treturn newcookie;\\n}\\n\\n/**\\n * async_schedule - schedule a function for asynchronous execution\\n * @func: function to execute asynchronously\\n * @data: data pointer to pass to the function\\n *\\n * Returns an async_cookie_t that may be used for checkpointing later.\\n * Note: This function may be called from atomic or non-atomic contexts.\\n */\\nasync_cookie_t async_schedule(async_func_t func, void *data)\\n{\\n\\treturn __async_schedule(func, data, &async_dfl_domain);\\n}\\nEXPORT_SYMBOL_GPL(async_schedule);\\n\\n/**\\n * async_schedule_domain - schedule a function for asynchronous execution within a certain domain\\n * @func: function to execute asynchronously\\n * @data: data pointer to pass to the function\\n * @domain: the domain\\n *\\n * Returns an async_cookie_t that may be used for checkpointing later.\\n * @domain may be used in the async_synchronize_*_domain() functions to\\n * wait within a certain synchronization domain rather than globally.  A\\n * synchronization domain is specified via @domain.  Note: This function\\n * may be called from atomic or non-atomic contexts.\\n */\\nasync_cookie_t async_schedule_domain(async_func_t func, void *data,\\n\\t\\t\\t\\t     struct async_domain *domain)\\n{\\n\\treturn __async_schedule(func, data, domain);\\n}\\nEXPORT_SYMBOL_GPL(async_schedule_domain);\\n\\n/**\\n * async_synchronize_full - synchronize all asynchronous function calls\\n *\\n * This function waits until all asynchronous function calls have been done.\\n */\\nvoid async_synchronize_full(void)\\n{\\n\\tasync_synchronize_full_domain(NULL);\\n}\\nEXPORT_SYMBOL_GPL(async_synchronize_full);\\n\\n/**\\n * async_unregister_domain - ensure no more anonymous waiters on this domain\\n * @domain: idle domain to flush out of any async_synchronize_full instances\\n *\\n * async_synchronize_{cookie|full}_domain() are not flushed since callers\\n * of these routines should know the lifetime of @domain\\n *\\n * Prefer ASYNC_DOMAIN_EXCLUSIVE() declarations over flushing\\n */\\nvoid async_unregister_domain(struct async_domain *domain)\\n{\\n\\tspin_lock_irq(&async_lock);\\n\\tWARN_ON(!domain->registered || !list_empty(&domain->pending));\\n\\tdomain->registered = 0;\\n\\tspin_unlock_irq(&async_lock);\\n}\\nEXPORT_SYMBOL_GPL(async_unregister_domain);\\n\\n/**\\n * async_synchronize_full_domain - synchronize all asynchronous function within a certain domain\\n * @domain: the domain to synchronize\\n *\\n * This function waits until all asynchronous function calls for the\\n * synchronization domain specified by @domain have been done.\\n */\\nvoid async_synchronize_full_domain(struct async_domain *domain)\\n{\\n\\tasync_synchronize_cookie_domain(ASYNC_COOKIE_MAX, domain);\\n}\\nEXPORT_SYMBOL_GPL(async_synchronize_full_domain);\\n\\n/**\\n * async_synchronize_cookie_domain - synchronize asynchronous function calls within a certain domain with cookie checkpointing\\n * @cookie: async_cookie_t to use as checkpoint\\n * @domain: the domain to synchronize (%NULL for all registered domains)\\n *\\n * This function waits until all asynchronous function calls for the\\n * synchronization domain specified by @domain submitted prior to @cookie\\n * have been done.\\n */\\nvoid async_synchronize_cookie_domain(async_cookie_t cookie, struct async_domain *domain)\\n{\\n\\tktime_t uninitialized_var(starttime), delta, endtime;\\n\\n\\tif (initcall_debug && system_state < SYSTEM_RUNNING) {\\n\\t\\tpr_debug(\"async_waiting @ %i\\\\n\", task_pid_nr(current));\\n\\t\\tstarttime = ktime_get();\\n\\t}\\n\\n\\twait_event(async_done, lowest_in_progress(domain) >= cookie);\\n\\n\\tif (initcall_debug && system_state < SYSTEM_RUNNING) {\\n\\t\\tendtime = ktime_get();\\n\\t\\tdelta = ktime_sub(endtime, starttime);\\n\\n\\t\\tpr_debug(\"async_continuing @ %i after %lli usec\\\\n\",\\n\\t\\t\\ttask_pid_nr(current),\\n\\t\\t\\t(long long)ktime_to_ns(delta) >> 10);\\n\\t}\\n}\\nEXPORT_SYMBOL_GPL(async_synchronize_cookie_domain);\\n\\n/**\\n * async_synchronize_cookie - synchronize asynchronous function calls with cookie checkpointing\\n * @cookie: async_cookie_t to use as checkpoint\\n *\\n * This function waits until all asynchronous function calls prior to @cookie\\n * have been done.\\n */\\nvoid async_synchronize_cookie(async_cookie_t cookie)\\n{\\n\\tasync_synchronize_cookie_domain(cookie, &async_dfl_domain);\\n}\\nEXPORT_SYMBOL_GPL(async_synchronize_cookie);\\n\\n/**\\n * current_is_async - is %current an async worker task?\\n *\\n * Returns %true if %current is an async worker task.\\n */\\nbool current_is_async(void)\\n{\\n\\tstruct worker *worker = current_wq_worker();\\n\\n\\treturn worker && worker->current_func == async_run_entry_fn;\\n}\\nEXPORT_SYMBOL_GPL(current_is_async);\\n\\n/*\\n *  Copyright (C) 2004 IBM Corporation\\n *\\n *  Author: Serge Hallyn <serue@us.ibm.com>\\n *\\n *  This program is free software; you can redistribute it and/or\\n *  modify it under the terms of the GNU General Public License as\\n *  published by the Free Software Foundation, version 2 of the\\n *  License.\\n */\\n\\n#include <linux/export.h>\\n#include <linux/uts.h>\\n#include <linux/utsname.h>\\n#include <linux/err.h>\\n#include <linux/slab.h>\\n#include <linux/cred.h>\\n#include <linux/user_namespace.h>\\n#include <linux/proc_ns.h>\\n#include <linux/sched/task.h>\\n\\nstatic struct kmem_cache *uts_ns_cache __ro_after_init;\\n\\nstatic struct ucounts *inc_uts_namespaces(struct user_namespace *ns)\\n{\\n\\treturn inc_ucount(ns, current_euid(), UCOUNT_UTS_NAMESPACES);\\n}\\n\\nstatic void dec_uts_namespaces(struct ucounts *ucounts)\\n{\\n\\tdec_ucount(ucounts, UCOUNT_UTS_NAMESPACES);\\n}\\n\\nstatic struct uts_namespace *create_uts_ns(void)\\n{\\n\\tstruct uts_namespace *uts_ns;\\n\\n\\tuts_ns = kmem_cache_alloc(uts_ns_cache, GFP_KERNEL);\\n\\tif (uts_ns)\\n\\t\\tkref_init(&uts_ns->kref);\\n\\treturn uts_ns;\\n}\\n\\n/*\\n * Clone a new ns copying an original utsname, setting refcount to 1\\n * @old_ns: namespace to clone\\n * Return ERR_PTR(-ENOMEM) on error (failure to allocate), new ns otherwise\\n */\\nstatic struct uts_namespace *clone_uts_ns(struct user_namespace *user_ns,\\n\\t\\t\\t\\t\\t  struct uts_namespace *old_ns)\\n{\\n\\tstruct uts_namespace *ns;\\n\\tstruct ucounts *ucounts;\\n\\tint err;\\n\\n\\terr = -ENOSPC;\\n\\tucounts = inc_uts_namespaces(user_ns);\\n\\tif (!ucounts)\\n\\t\\tgoto fail;\\n\\n\\terr = -ENOMEM;\\n\\tns = create_uts_ns();\\n\\tif (!ns)\\n\\t\\tgoto fail_dec;\\n\\n\\terr = ns_alloc_inum(&ns->ns);\\n\\tif (err)\\n\\t\\tgoto fail_free;\\n\\n\\tns->ucounts = ucounts;\\n\\tns->ns.ops = &utsns_operations;\\n\\n\\tdown_read(&uts_sem);\\n\\tmemcpy(&ns->name, &old_ns->name, sizeof(ns->name));\\n\\tns->user_ns = get_user_ns(user_ns);\\n\\tup_read(&uts_sem);\\n\\treturn ns;\\n\\nfail_free:\\n\\tkmem_cache_free(uts_ns_cache, ns);\\nfail_dec:\\n\\tdec_uts_namespaces(ucounts);\\nfail:\\n\\treturn ERR_PTR(err);\\n}\\n\\n/*\\n * Copy task tsk\\'s utsname namespace, or clone it if flags\\n * specifies CLONE_NEWUTS.  In latter case, changes to the\\n * utsname of this process won\\'t be seen by parent, and vice\\n * versa.\\n */\\nstruct uts_namespace *copy_utsname(unsigned long flags,\\n\\tstruct user_namespace *user_ns, struct uts_namespace *old_ns)\\n{\\n\\tstruct uts_namespace *new_ns;\\n\\n\\tBUG_ON(!old_ns);\\n\\tget_uts_ns(old_ns);\\n\\n\\tif (!(flags & CLONE_NEWUTS))\\n\\t\\treturn old_ns;\\n\\n\\tnew_ns = clone_uts_ns(user_ns, old_ns);\\n\\n\\tput_uts_ns(old_ns);\\n\\treturn new_ns;\\n}\\n\\nvoid free_uts_ns(struct kref *kref)\\n{\\n\\tstruct uts_namespace *ns;\\n\\n\\tns = container_of(kref, struct uts_namespace, kref);\\n\\tdec_uts_namespaces(ns->ucounts);\\n\\tput_user_ns(ns->user_ns);\\n\\tns_free_inum(&ns->ns);\\n\\tkmem_cache_free(uts_ns_cache, ns);\\n}\\n\\nstatic inline struct uts_namespace *to_uts_ns(struct ns_common *ns)\\n{\\n\\treturn container_of(ns, struct uts_namespace, ns);\\n}\\n\\nstatic struct ns_common *utsns_get(struct task_struct *task)\\n{\\n\\tstruct uts_namespace *ns = NULL;\\n\\tstruct nsproxy *nsproxy;\\n\\n\\ttask_lock(task);\\n\\tnsproxy = task->nsproxy;\\n\\tif (nsproxy) {\\n\\t\\tns = nsproxy->uts_ns;\\n\\t\\tget_uts_ns(ns);\\n\\t}\\n\\ttask_unlock(task);\\n\\n\\treturn ns ? &ns->ns : NULL;\\n}\\n\\nstatic void utsns_put(struct ns_common *ns)\\n{\\n\\tput_uts_ns(to_uts_ns(ns));\\n}\\n\\nstatic int utsns_install(struct nsproxy *nsproxy, struct ns_common *new)\\n{\\n\\tstruct uts_namespace *ns = to_uts_ns(new);\\n\\n\\tif (!ns_capable(ns->user_ns, CAP_SYS_ADMIN) ||\\n\\t    !ns_capable(current_user_ns(), CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\tget_uts_ns(ns);\\n\\tput_uts_ns(nsproxy->uts_ns);\\n\\tnsproxy->uts_ns = ns;\\n\\treturn 0;\\n}\\n\\nstatic struct user_namespace *utsns_owner(struct ns_common *ns)\\n{\\n\\treturn to_uts_ns(ns)->user_ns;\\n}\\n\\nconst struct proc_ns_operations utsns_operations = {\\n\\t.name\\t\\t= \"uts\",\\n\\t.type\\t\\t= CLONE_NEWUTS,\\n\\t.get\\t\\t= utsns_get,\\n\\t.put\\t\\t= utsns_put,\\n\\t.install\\t= utsns_install,\\n\\t.owner\\t\\t= utsns_owner,\\n};\\n\\nvoid __init uts_ns_init(void)\\n{\\n\\tuts_ns_cache = kmem_cache_create_usercopy(\\n\\t\\t\\t\"uts_namespace\", sizeof(struct uts_namespace), 0,\\n\\t\\t\\tSLAB_PANIC|SLAB_ACCOUNT,\\n\\t\\t\\toffsetof(struct uts_namespace, name),\\n\\t\\t\\tsizeof_field(struct uts_namespace, name),\\n\\t\\t\\tNULL);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0+\\n/*\\n * Restartable sequences system call\\n *\\n * Copyright (C) 2015, Google, Inc.,\\n * Paul Turner <pjt@google.com> and Andrew Hunter <ahh@google.com>\\n * Copyright (C) 2015-2018, EfficiOS Inc.,\\n * Mathieu Desnoyers <mathieu.desnoyers@efficios.com>\\n */\\n\\n#include <linux/sched.h>\\n#include <linux/uaccess.h>\\n#include <linux/syscalls.h>\\n#include <linux/rseq.h>\\n#include <linux/types.h>\\n#include <asm/ptrace.h>\\n\\n#define CREATE_TRACE_POINTS\\n#include <trace/events/rseq.h>\\n\\n#define RSEQ_CS_PREEMPT_MIGRATE_FLAGS (RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE | \\\\\\n\\t\\t\\t\\t       RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT)\\n\\n/*\\n *\\n * Restartable sequences are a lightweight interface that allows\\n * user-level code to be executed atomically relative to scheduler\\n * preemption and signal delivery. Typically used for implementing\\n * per-cpu operations.\\n *\\n * It allows user-space to perform update operations on per-cpu data\\n * without requiring heavy-weight atomic operations.\\n *\\n * Detailed algorithm of rseq user-space assembly sequences:\\n *\\n *                     init(rseq_cs)\\n *                     cpu = TLS->rseq::cpu_id_start\\n *   [1]               TLS->rseq::rseq_cs = rseq_cs\\n *   [start_ip]        ----------------------------\\n *   [2]               if (cpu != TLS->rseq::cpu_id)\\n *                             goto abort_ip;\\n *   [3]               <last_instruction_in_cs>\\n *   [post_commit_ip]  ----------------------------\\n *\\n *   The address of jump target abort_ip must be outside the critical\\n *   region, i.e.:\\n *\\n *     [abort_ip] < [start_ip]  || [abort_ip] >= [post_commit_ip]\\n *\\n *   Steps [2]-[3] (inclusive) need to be a sequence of instructions in\\n *   userspace that can handle being interrupted between any of those\\n *   instructions, and then resumed to the abort_ip.\\n *\\n *   1.  Userspace stores the address of the struct rseq_cs assembly\\n *       block descriptor into the rseq_cs field of the registered\\n *       struct rseq TLS area. This update is performed through a single\\n *       store within the inline assembly instruction sequence.\\n *       [start_ip]\\n *\\n *   2.  Userspace tests to check whether the current cpu_id field match\\n *       the cpu number loaded before start_ip, branching to abort_ip\\n *       in case of a mismatch.\\n *\\n *       If the sequence is preempted or interrupted by a signal\\n *       at or after start_ip and before post_commit_ip, then the kernel\\n *       clears TLS->__rseq_abi::rseq_cs, and sets the user-space return\\n *       ip to abort_ip before returning to user-space, so the preempted\\n *       execution resumes at abort_ip.\\n *\\n *   3.  Userspace critical section final instruction before\\n *       post_commit_ip is the commit. The critical section is\\n *       self-terminating.\\n *       [post_commit_ip]\\n *\\n *   4.  <success>\\n *\\n *   On failure at [2], or if interrupted by preempt or signal delivery\\n *   between [1] and [3]:\\n *\\n *       [abort_ip]\\n *   F1. <failure>\\n */\\n\\nstatic int rseq_update_cpu_id(struct task_struct *t)\\n{\\n\\tu32 cpu_id = raw_smp_processor_id();\\n\\n\\tif (put_user(cpu_id, &t->rseq->cpu_id_start))\\n\\t\\treturn -EFAULT;\\n\\tif (put_user(cpu_id, &t->rseq->cpu_id))\\n\\t\\treturn -EFAULT;\\n\\ttrace_rseq_update(t);\\n\\treturn 0;\\n}\\n\\nstatic int rseq_reset_rseq_cpu_id(struct task_struct *t)\\n{\\n\\tu32 cpu_id_start = 0, cpu_id = RSEQ_CPU_ID_UNINITIALIZED;\\n\\n\\t/*\\n\\t * Reset cpu_id_start to its initial state (0).\\n\\t */\\n\\tif (put_user(cpu_id_start, &t->rseq->cpu_id_start))\\n\\t\\treturn -EFAULT;\\n\\t/*\\n\\t * Reset cpu_id to RSEQ_CPU_ID_UNINITIALIZED, so any user coming\\n\\t * in after unregistration can figure out that rseq needs to be\\n\\t * registered again.\\n\\t */\\n\\tif (put_user(cpu_id, &t->rseq->cpu_id))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n\\nstatic int rseq_get_rseq_cs(struct task_struct *t, struct rseq_cs *rseq_cs)\\n{\\n\\tstruct rseq_cs __user *urseq_cs;\\n\\tu64 ptr;\\n\\tu32 __user *usig;\\n\\tu32 sig;\\n\\tint ret;\\n\\n\\tif (copy_from_user(&ptr, &t->rseq->rseq_cs.ptr64, sizeof(ptr)))\\n\\t\\treturn -EFAULT;\\n\\tif (!ptr) {\\n\\t\\tmemset(rseq_cs, 0, sizeof(*rseq_cs));\\n\\t\\treturn 0;\\n\\t}\\n\\tif (ptr >= TASK_SIZE)\\n\\t\\treturn -EINVAL;\\n\\turseq_cs = (struct rseq_cs __user *)(unsigned long)ptr;\\n\\tif (copy_from_user(rseq_cs, urseq_cs, sizeof(*rseq_cs)))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (rseq_cs->start_ip >= TASK_SIZE ||\\n\\t    rseq_cs->start_ip + rseq_cs->post_commit_offset >= TASK_SIZE ||\\n\\t    rseq_cs->abort_ip >= TASK_SIZE ||\\n\\t    rseq_cs->version > 0)\\n\\t\\treturn -EINVAL;\\n\\t/* Check for overflow. */\\n\\tif (rseq_cs->start_ip + rseq_cs->post_commit_offset < rseq_cs->start_ip)\\n\\t\\treturn -EINVAL;\\n\\t/* Ensure that abort_ip is not in the critical section. */\\n\\tif (rseq_cs->abort_ip - rseq_cs->start_ip < rseq_cs->post_commit_offset)\\n\\t\\treturn -EINVAL;\\n\\n\\tusig = (u32 __user *)(unsigned long)(rseq_cs->abort_ip - sizeof(u32));\\n\\tret = get_user(sig, usig);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tif (current->rseq_sig != sig) {\\n\\t\\tprintk_ratelimited(KERN_WARNING\\n\\t\\t\\t\"Possible attack attempt. Unexpected rseq signature 0x%x, expecting 0x%x (pid=%d, addr=%p).\\\\n\",\\n\\t\\t\\tsig, current->rseq_sig, current->pid, usig);\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int rseq_need_restart(struct task_struct *t, u32 cs_flags)\\n{\\n\\tu32 flags, event_mask;\\n\\tint ret;\\n\\n\\t/* Get thread flags. */\\n\\tret = get_user(flags, &t->rseq->flags);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\t/* Take critical section flags into account. */\\n\\tflags |= cs_flags;\\n\\n\\t/*\\n\\t * Restart on signal can only be inhibited when restart on\\n\\t * preempt and restart on migrate are inhibited too. Otherwise,\\n\\t * a preempted signal handler could fail to restart the prior\\n\\t * execution context on sigreturn.\\n\\t */\\n\\tif (unlikely((flags & RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL) &&\\n\\t\\t     (flags & RSEQ_CS_PREEMPT_MIGRATE_FLAGS) !=\\n\\t\\t     RSEQ_CS_PREEMPT_MIGRATE_FLAGS))\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * Load and clear event mask atomically with respect to\\n\\t * scheduler preemption.\\n\\t */\\n\\tpreempt_disable();\\n\\tevent_mask = t->rseq_event_mask;\\n\\tt->rseq_event_mask = 0;\\n\\tpreempt_enable();\\n\\n\\treturn !!(event_mask & ~flags);\\n}\\n\\nstatic int clear_rseq_cs(struct task_struct *t)\\n{\\n\\t/*\\n\\t * The rseq_cs field is set to NULL on preemption or signal\\n\\t * delivery on top of rseq assembly block, as well as on top\\n\\t * of code outside of the rseq assembly block. This performs\\n\\t * a lazy clear of the rseq_cs field.\\n\\t *\\n\\t * Set rseq_cs to NULL.\\n\\t */\\n\\tif (clear_user(&t->rseq->rseq_cs.ptr64, sizeof(t->rseq->rseq_cs.ptr64)))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n\\n/*\\n * Unsigned comparison will be true when ip >= start_ip, and when\\n * ip < start_ip + post_commit_offset.\\n */\\nstatic bool in_rseq_cs(unsigned long ip, struct rseq_cs *rseq_cs)\\n{\\n\\treturn ip - rseq_cs->start_ip < rseq_cs->post_commit_offset;\\n}\\n\\nstatic int rseq_ip_fixup(struct pt_regs *regs)\\n{\\n\\tunsigned long ip = instruction_pointer(regs);\\n\\tstruct task_struct *t = current;\\n\\tstruct rseq_cs rseq_cs;\\n\\tint ret;\\n\\n\\tret = rseq_get_rseq_cs(t, &rseq_cs);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\t/*\\n\\t * Handle potentially not being within a critical section.\\n\\t * If not nested over a rseq critical section, restart is useless.\\n\\t * Clear the rseq_cs pointer and return.\\n\\t */\\n\\tif (!in_rseq_cs(ip, &rseq_cs))\\n\\t\\treturn clear_rseq_cs(t);\\n\\tret = rseq_need_restart(t, rseq_cs.flags);\\n\\tif (ret <= 0)\\n\\t\\treturn ret;\\n\\tret = clear_rseq_cs(t);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\ttrace_rseq_ip_fixup(ip, rseq_cs.start_ip, rseq_cs.post_commit_offset,\\n\\t\\t\\t    rseq_cs.abort_ip);\\n\\tinstruction_pointer_set(regs, (unsigned long)rseq_cs.abort_ip);\\n\\treturn 0;\\n}\\n\\n/*\\n * This resume handler must always be executed between any of:\\n * - preemption,\\n * - signal delivery,\\n * and return to user-space.\\n *\\n * This is how we can ensure that the entire rseq critical section,\\n * consisting of both the C part and the assembly instruction sequence,\\n * will issue the commit instruction only if executed atomically with\\n * respect to other threads scheduled on the same CPU, and with respect\\n * to signal handlers.\\n */\\nvoid __rseq_handle_notify_resume(struct ksignal *ksig, struct pt_regs *regs)\\n{\\n\\tstruct task_struct *t = current;\\n\\tint ret, sig;\\n\\n\\tif (unlikely(t->flags & PF_EXITING))\\n\\t\\treturn;\\n\\tif (unlikely(!access_ok(t->rseq, sizeof(*t->rseq))))\\n\\t\\tgoto error;\\n\\tret = rseq_ip_fixup(regs);\\n\\tif (unlikely(ret < 0))\\n\\t\\tgoto error;\\n\\tif (unlikely(rseq_update_cpu_id(t)))\\n\\t\\tgoto error;\\n\\treturn;\\n\\nerror:\\n\\tsig = ksig ? ksig->sig : 0;\\n\\tforce_sigsegv(sig, t);\\n}\\n\\n#ifdef CONFIG_DEBUG_RSEQ\\n\\n/*\\n * Terminate the process if a syscall is issued within a restartable\\n * sequence.\\n */\\nvoid rseq_syscall(struct pt_regs *regs)\\n{\\n\\tunsigned long ip = instruction_pointer(regs);\\n\\tstruct task_struct *t = current;\\n\\tstruct rseq_cs rseq_cs;\\n\\n\\tif (!t->rseq)\\n\\t\\treturn;\\n\\tif (!access_ok(t->rseq, sizeof(*t->rseq)) ||\\n\\t    rseq_get_rseq_cs(t, &rseq_cs) || in_rseq_cs(ip, &rseq_cs))\\n\\t\\tforce_sig(SIGSEGV, t);\\n}\\n\\n#endif\\n\\n/*\\n * sys_rseq - setup restartable sequences for caller thread.\\n */\\nSYSCALL_DEFINE4(rseq, struct rseq __user *, rseq, u32, rseq_len,\\n\\t\\tint, flags, u32, sig)\\n{\\n\\tint ret;\\n\\n\\tif (flags & RSEQ_FLAG_UNREGISTER) {\\n\\t\\t/* Unregister rseq for current thread. */\\n\\t\\tif (current->rseq != rseq || !current->rseq)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (current->rseq_len != rseq_len)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (current->rseq_sig != sig)\\n\\t\\t\\treturn -EPERM;\\n\\t\\tret = rseq_reset_rseq_cpu_id(current);\\n\\t\\tif (ret)\\n\\t\\t\\treturn ret;\\n\\t\\tcurrent->rseq = NULL;\\n\\t\\tcurrent->rseq_len = 0;\\n\\t\\tcurrent->rseq_sig = 0;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tif (unlikely(flags))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (current->rseq) {\\n\\t\\t/*\\n\\t\\t * If rseq is already registered, check whether\\n\\t\\t * the provided address differs from the prior\\n\\t\\t * one.\\n\\t\\t */\\n\\t\\tif (current->rseq != rseq || current->rseq_len != rseq_len)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (current->rseq_sig != sig)\\n\\t\\t\\treturn -EPERM;\\n\\t\\t/* Already registered. */\\n\\t\\treturn -EBUSY;\\n\\t}\\n\\n\\t/*\\n\\t * If there was no rseq previously registered,\\n\\t * ensure the provided rseq is properly aligned and valid.\\n\\t */\\n\\tif (!IS_ALIGNED((unsigned long)rseq, __alignof__(*rseq)) ||\\n\\t    rseq_len != sizeof(*rseq))\\n\\t\\treturn -EINVAL;\\n\\tif (!access_ok(rseq, rseq_len))\\n\\t\\treturn -EFAULT;\\n\\tcurrent->rseq = rseq;\\n\\tcurrent->rseq_len = rseq_len;\\n\\tcurrent->rseq_sig = sig;\\n\\t/*\\n\\t * If rseq was previously inactive, and has just been\\n\\t * registered, ensure the cpu_id_start and cpu_id fields\\n\\t * are updated before returning to user-space.\\n\\t */\\n\\trseq_set_notify_resume(current);\\n\\n\\treturn 0;\\n}\\n\\n/* Module signature checker\\n *\\n * Copyright (C) 2012 Red Hat, Inc. All Rights Reserved.\\n * Written by David Howells (dhowells@redhat.com)\\n *\\n * This program is free software; you can redistribute it and/or\\n * modify it under the terms of the GNU General Public Licence\\n * as published by the Free Software Foundation; either version\\n * 2 of the Licence, or (at your option) any later version.\\n */\\n\\n#include <linux/kernel.h>\\n#include <linux/errno.h>\\n#include <linux/string.h>\\n#include <linux/verification.h>\\n#include <crypto/public_key.h>\\n#include \"module-internal.h\"\\n\\nenum pkey_id_type {\\n\\tPKEY_ID_PGP,\\t\\t/* OpenPGP generated key ID */\\n\\tPKEY_ID_X509,\\t\\t/* X.509 arbitrary subjectKeyIdentifier */\\n\\tPKEY_ID_PKCS7,\\t\\t/* Signature in PKCS#7 message */\\n};\\n\\n/*\\n * Module signature information block.\\n *\\n * The constituents of the signature section are, in order:\\n *\\n *\\t- Signer\\'s name\\n *\\t- Key identifier\\n *\\t- Signature data\\n *\\t- Information block\\n */\\nstruct module_signature {\\n\\tu8\\talgo;\\t\\t/* Public-key crypto algorithm [0] */\\n\\tu8\\thash;\\t\\t/* Digest algorithm [0] */\\n\\tu8\\tid_type;\\t/* Key identifier type [PKEY_ID_PKCS7] */\\n\\tu8\\tsigner_len;\\t/* Length of signer\\'s name [0] */\\n\\tu8\\tkey_id_len;\\t/* Length of key identifier [0] */\\n\\tu8\\t__pad[3];\\n\\t__be32\\tsig_len;\\t/* Length of signature data */\\n};\\n\\n/*\\n * Verify the signature on a module.\\n */\\nint mod_verify_sig(const void *mod, struct load_info *info)\\n{\\n\\tstruct module_signature ms;\\n\\tsize_t sig_len, modlen = info->len;\\n\\n\\tpr_devel(\"==>%s(,%zu)\\\\n\", __func__, modlen);\\n\\n\\tif (modlen <= sizeof(ms))\\n\\t\\treturn -EBADMSG;\\n\\n\\tmemcpy(&ms, mod + (modlen - sizeof(ms)), sizeof(ms));\\n\\tmodlen -= sizeof(ms);\\n\\n\\tsig_len = be32_to_cpu(ms.sig_len);\\n\\tif (sig_len >= modlen)\\n\\t\\treturn -EBADMSG;\\n\\tmodlen -= sig_len;\\n\\tinfo->len = modlen;\\n\\n\\tif (ms.id_type != PKEY_ID_PKCS7) {\\n\\t\\tpr_err(\"%s: Module is not signed with expected PKCS#7 message\\\\n\",\\n\\t\\t       info->name);\\n\\t\\treturn -ENOPKG;\\n\\t}\\n\\n\\tif (ms.algo != 0 ||\\n\\t    ms.hash != 0 ||\\n\\t    ms.signer_len != 0 ||\\n\\t    ms.key_id_len != 0 ||\\n\\t    ms.__pad[0] != 0 ||\\n\\t    ms.__pad[1] != 0 ||\\n\\t    ms.__pad[2] != 0) {\\n\\t\\tpr_err(\"%s: PKCS#7 signature info has unexpected non-zero params\\\\n\",\\n\\t\\t       info->name);\\n\\t\\treturn -EBADMSG;\\n\\t}\\n\\n\\treturn verify_pkcs7_signature(mod, modlen, mod + modlen, sig_len,\\n\\t\\t\\t\\t      VERIFY_USE_SECONDARY_KEYRING,\\n\\t\\t\\t\\t      VERIFYING_MODULE_SIGNATURE,\\n\\t\\t\\t\\t      NULL, NULL);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * Handling of different ABIs (personalities).\\n *\\n * We group personalities into execution domains which have their\\n * own handlers for kernel entry points, signal mapping, etc...\\n *\\n * 2001-05-06\\tComplete rewrite,  Christoph Hellwig (hch@infradead.org)\\n */\\n\\n#include <linux/init.h>\\n#include <linux/kernel.h>\\n#include <linux/kmod.h>\\n#include <linux/module.h>\\n#include <linux/personality.h>\\n#include <linux/proc_fs.h>\\n#include <linux/sched.h>\\n#include <linux/seq_file.h>\\n#include <linux/syscalls.h>\\n#include <linux/sysctl.h>\\n#include <linux/types.h>\\n\\n#ifdef CONFIG_PROC_FS\\nstatic int execdomains_proc_show(struct seq_file *m, void *v)\\n{\\n\\tseq_puts(m, \"0-0\\\\tLinux           \\\\t[kernel]\\\\n\");\\n\\treturn 0;\\n}\\n\\nstatic int __init proc_execdomains_init(void)\\n{\\n\\tproc_create_single(\"execdomains\", 0, NULL, execdomains_proc_show);\\n\\treturn 0;\\n}\\nmodule_init(proc_execdomains_init);\\n#endif\\n\\nSYSCALL_DEFINE1(personality, unsigned int, personality)\\n{\\n\\tunsigned int old = current->personality;\\n\\n\\tif (personality != 0xffffffff)\\n\\t\\tset_personality(personality);\\n\\n\\treturn old;\\n}\\n\\n/*\\n * Common SMP CPU bringup/teardown functions\\n */\\n#include <linux/cpu.h>\\n#include <linux/err.h>\\n#include <linux/smp.h>\\n#include <linux/delay.h>\\n#include <linux/init.h>\\n#include <linux/list.h>\\n#include <linux/slab.h>\\n#include <linux/sched.h>\\n#include <linux/sched/task.h>\\n#include <linux/export.h>\\n#include <linux/percpu.h>\\n#include <linux/kthread.h>\\n#include <linux/smpboot.h>\\n\\n#include \"smpboot.h\"\\n\\n#ifdef CONFIG_SMP\\n\\n#ifdef CONFIG_GENERIC_SMP_IDLE_THREAD\\n/*\\n * For the hotplug case we keep the task structs around and reuse\\n * them.\\n */\\nstatic DEFINE_PER_CPU(struct task_struct *, idle_threads);\\n\\nstruct task_struct *idle_thread_get(unsigned int cpu)\\n{\\n\\tstruct task_struct *tsk = per_cpu(idle_threads, cpu);\\n\\n\\tif (!tsk)\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\tinit_idle(tsk, cpu);\\n\\treturn tsk;\\n}\\n\\nvoid __init idle_thread_set_boot_cpu(void)\\n{\\n\\tper_cpu(idle_threads, smp_processor_id()) = current;\\n}\\n\\n/**\\n * idle_init - Initialize the idle thread for a cpu\\n * @cpu:\\tThe cpu for which the idle thread should be initialized\\n *\\n * Creates the thread if it does not exist.\\n */\\nstatic inline void idle_init(unsigned int cpu)\\n{\\n\\tstruct task_struct *tsk = per_cpu(idle_threads, cpu);\\n\\n\\tif (!tsk) {\\n\\t\\ttsk = fork_idle(cpu);\\n\\t\\tif (IS_ERR(tsk))\\n\\t\\t\\tpr_err(\"SMP: fork_idle() failed for CPU %u\\\\n\", cpu);\\n\\t\\telse\\n\\t\\t\\tper_cpu(idle_threads, cpu) = tsk;\\n\\t}\\n}\\n\\n/**\\n * idle_threads_init - Initialize idle threads for all cpus\\n */\\nvoid __init idle_threads_init(void)\\n{\\n\\tunsigned int cpu, boot_cpu;\\n\\n\\tboot_cpu = smp_processor_id();\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tif (cpu != boot_cpu)\\n\\t\\t\\tidle_init(cpu);\\n\\t}\\n}\\n#endif\\n\\n#endif /* #ifdef CONFIG_SMP */\\n\\nstatic LIST_HEAD(hotplug_threads);\\nstatic DEFINE_MUTEX(smpboot_threads_lock);\\n\\nstruct smpboot_thread_data {\\n\\tunsigned int\\t\\t\\tcpu;\\n\\tunsigned int\\t\\t\\tstatus;\\n\\tstruct smp_hotplug_thread\\t*ht;\\n};\\n\\nenum {\\n\\tHP_THREAD_NONE = 0,\\n\\tHP_THREAD_ACTIVE,\\n\\tHP_THREAD_PARKED,\\n};\\n\\n/**\\n * smpboot_thread_fn - percpu hotplug thread loop function\\n * @data:\\tthread data pointer\\n *\\n * Checks for thread stop and park conditions. Calls the necessary\\n * setup, cleanup, park and unpark functions for the registered\\n * thread.\\n *\\n * Returns 1 when the thread should exit, 0 otherwise.\\n */\\nstatic int smpboot_thread_fn(void *data)\\n{\\n\\tstruct smpboot_thread_data *td = data;\\n\\tstruct smp_hotplug_thread *ht = td->ht;\\n\\n\\twhile (1) {\\n\\t\\tset_current_state(TASK_INTERRUPTIBLE);\\n\\t\\tpreempt_disable();\\n\\t\\tif (kthread_should_stop()) {\\n\\t\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\t\\tpreempt_enable();\\n\\t\\t\\t/* cleanup must mirror setup */\\n\\t\\t\\tif (ht->cleanup && td->status != HP_THREAD_NONE)\\n\\t\\t\\t\\tht->cleanup(td->cpu, cpu_online(td->cpu));\\n\\t\\t\\tkfree(td);\\n\\t\\t\\treturn 0;\\n\\t\\t}\\n\\n\\t\\tif (kthread_should_park()) {\\n\\t\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\t\\tpreempt_enable();\\n\\t\\t\\tif (ht->park && td->status == HP_THREAD_ACTIVE) {\\n\\t\\t\\t\\tBUG_ON(td->cpu != smp_processor_id());\\n\\t\\t\\t\\tht->park(td->cpu);\\n\\t\\t\\t\\ttd->status = HP_THREAD_PARKED;\\n\\t\\t\\t}\\n\\t\\t\\tkthread_parkme();\\n\\t\\t\\t/* We might have been woken for stop */\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\tBUG_ON(td->cpu != smp_processor_id());\\n\\n\\t\\t/* Check for state change setup */\\n\\t\\tswitch (td->status) {\\n\\t\\tcase HP_THREAD_NONE:\\n\\t\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\t\\tpreempt_enable();\\n\\t\\t\\tif (ht->setup)\\n\\t\\t\\t\\tht->setup(td->cpu);\\n\\t\\t\\ttd->status = HP_THREAD_ACTIVE;\\n\\t\\t\\tcontinue;\\n\\n\\t\\tcase HP_THREAD_PARKED:\\n\\t\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\t\\tpreempt_enable();\\n\\t\\t\\tif (ht->unpark)\\n\\t\\t\\t\\tht->unpark(td->cpu);\\n\\t\\t\\ttd->status = HP_THREAD_ACTIVE;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\tif (!ht->thread_should_run(td->cpu)) {\\n\\t\\t\\tpreempt_enable_no_resched();\\n\\t\\t\\tschedule();\\n\\t\\t} else {\\n\\t\\t\\t__set_current_state(TASK_RUNNING);\\n\\t\\t\\tpreempt_enable();\\n\\t\\t\\tht->thread_fn(td->cpu);\\n\\t\\t}\\n\\t}\\n}\\n\\nstatic int\\n__smpboot_create_thread(struct smp_hotplug_thread *ht, unsigned int cpu)\\n{\\n\\tstruct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);\\n\\tstruct smpboot_thread_data *td;\\n\\n\\tif (tsk)\\n\\t\\treturn 0;\\n\\n\\ttd = kzalloc_node(sizeof(*td), GFP_KERNEL, cpu_to_node(cpu));\\n\\tif (!td)\\n\\t\\treturn -ENOMEM;\\n\\ttd->cpu = cpu;\\n\\ttd->ht = ht;\\n\\n\\ttsk = kthread_create_on_cpu(smpboot_thread_fn, td, cpu,\\n\\t\\t\\t\\t    ht->thread_comm);\\n\\tif (IS_ERR(tsk)) {\\n\\t\\tkfree(td);\\n\\t\\treturn PTR_ERR(tsk);\\n\\t}\\n\\t/*\\n\\t * Park the thread so that it could start right on the CPU\\n\\t * when it is available.\\n\\t */\\n\\tkthread_park(tsk);\\n\\tget_task_struct(tsk);\\n\\t*per_cpu_ptr(ht->store, cpu) = tsk;\\n\\tif (ht->create) {\\n\\t\\t/*\\n\\t\\t * Make sure that the task has actually scheduled out\\n\\t\\t * into park position, before calling the create\\n\\t\\t * callback. At least the migration thread callback\\n\\t\\t * requires that the task is off the runqueue.\\n\\t\\t */\\n\\t\\tif (!wait_task_inactive(tsk, TASK_PARKED))\\n\\t\\t\\tWARN_ON(1);\\n\\t\\telse\\n\\t\\t\\tht->create(cpu);\\n\\t}\\n\\treturn 0;\\n}\\n\\nint smpboot_create_threads(unsigned int cpu)\\n{\\n\\tstruct smp_hotplug_thread *cur;\\n\\tint ret = 0;\\n\\n\\tmutex_lock(&smpboot_threads_lock);\\n\\tlist_for_each_entry(cur, &hotplug_threads, list) {\\n\\t\\tret = __smpboot_create_thread(cur, cpu);\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\t}\\n\\tmutex_unlock(&smpboot_threads_lock);\\n\\treturn ret;\\n}\\n\\nstatic void smpboot_unpark_thread(struct smp_hotplug_thread *ht, unsigned int cpu)\\n{\\n\\tstruct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);\\n\\n\\tif (!ht->selfparking)\\n\\t\\tkthread_unpark(tsk);\\n}\\n\\nint smpboot_unpark_threads(unsigned int cpu)\\n{\\n\\tstruct smp_hotplug_thread *cur;\\n\\n\\tmutex_lock(&smpboot_threads_lock);\\n\\tlist_for_each_entry(cur, &hotplug_threads, list)\\n\\t\\tsmpboot_unpark_thread(cur, cpu);\\n\\tmutex_unlock(&smpboot_threads_lock);\\n\\treturn 0;\\n}\\n\\nstatic void smpboot_park_thread(struct smp_hotplug_thread *ht, unsigned int cpu)\\n{\\n\\tstruct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);\\n\\n\\tif (tsk && !ht->selfparking)\\n\\t\\tkthread_park(tsk);\\n}\\n\\nint smpboot_park_threads(unsigned int cpu)\\n{\\n\\tstruct smp_hotplug_thread *cur;\\n\\n\\tmutex_lock(&smpboot_threads_lock);\\n\\tlist_for_each_entry_reverse(cur, &hotplug_threads, list)\\n\\t\\tsmpboot_park_thread(cur, cpu);\\n\\tmutex_unlock(&smpboot_threads_lock);\\n\\treturn 0;\\n}\\n\\nstatic void smpboot_destroy_threads(struct smp_hotplug_thread *ht)\\n{\\n\\tunsigned int cpu;\\n\\n\\t/* We need to destroy also the parked threads of offline cpus */\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tstruct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);\\n\\n\\t\\tif (tsk) {\\n\\t\\t\\tkthread_stop(tsk);\\n\\t\\t\\tput_task_struct(tsk);\\n\\t\\t\\t*per_cpu_ptr(ht->store, cpu) = NULL;\\n\\t\\t}\\n\\t}\\n}\\n\\n/**\\n * smpboot_register_percpu_thread - Register a per_cpu thread related\\n * \\t\\t\\t\\t\\t    to hotplug\\n * @plug_thread:\\tHotplug thread descriptor\\n *\\n * Creates and starts the threads on all online cpus.\\n */\\nint smpboot_register_percpu_thread(struct smp_hotplug_thread *plug_thread)\\n{\\n\\tunsigned int cpu;\\n\\tint ret = 0;\\n\\n\\tget_online_cpus();\\n\\tmutex_lock(&smpboot_threads_lock);\\n\\tfor_each_online_cpu(cpu) {\\n\\t\\tret = __smpboot_create_thread(plug_thread, cpu);\\n\\t\\tif (ret) {\\n\\t\\t\\tsmpboot_destroy_threads(plug_thread);\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t\\tsmpboot_unpark_thread(plug_thread, cpu);\\n\\t}\\n\\tlist_add(&plug_thread->list, &hotplug_threads);\\nout:\\n\\tmutex_unlock(&smpboot_threads_lock);\\n\\tput_online_cpus();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(smpboot_register_percpu_thread);\\n\\n/**\\n * smpboot_unregister_percpu_thread - Unregister a per_cpu thread related to hotplug\\n * @plug_thread:\\tHotplug thread descriptor\\n *\\n * Stops all threads on all possible cpus.\\n */\\nvoid smpboot_unregister_percpu_thread(struct smp_hotplug_thread *plug_thread)\\n{\\n\\tget_online_cpus();\\n\\tmutex_lock(&smpboot_threads_lock);\\n\\tlist_del(&plug_thread->list);\\n\\tsmpboot_destroy_threads(plug_thread);\\n\\tmutex_unlock(&smpboot_threads_lock);\\n\\tput_online_cpus();\\n}\\nEXPORT_SYMBOL_GPL(smpboot_unregister_percpu_thread);\\n\\nstatic DEFINE_PER_CPU(atomic_t, cpu_hotplug_state) = ATOMIC_INIT(CPU_POST_DEAD);\\n\\n/*\\n * Called to poll specified CPU\\'s state, for example, when waiting for\\n * a CPU to come online.\\n */\\nint cpu_report_state(int cpu)\\n{\\n\\treturn atomic_read(&per_cpu(cpu_hotplug_state, cpu));\\n}\\n\\n/*\\n * If CPU has died properly, set its state to CPU_UP_PREPARE and\\n * return success.  Otherwise, return -EBUSY if the CPU died after\\n * cpu_wait_death() timed out.  And yet otherwise again, return -EAGAIN\\n * if cpu_wait_death() timed out and the CPU still hasn\\'t gotten around\\n * to dying.  In the latter two cases, the CPU might not be set up\\n * properly, but it is up to the arch-specific code to decide.\\n * Finally, -EIO indicates an unanticipated problem.\\n *\\n * Note that it is permissible to omit this call entirely, as is\\n * done in architectures that do no CPU-hotplug error checking.\\n */\\nint cpu_check_up_prepare(int cpu)\\n{\\n\\tif (!IS_ENABLED(CONFIG_HOTPLUG_CPU)) {\\n\\t\\tatomic_set(&per_cpu(cpu_hotplug_state, cpu), CPU_UP_PREPARE);\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tswitch (atomic_read(&per_cpu(cpu_hotplug_state, cpu))) {\\n\\n\\tcase CPU_POST_DEAD:\\n\\n\\t\\t/* The CPU died properly, so just start it up again. */\\n\\t\\tatomic_set(&per_cpu(cpu_hotplug_state, cpu), CPU_UP_PREPARE);\\n\\t\\treturn 0;\\n\\n\\tcase CPU_DEAD_FROZEN:\\n\\n\\t\\t/*\\n\\t\\t * Timeout during CPU death, so let caller know.\\n\\t\\t * The outgoing CPU completed its processing, but after\\n\\t\\t * cpu_wait_death() timed out and reported the error. The\\n\\t\\t * caller is free to proceed, in which case the state\\n\\t\\t * will be reset properly by cpu_set_state_online().\\n\\t\\t * Proceeding despite this -EBUSY return makes sense\\n\\t\\t * for systems where the outgoing CPUs take themselves\\n\\t\\t * offline, with no post-death manipulation required from\\n\\t\\t * a surviving CPU.\\n\\t\\t */\\n\\t\\treturn -EBUSY;\\n\\n\\tcase CPU_BROKEN:\\n\\n\\t\\t/*\\n\\t\\t * The most likely reason we got here is that there was\\n\\t\\t * a timeout during CPU death, and the outgoing CPU never\\n\\t\\t * did complete its processing.  This could happen on\\n\\t\\t * a virtualized system if the outgoing VCPU gets preempted\\n\\t\\t * for more than five seconds, and the user attempts to\\n\\t\\t * immediately online that same CPU.  Trying again later\\n\\t\\t * might return -EBUSY above, hence -EAGAIN.\\n\\t\\t */\\n\\t\\treturn -EAGAIN;\\n\\n\\tdefault:\\n\\n\\t\\t/* Should not happen.  Famous last words. */\\n\\t\\treturn -EIO;\\n\\t}\\n}\\n\\n/*\\n * Mark the specified CPU online.\\n *\\n * Note that it is permissible to omit this call entirely, as is\\n * done in architectures that do no CPU-hotplug error checking.\\n */\\nvoid cpu_set_state_online(int cpu)\\n{\\n\\t(void)atomic_xchg(&per_cpu(cpu_hotplug_state, cpu), CPU_ONLINE);\\n}\\n\\n#ifdef CONFIG_HOTPLUG_CPU\\n\\n/*\\n * Wait for the specified CPU to exit the idle loop and die.\\n */\\nbool cpu_wait_death(unsigned int cpu, int seconds)\\n{\\n\\tint jf_left = seconds * HZ;\\n\\tint oldstate;\\n\\tbool ret = true;\\n\\tint sleep_jf = 1;\\n\\n\\tmight_sleep();\\n\\n\\t/* The outgoing CPU will normally get done quite quickly. */\\n\\tif (atomic_read(&per_cpu(cpu_hotplug_state, cpu)) == CPU_DEAD)\\n\\t\\tgoto update_state;\\n\\tudelay(5);\\n\\n\\t/* But if the outgoing CPU dawdles, wait increasingly long times. */\\n\\twhile (atomic_read(&per_cpu(cpu_hotplug_state, cpu)) != CPU_DEAD) {\\n\\t\\tschedule_timeout_uninterruptible(sleep_jf);\\n\\t\\tjf_left -= sleep_jf;\\n\\t\\tif (jf_left <= 0)\\n\\t\\t\\tbreak;\\n\\t\\tsleep_jf = DIV_ROUND_UP(sleep_jf * 11, 10);\\n\\t}\\nupdate_state:\\n\\toldstate = atomic_read(&per_cpu(cpu_hotplug_state, cpu));\\n\\tif (oldstate == CPU_DEAD) {\\n\\t\\t/* Outgoing CPU died normally, update state. */\\n\\t\\tsmp_mb(); /* atomic_read() before update. */\\n\\t\\tatomic_set(&per_cpu(cpu_hotplug_state, cpu), CPU_POST_DEAD);\\n\\t} else {\\n\\t\\t/* Outgoing CPU still hasn\\'t died, set state accordingly. */\\n\\t\\tif (atomic_cmpxchg(&per_cpu(cpu_hotplug_state, cpu),\\n\\t\\t\\t\\t   oldstate, CPU_BROKEN) != oldstate)\\n\\t\\t\\tgoto update_state;\\n\\t\\tret = false;\\n\\t}\\n\\treturn ret;\\n}\\n\\n/*\\n * Called by the outgoing CPU to report its successful death.  Return\\n * false if this report follows the surviving CPU\\'s timing out.\\n *\\n * A separate \"CPU_DEAD_FROZEN\" is used when the surviving CPU\\n * timed out.  This approach allows architectures to omit calls to\\n * cpu_check_up_prepare() and cpu_set_state_online() without defeating\\n * the next cpu_wait_death()\\'s polling loop.\\n */\\nbool cpu_report_death(void)\\n{\\n\\tint oldstate;\\n\\tint newstate;\\n\\tint cpu = smp_processor_id();\\n\\n\\tdo {\\n\\t\\toldstate = atomic_read(&per_cpu(cpu_hotplug_state, cpu));\\n\\t\\tif (oldstate != CPU_BROKEN)\\n\\t\\t\\tnewstate = CPU_DEAD;\\n\\t\\telse\\n\\t\\t\\tnewstate = CPU_DEAD_FROZEN;\\n\\t} while (atomic_cmpxchg(&per_cpu(cpu_hotplug_state, cpu),\\n\\t\\t\\t\\toldstate, newstate) != oldstate);\\n\\treturn newstate == CPU_DEAD;\\n}\\n\\n#endif /* #ifdef CONFIG_HOTPLUG_CPU */\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n * Generate definitions needed by the preprocessor.\\n * This code generates raw asm output which is post-processed\\n * to extract and format the required data.\\n */\\n\\n#define __GENERATING_BOUNDS_H\\n/* Include headers that define the enum constants of interest */\\n#include <linux/page-flags.h>\\n#include <linux/mmzone.h>\\n#include <linux/kbuild.h>\\n#include <linux/log2.h>\\n#include <linux/spinlock_types.h>\\n\\nint main(void)\\n{\\n\\t/* The enum constants to put into include/generated/bounds.h */\\n\\tDEFINE(NR_PAGEFLAGS, __NR_PAGEFLAGS);\\n\\tDEFINE(MAX_NR_ZONES, __MAX_NR_ZONES);\\n#ifdef CONFIG_SMP\\n\\tDEFINE(NR_CPUS_BITS, ilog2(CONFIG_NR_CPUS));\\n#endif\\n\\tDEFINE(SPINLOCK_SIZE, sizeof(spinlock_t));\\n\\t/* End of constants */\\n\\n\\treturn 0;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n/*\\n *  linux/kernel/sys.c\\n *\\n *  Copyright (C) 1991, 1992  Linus Torvalds\\n */\\n\\n#include <linux/export.h>\\n#include <linux/mm.h>\\n#include <linux/utsname.h>\\n#include <linux/mman.h>\\n#include <linux/reboot.h>\\n#include <linux/prctl.h>\\n#include <linux/highuid.h>\\n#include <linux/fs.h>\\n#include <linux/kmod.h>\\n#include <linux/perf_event.h>\\n#include <linux/resource.h>\\n#include <linux/kernel.h>\\n#include <linux/workqueue.h>\\n#include <linux/capability.h>\\n#include <linux/device.h>\\n#include <linux/key.h>\\n#include <linux/times.h>\\n#include <linux/posix-timers.h>\\n#include <linux/security.h>\\n#include <linux/dcookies.h>\\n#include <linux/suspend.h>\\n#include <linux/tty.h>\\n#include <linux/signal.h>\\n#include <linux/cn_proc.h>\\n#include <linux/getcpu.h>\\n#include <linux/task_io_accounting_ops.h>\\n#include <linux/seccomp.h>\\n#include <linux/cpu.h>\\n#include <linux/personality.h>\\n#include <linux/ptrace.h>\\n#include <linux/fs_struct.h>\\n#include <linux/file.h>\\n#include <linux/mount.h>\\n#include <linux/gfp.h>\\n#include <linux/syscore_ops.h>\\n#include <linux/version.h>\\n#include <linux/ctype.h>\\n\\n#include <linux/compat.h>\\n#include <linux/syscalls.h>\\n#include <linux/kprobes.h>\\n#include <linux/user_namespace.h>\\n#include <linux/binfmts.h>\\n\\n#include <linux/sched.h>\\n#include <linux/sched/autogroup.h>\\n#include <linux/sched/loadavg.h>\\n#include <linux/sched/stat.h>\\n#include <linux/sched/mm.h>\\n#include <linux/sched/coredump.h>\\n#include <linux/sched/task.h>\\n#include <linux/sched/cputime.h>\\n#include <linux/rcupdate.h>\\n#include <linux/uidgid.h>\\n#include <linux/cred.h>\\n\\n#include <linux/nospec.h>\\n\\n#include <linux/kmsg_dump.h>\\n/* Move somewhere else to avoid recompiling? */\\n#include <generated/utsrelease.h>\\n\\n#include <linux/uaccess.h>\\n#include <asm/io.h>\\n#include <asm/unistd.h>\\n\\n#include \"uid16.h\"\\n\\n#ifndef SET_UNALIGN_CTL\\n# define SET_UNALIGN_CTL(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef GET_UNALIGN_CTL\\n# define GET_UNALIGN_CTL(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef SET_FPEMU_CTL\\n# define SET_FPEMU_CTL(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef GET_FPEMU_CTL\\n# define GET_FPEMU_CTL(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef SET_FPEXC_CTL\\n# define SET_FPEXC_CTL(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef GET_FPEXC_CTL\\n# define GET_FPEXC_CTL(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef GET_ENDIAN\\n# define GET_ENDIAN(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef SET_ENDIAN\\n# define SET_ENDIAN(a, b)\\t(-EINVAL)\\n#endif\\n#ifndef GET_TSC_CTL\\n# define GET_TSC_CTL(a)\\t\\t(-EINVAL)\\n#endif\\n#ifndef SET_TSC_CTL\\n# define SET_TSC_CTL(a)\\t\\t(-EINVAL)\\n#endif\\n#ifndef MPX_ENABLE_MANAGEMENT\\n# define MPX_ENABLE_MANAGEMENT()\\t(-EINVAL)\\n#endif\\n#ifndef MPX_DISABLE_MANAGEMENT\\n# define MPX_DISABLE_MANAGEMENT()\\t(-EINVAL)\\n#endif\\n#ifndef GET_FP_MODE\\n# define GET_FP_MODE(a)\\t\\t(-EINVAL)\\n#endif\\n#ifndef SET_FP_MODE\\n# define SET_FP_MODE(a,b)\\t(-EINVAL)\\n#endif\\n#ifndef SVE_SET_VL\\n# define SVE_SET_VL(a)\\t\\t(-EINVAL)\\n#endif\\n#ifndef SVE_GET_VL\\n# define SVE_GET_VL()\\t\\t(-EINVAL)\\n#endif\\n#ifndef PAC_RESET_KEYS\\n# define PAC_RESET_KEYS(a, b)\\t(-EINVAL)\\n#endif\\n\\n/*\\n * this is where the system-wide overflow UID and GID are defined, for\\n * architectures that now have 32-bit UID/GID but didn\\'t in the past\\n */\\n\\nint overflowuid = DEFAULT_OVERFLOWUID;\\nint overflowgid = DEFAULT_OVERFLOWGID;\\n\\nEXPORT_SYMBOL(overflowuid);\\nEXPORT_SYMBOL(overflowgid);\\n\\n/*\\n * the same as above, but for filesystems which can only store a 16-bit\\n * UID and GID. as such, this is needed on all architectures\\n */\\n\\nint fs_overflowuid = DEFAULT_FS_OVERFLOWUID;\\nint fs_overflowgid = DEFAULT_FS_OVERFLOWGID;\\n\\nEXPORT_SYMBOL(fs_overflowuid);\\nEXPORT_SYMBOL(fs_overflowgid);\\n\\n/*\\n * Returns true if current\\'s euid is same as p\\'s uid or euid,\\n * or has CAP_SYS_NICE to p\\'s user_ns.\\n *\\n * Called with rcu_read_lock, creds are safe\\n */\\nstatic bool set_one_prio_perm(struct task_struct *p)\\n{\\n\\tconst struct cred *cred = current_cred(), *pcred = __task_cred(p);\\n\\n\\tif (uid_eq(pcred->uid,  cred->euid) ||\\n\\t    uid_eq(pcred->euid, cred->euid))\\n\\t\\treturn true;\\n\\tif (ns_capable(pcred->user_ns, CAP_SYS_NICE))\\n\\t\\treturn true;\\n\\treturn false;\\n}\\n\\n/*\\n * set the priority of a task\\n * - the caller must hold the RCU read lock\\n */\\nstatic int set_one_prio(struct task_struct *p, int niceval, int error)\\n{\\n\\tint no_nice;\\n\\n\\tif (!set_one_prio_perm(p)) {\\n\\t\\terror = -EPERM;\\n\\t\\tgoto out;\\n\\t}\\n\\tif (niceval < task_nice(p) && !can_nice(p, niceval)) {\\n\\t\\terror = -EACCES;\\n\\t\\tgoto out;\\n\\t}\\n\\tno_nice = security_task_setnice(p, niceval);\\n\\tif (no_nice) {\\n\\t\\terror = no_nice;\\n\\t\\tgoto out;\\n\\t}\\n\\tif (error == -ESRCH)\\n\\t\\terror = 0;\\n\\tset_user_nice(p, niceval);\\nout:\\n\\treturn error;\\n}\\n\\nSYSCALL_DEFINE3(setpriority, int, which, int, who, int, niceval)\\n{\\n\\tstruct task_struct *g, *p;\\n\\tstruct user_struct *user;\\n\\tconst struct cred *cred = current_cred();\\n\\tint error = -EINVAL;\\n\\tstruct pid *pgrp;\\n\\tkuid_t uid;\\n\\n\\tif (which > PRIO_USER || which < PRIO_PROCESS)\\n\\t\\tgoto out;\\n\\n\\t/* normalize: avoid signed division (rounding problems) */\\n\\terror = -ESRCH;\\n\\tif (niceval < MIN_NICE)\\n\\t\\tniceval = MIN_NICE;\\n\\tif (niceval > MAX_NICE)\\n\\t\\tniceval = MAX_NICE;\\n\\n\\trcu_read_lock();\\n\\tread_lock(&tasklist_lock);\\n\\tswitch (which) {\\n\\tcase PRIO_PROCESS:\\n\\t\\tif (who)\\n\\t\\t\\tp = find_task_by_vpid(who);\\n\\t\\telse\\n\\t\\t\\tp = current;\\n\\t\\tif (p)\\n\\t\\t\\terror = set_one_prio(p, niceval, error);\\n\\t\\tbreak;\\n\\tcase PRIO_PGRP:\\n\\t\\tif (who)\\n\\t\\t\\tpgrp = find_vpid(who);\\n\\t\\telse\\n\\t\\t\\tpgrp = task_pgrp(current);\\n\\t\\tdo_each_pid_thread(pgrp, PIDTYPE_PGID, p) {\\n\\t\\t\\terror = set_one_prio(p, niceval, error);\\n\\t\\t} while_each_pid_thread(pgrp, PIDTYPE_PGID, p);\\n\\t\\tbreak;\\n\\tcase PRIO_USER:\\n\\t\\tuid = make_kuid(cred->user_ns, who);\\n\\t\\tuser = cred->user;\\n\\t\\tif (!who)\\n\\t\\t\\tuid = cred->uid;\\n\\t\\telse if (!uid_eq(uid, cred->uid)) {\\n\\t\\t\\tuser = find_user(uid);\\n\\t\\t\\tif (!user)\\n\\t\\t\\t\\tgoto out_unlock;\\t/* No processes for this user */\\n\\t\\t}\\n\\t\\tdo_each_thread(g, p) {\\n\\t\\t\\tif (uid_eq(task_uid(p), uid) && task_pid_vnr(p))\\n\\t\\t\\t\\terror = set_one_prio(p, niceval, error);\\n\\t\\t} while_each_thread(g, p);\\n\\t\\tif (!uid_eq(uid, cred->uid))\\n\\t\\t\\tfree_uid(user);\\t\\t/* For find_user() */\\n\\t\\tbreak;\\n\\t}\\nout_unlock:\\n\\tread_unlock(&tasklist_lock);\\n\\trcu_read_unlock();\\nout:\\n\\treturn error;\\n}\\n\\n/*\\n * Ugh. To avoid negative return values, \"getpriority()\" will\\n * not return the normal nice-value, but a negated value that\\n * has been offset by 20 (ie it returns 40..1 instead of -20..19)\\n * to stay compatible.\\n */\\nSYSCALL_DEFINE2(getpriority, int, which, int, who)\\n{\\n\\tstruct task_struct *g, *p;\\n\\tstruct user_struct *user;\\n\\tconst struct cred *cred = current_cred();\\n\\tlong niceval, retval = -ESRCH;\\n\\tstruct pid *pgrp;\\n\\tkuid_t uid;\\n\\n\\tif (which > PRIO_USER || which < PRIO_PROCESS)\\n\\t\\treturn -EINVAL;\\n\\n\\trcu_read_lock();\\n\\tread_lock(&tasklist_lock);\\n\\tswitch (which) {\\n\\tcase PRIO_PROCESS:\\n\\t\\tif (who)\\n\\t\\t\\tp = find_task_by_vpid(who);\\n\\t\\telse\\n\\t\\t\\tp = current;\\n\\t\\tif (p) {\\n\\t\\t\\tniceval = nice_to_rlimit(task_nice(p));\\n\\t\\t\\tif (niceval > retval)\\n\\t\\t\\t\\tretval = niceval;\\n\\t\\t}\\n\\t\\tbreak;\\n\\tcase PRIO_PGRP:\\n\\t\\tif (who)\\n\\t\\t\\tpgrp = find_vpid(who);\\n\\t\\telse\\n\\t\\t\\tpgrp = task_pgrp(current);\\n\\t\\tdo_each_pid_thread(pgrp, PIDTYPE_PGID, p) {\\n\\t\\t\\tniceval = nice_to_rlimit(task_nice(p));\\n\\t\\t\\tif (niceval > retval)\\n\\t\\t\\t\\tretval = niceval;\\n\\t\\t} while_each_pid_thread(pgrp, PIDTYPE_PGID, p);\\n\\t\\tbreak;\\n\\tcase PRIO_USER:\\n\\t\\tuid = make_kuid(cred->user_ns, who);\\n\\t\\tuser = cred->user;\\n\\t\\tif (!who)\\n\\t\\t\\tuid = cred->uid;\\n\\t\\telse if (!uid_eq(uid, cred->uid)) {\\n\\t\\t\\tuser = find_user(uid);\\n\\t\\t\\tif (!user)\\n\\t\\t\\t\\tgoto out_unlock;\\t/* No processes for this user */\\n\\t\\t}\\n\\t\\tdo_each_thread(g, p) {\\n\\t\\t\\tif (uid_eq(task_uid(p), uid) && task_pid_vnr(p)) {\\n\\t\\t\\t\\tniceval = nice_to_rlimit(task_nice(p));\\n\\t\\t\\t\\tif (niceval > retval)\\n\\t\\t\\t\\t\\tretval = niceval;\\n\\t\\t\\t}\\n\\t\\t} while_each_thread(g, p);\\n\\t\\tif (!uid_eq(uid, cred->uid))\\n\\t\\t\\tfree_uid(user);\\t\\t/* for find_user() */\\n\\t\\tbreak;\\n\\t}\\nout_unlock:\\n\\tread_unlock(&tasklist_lock);\\n\\trcu_read_unlock();\\n\\n\\treturn retval;\\n}\\n\\n/*\\n * Unprivileged users may change the real gid to the effective gid\\n * or vice versa.  (BSD-style)\\n *\\n * If you set the real gid at all, or set the effective gid to a value not\\n * equal to the real gid, then the saved gid is set to the new effective gid.\\n *\\n * This makes it possible for a setgid program to completely drop its\\n * privileges, which is often a useful assertion to make when you are doing\\n * a security audit over a program.\\n *\\n * The general idea is that a program which uses just setregid() will be\\n * 100% compatible with BSD.  A program which uses just setgid() will be\\n * 100% compatible with POSIX with saved IDs.\\n *\\n * SMP: There are not races, the GIDs are checked only by filesystem\\n *      operations (as far as semantic preservation is concerned).\\n */\\n#ifdef CONFIG_MULTIUSER\\nlong __sys_setregid(gid_t rgid, gid_t egid)\\n{\\n\\tstruct user_namespace *ns = current_user_ns();\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tint retval;\\n\\tkgid_t krgid, kegid;\\n\\n\\tkrgid = make_kgid(ns, rgid);\\n\\tkegid = make_kgid(ns, egid);\\n\\n\\tif ((rgid != (gid_t) -1) && !gid_valid(krgid))\\n\\t\\treturn -EINVAL;\\n\\tif ((egid != (gid_t) -1) && !gid_valid(kegid))\\n\\t\\treturn -EINVAL;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\told = current_cred();\\n\\n\\tretval = -EPERM;\\n\\tif (rgid != (gid_t) -1) {\\n\\t\\tif (gid_eq(old->gid, krgid) ||\\n\\t\\t    gid_eq(old->egid, krgid) ||\\n\\t\\t    ns_capable(old->user_ns, CAP_SETGID))\\n\\t\\t\\tnew->gid = krgid;\\n\\t\\telse\\n\\t\\t\\tgoto error;\\n\\t}\\n\\tif (egid != (gid_t) -1) {\\n\\t\\tif (gid_eq(old->gid, kegid) ||\\n\\t\\t    gid_eq(old->egid, kegid) ||\\n\\t\\t    gid_eq(old->sgid, kegid) ||\\n\\t\\t    ns_capable(old->user_ns, CAP_SETGID))\\n\\t\\t\\tnew->egid = kegid;\\n\\t\\telse\\n\\t\\t\\tgoto error;\\n\\t}\\n\\n\\tif (rgid != (gid_t) -1 ||\\n\\t    (egid != (gid_t) -1 && !gid_eq(kegid, old->gid)))\\n\\t\\tnew->sgid = new->egid;\\n\\tnew->fsgid = new->egid;\\n\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE2(setregid, gid_t, rgid, gid_t, egid)\\n{\\n\\treturn __sys_setregid(rgid, egid);\\n}\\n\\n/*\\n * setgid() is implemented like SysV w/ SAVED_IDS\\n *\\n * SMP: Same implicit races as above.\\n */\\nlong __sys_setgid(gid_t gid)\\n{\\n\\tstruct user_namespace *ns = current_user_ns();\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tint retval;\\n\\tkgid_t kgid;\\n\\n\\tkgid = make_kgid(ns, gid);\\n\\tif (!gid_valid(kgid))\\n\\t\\treturn -EINVAL;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\told = current_cred();\\n\\n\\tretval = -EPERM;\\n\\tif (ns_capable(old->user_ns, CAP_SETGID))\\n\\t\\tnew->gid = new->egid = new->sgid = new->fsgid = kgid;\\n\\telse if (gid_eq(kgid, old->gid) || gid_eq(kgid, old->sgid))\\n\\t\\tnew->egid = new->fsgid = kgid;\\n\\telse\\n\\t\\tgoto error;\\n\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE1(setgid, gid_t, gid)\\n{\\n\\treturn __sys_setgid(gid);\\n}\\n\\n/*\\n * change the user struct in a credentials set to match the new UID\\n */\\nstatic int set_user(struct cred *new)\\n{\\n\\tstruct user_struct *new_user;\\n\\n\\tnew_user = alloc_uid(new->uid);\\n\\tif (!new_user)\\n\\t\\treturn -EAGAIN;\\n\\n\\t/*\\n\\t * We don\\'t fail in case of NPROC limit excess here because too many\\n\\t * poorly written programs don\\'t check set*uid() return code, assuming\\n\\t * it never fails if called by root.  We may still enforce NPROC limit\\n\\t * for programs doing set*uid()+execve() by harmlessly deferring the\\n\\t * failure to the execve() stage.\\n\\t */\\n\\tif (atomic_read(&new_user->processes) >= rlimit(RLIMIT_NPROC) &&\\n\\t\\t\\tnew_user != INIT_USER)\\n\\t\\tcurrent->flags |= PF_NPROC_EXCEEDED;\\n\\telse\\n\\t\\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\\n\\n\\tfree_uid(new->user);\\n\\tnew->user = new_user;\\n\\treturn 0;\\n}\\n\\n/*\\n * Unprivileged users may change the real uid to the effective uid\\n * or vice versa.  (BSD-style)\\n *\\n * If you set the real uid at all, or set the effective uid to a value not\\n * equal to the real uid, then the saved uid is set to the new effective uid.\\n *\\n * This makes it possible for a setuid program to completely drop its\\n * privileges, which is often a useful assertion to make when you are doing\\n * a security audit over a program.\\n *\\n * The general idea is that a program which uses just setreuid() will be\\n * 100% compatible with BSD.  A program which uses just setuid() will be\\n * 100% compatible with POSIX with saved IDs.\\n */\\nlong __sys_setreuid(uid_t ruid, uid_t euid)\\n{\\n\\tstruct user_namespace *ns = current_user_ns();\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tint retval;\\n\\tkuid_t kruid, keuid;\\n\\n\\tkruid = make_kuid(ns, ruid);\\n\\tkeuid = make_kuid(ns, euid);\\n\\n\\tif ((ruid != (uid_t) -1) && !uid_valid(kruid))\\n\\t\\treturn -EINVAL;\\n\\tif ((euid != (uid_t) -1) && !uid_valid(keuid))\\n\\t\\treturn -EINVAL;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\told = current_cred();\\n\\n\\tretval = -EPERM;\\n\\tif (ruid != (uid_t) -1) {\\n\\t\\tnew->uid = kruid;\\n\\t\\tif (!uid_eq(old->uid, kruid) &&\\n\\t\\t    !uid_eq(old->euid, kruid) &&\\n\\t\\t    !ns_capable(old->user_ns, CAP_SETUID))\\n\\t\\t\\tgoto error;\\n\\t}\\n\\n\\tif (euid != (uid_t) -1) {\\n\\t\\tnew->euid = keuid;\\n\\t\\tif (!uid_eq(old->uid, keuid) &&\\n\\t\\t    !uid_eq(old->euid, keuid) &&\\n\\t\\t    !uid_eq(old->suid, keuid) &&\\n\\t\\t    !ns_capable(old->user_ns, CAP_SETUID))\\n\\t\\t\\tgoto error;\\n\\t}\\n\\n\\tif (!uid_eq(new->uid, old->uid)) {\\n\\t\\tretval = set_user(new);\\n\\t\\tif (retval < 0)\\n\\t\\t\\tgoto error;\\n\\t}\\n\\tif (ruid != (uid_t) -1 ||\\n\\t    (euid != (uid_t) -1 && !uid_eq(keuid, old->uid)))\\n\\t\\tnew->suid = new->euid;\\n\\tnew->fsuid = new->euid;\\n\\n\\tretval = security_task_fix_setuid(new, old, LSM_SETID_RE);\\n\\tif (retval < 0)\\n\\t\\tgoto error;\\n\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE2(setreuid, uid_t, ruid, uid_t, euid)\\n{\\n\\treturn __sys_setreuid(ruid, euid);\\n}\\n\\n/*\\n * setuid() is implemented like SysV with SAVED_IDS\\n *\\n * Note that SAVED_ID\\'s is deficient in that a setuid root program\\n * like sendmail, for example, cannot set its uid to be a normal\\n * user and then switch back, because if you\\'re root, setuid() sets\\n * the saved uid too.  If you don\\'t like this, blame the bright people\\n * in the POSIX committee and/or USG.  Note that the BSD-style setreuid()\\n * will allow a root program to temporarily drop privileges and be able to\\n * regain them by swapping the real and effective uid.\\n */\\nlong __sys_setuid(uid_t uid)\\n{\\n\\tstruct user_namespace *ns = current_user_ns();\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tint retval;\\n\\tkuid_t kuid;\\n\\n\\tkuid = make_kuid(ns, uid);\\n\\tif (!uid_valid(kuid))\\n\\t\\treturn -EINVAL;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\told = current_cred();\\n\\n\\tretval = -EPERM;\\n\\tif (ns_capable(old->user_ns, CAP_SETUID)) {\\n\\t\\tnew->suid = new->uid = kuid;\\n\\t\\tif (!uid_eq(kuid, old->uid)) {\\n\\t\\t\\tretval = set_user(new);\\n\\t\\t\\tif (retval < 0)\\n\\t\\t\\t\\tgoto error;\\n\\t\\t}\\n\\t} else if (!uid_eq(kuid, old->uid) && !uid_eq(kuid, new->suid)) {\\n\\t\\tgoto error;\\n\\t}\\n\\n\\tnew->fsuid = new->euid = kuid;\\n\\n\\tretval = security_task_fix_setuid(new, old, LSM_SETID_ID);\\n\\tif (retval < 0)\\n\\t\\tgoto error;\\n\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE1(setuid, uid_t, uid)\\n{\\n\\treturn __sys_setuid(uid);\\n}\\n\\n\\n/*\\n * This function implements a generic ability to update ruid, euid,\\n * and suid.  This allows you to implement the 4.4 compatible seteuid().\\n */\\nlong __sys_setresuid(uid_t ruid, uid_t euid, uid_t suid)\\n{\\n\\tstruct user_namespace *ns = current_user_ns();\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tint retval;\\n\\tkuid_t kruid, keuid, ksuid;\\n\\n\\tkruid = make_kuid(ns, ruid);\\n\\tkeuid = make_kuid(ns, euid);\\n\\tksuid = make_kuid(ns, suid);\\n\\n\\tif ((ruid != (uid_t) -1) && !uid_valid(kruid))\\n\\t\\treturn -EINVAL;\\n\\n\\tif ((euid != (uid_t) -1) && !uid_valid(keuid))\\n\\t\\treturn -EINVAL;\\n\\n\\tif ((suid != (uid_t) -1) && !uid_valid(ksuid))\\n\\t\\treturn -EINVAL;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\n\\told = current_cred();\\n\\n\\tretval = -EPERM;\\n\\tif (!ns_capable(old->user_ns, CAP_SETUID)) {\\n\\t\\tif (ruid != (uid_t) -1        && !uid_eq(kruid, old->uid) &&\\n\\t\\t    !uid_eq(kruid, old->euid) && !uid_eq(kruid, old->suid))\\n\\t\\t\\tgoto error;\\n\\t\\tif (euid != (uid_t) -1        && !uid_eq(keuid, old->uid) &&\\n\\t\\t    !uid_eq(keuid, old->euid) && !uid_eq(keuid, old->suid))\\n\\t\\t\\tgoto error;\\n\\t\\tif (suid != (uid_t) -1        && !uid_eq(ksuid, old->uid) &&\\n\\t\\t    !uid_eq(ksuid, old->euid) && !uid_eq(ksuid, old->suid))\\n\\t\\t\\tgoto error;\\n\\t}\\n\\n\\tif (ruid != (uid_t) -1) {\\n\\t\\tnew->uid = kruid;\\n\\t\\tif (!uid_eq(kruid, old->uid)) {\\n\\t\\t\\tretval = set_user(new);\\n\\t\\t\\tif (retval < 0)\\n\\t\\t\\t\\tgoto error;\\n\\t\\t}\\n\\t}\\n\\tif (euid != (uid_t) -1)\\n\\t\\tnew->euid = keuid;\\n\\tif (suid != (uid_t) -1)\\n\\t\\tnew->suid = ksuid;\\n\\tnew->fsuid = new->euid;\\n\\n\\tretval = security_task_fix_setuid(new, old, LSM_SETID_RES);\\n\\tif (retval < 0)\\n\\t\\tgoto error;\\n\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE3(setresuid, uid_t, ruid, uid_t, euid, uid_t, suid)\\n{\\n\\treturn __sys_setresuid(ruid, euid, suid);\\n}\\n\\nSYSCALL_DEFINE3(getresuid, uid_t __user *, ruidp, uid_t __user *, euidp, uid_t __user *, suidp)\\n{\\n\\tconst struct cred *cred = current_cred();\\n\\tint retval;\\n\\tuid_t ruid, euid, suid;\\n\\n\\truid = from_kuid_munged(cred->user_ns, cred->uid);\\n\\teuid = from_kuid_munged(cred->user_ns, cred->euid);\\n\\tsuid = from_kuid_munged(cred->user_ns, cred->suid);\\n\\n\\tretval = put_user(ruid, ruidp);\\n\\tif (!retval) {\\n\\t\\tretval = put_user(euid, euidp);\\n\\t\\tif (!retval)\\n\\t\\t\\treturn put_user(suid, suidp);\\n\\t}\\n\\treturn retval;\\n}\\n\\n/*\\n * Same as above, but for rgid, egid, sgid.\\n */\\nlong __sys_setresgid(gid_t rgid, gid_t egid, gid_t sgid)\\n{\\n\\tstruct user_namespace *ns = current_user_ns();\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tint retval;\\n\\tkgid_t krgid, kegid, ksgid;\\n\\n\\tkrgid = make_kgid(ns, rgid);\\n\\tkegid = make_kgid(ns, egid);\\n\\tksgid = make_kgid(ns, sgid);\\n\\n\\tif ((rgid != (gid_t) -1) && !gid_valid(krgid))\\n\\t\\treturn -EINVAL;\\n\\tif ((egid != (gid_t) -1) && !gid_valid(kegid))\\n\\t\\treturn -EINVAL;\\n\\tif ((sgid != (gid_t) -1) && !gid_valid(ksgid))\\n\\t\\treturn -EINVAL;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn -ENOMEM;\\n\\told = current_cred();\\n\\n\\tretval = -EPERM;\\n\\tif (!ns_capable(old->user_ns, CAP_SETGID)) {\\n\\t\\tif (rgid != (gid_t) -1        && !gid_eq(krgid, old->gid) &&\\n\\t\\t    !gid_eq(krgid, old->egid) && !gid_eq(krgid, old->sgid))\\n\\t\\t\\tgoto error;\\n\\t\\tif (egid != (gid_t) -1        && !gid_eq(kegid, old->gid) &&\\n\\t\\t    !gid_eq(kegid, old->egid) && !gid_eq(kegid, old->sgid))\\n\\t\\t\\tgoto error;\\n\\t\\tif (sgid != (gid_t) -1        && !gid_eq(ksgid, old->gid) &&\\n\\t\\t    !gid_eq(ksgid, old->egid) && !gid_eq(ksgid, old->sgid))\\n\\t\\t\\tgoto error;\\n\\t}\\n\\n\\tif (rgid != (gid_t) -1)\\n\\t\\tnew->gid = krgid;\\n\\tif (egid != (gid_t) -1)\\n\\t\\tnew->egid = kegid;\\n\\tif (sgid != (gid_t) -1)\\n\\t\\tnew->sgid = ksgid;\\n\\tnew->fsgid = new->egid;\\n\\n\\treturn commit_creds(new);\\n\\nerror:\\n\\tabort_creds(new);\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE3(setresgid, gid_t, rgid, gid_t, egid, gid_t, sgid)\\n{\\n\\treturn __sys_setresgid(rgid, egid, sgid);\\n}\\n\\nSYSCALL_DEFINE3(getresgid, gid_t __user *, rgidp, gid_t __user *, egidp, gid_t __user *, sgidp)\\n{\\n\\tconst struct cred *cred = current_cred();\\n\\tint retval;\\n\\tgid_t rgid, egid, sgid;\\n\\n\\trgid = from_kgid_munged(cred->user_ns, cred->gid);\\n\\tegid = from_kgid_munged(cred->user_ns, cred->egid);\\n\\tsgid = from_kgid_munged(cred->user_ns, cred->sgid);\\n\\n\\tretval = put_user(rgid, rgidp);\\n\\tif (!retval) {\\n\\t\\tretval = put_user(egid, egidp);\\n\\t\\tif (!retval)\\n\\t\\t\\tretval = put_user(sgid, sgidp);\\n\\t}\\n\\n\\treturn retval;\\n}\\n\\n\\n/*\\n * \"setfsuid()\" sets the fsuid - the uid used for filesystem checks. This\\n * is used for \"access()\" and for the NFS daemon (letting nfsd stay at\\n * whatever uid it wants to). It normally shadows \"euid\", except when\\n * explicitly set by setfsuid() or for access..\\n */\\nlong __sys_setfsuid(uid_t uid)\\n{\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tuid_t old_fsuid;\\n\\tkuid_t kuid;\\n\\n\\told = current_cred();\\n\\told_fsuid = from_kuid_munged(old->user_ns, old->fsuid);\\n\\n\\tkuid = make_kuid(old->user_ns, uid);\\n\\tif (!uid_valid(kuid))\\n\\t\\treturn old_fsuid;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn old_fsuid;\\n\\n\\tif (uid_eq(kuid, old->uid)  || uid_eq(kuid, old->euid)  ||\\n\\t    uid_eq(kuid, old->suid) || uid_eq(kuid, old->fsuid) ||\\n\\t    ns_capable(old->user_ns, CAP_SETUID)) {\\n\\t\\tif (!uid_eq(kuid, old->fsuid)) {\\n\\t\\t\\tnew->fsuid = kuid;\\n\\t\\t\\tif (security_task_fix_setuid(new, old, LSM_SETID_FS) == 0)\\n\\t\\t\\t\\tgoto change_okay;\\n\\t\\t}\\n\\t}\\n\\n\\tabort_creds(new);\\n\\treturn old_fsuid;\\n\\nchange_okay:\\n\\tcommit_creds(new);\\n\\treturn old_fsuid;\\n}\\n\\nSYSCALL_DEFINE1(setfsuid, uid_t, uid)\\n{\\n\\treturn __sys_setfsuid(uid);\\n}\\n\\n/*\\n * Samma på svenska..\\n */\\nlong __sys_setfsgid(gid_t gid)\\n{\\n\\tconst struct cred *old;\\n\\tstruct cred *new;\\n\\tgid_t old_fsgid;\\n\\tkgid_t kgid;\\n\\n\\told = current_cred();\\n\\told_fsgid = from_kgid_munged(old->user_ns, old->fsgid);\\n\\n\\tkgid = make_kgid(old->user_ns, gid);\\n\\tif (!gid_valid(kgid))\\n\\t\\treturn old_fsgid;\\n\\n\\tnew = prepare_creds();\\n\\tif (!new)\\n\\t\\treturn old_fsgid;\\n\\n\\tif (gid_eq(kgid, old->gid)  || gid_eq(kgid, old->egid)  ||\\n\\t    gid_eq(kgid, old->sgid) || gid_eq(kgid, old->fsgid) ||\\n\\t    ns_capable(old->user_ns, CAP_SETGID)) {\\n\\t\\tif (!gid_eq(kgid, old->fsgid)) {\\n\\t\\t\\tnew->fsgid = kgid;\\n\\t\\t\\tgoto change_okay;\\n\\t\\t}\\n\\t}\\n\\n\\tabort_creds(new);\\n\\treturn old_fsgid;\\n\\nchange_okay:\\n\\tcommit_creds(new);\\n\\treturn old_fsgid;\\n}\\n\\nSYSCALL_DEFINE1(setfsgid, gid_t, gid)\\n{\\n\\treturn __sys_setfsgid(gid);\\n}\\n#endif /* CONFIG_MULTIUSER */\\n\\n/**\\n * sys_getpid - return the thread group id of the current process\\n *\\n * Note, despite the name, this returns the tgid not the pid.  The tgid and\\n * the pid are identical unless CLONE_THREAD was specified on clone() in\\n * which case the tgid is the same in all threads of the same group.\\n *\\n * This is SMP safe as current->tgid does not change.\\n */\\nSYSCALL_DEFINE0(getpid)\\n{\\n\\treturn task_tgid_vnr(current);\\n}\\n\\n/* Thread ID - the internal kernel \"pid\" */\\nSYSCALL_DEFINE0(gettid)\\n{\\n\\treturn task_pid_vnr(current);\\n}\\n\\n/*\\n * Accessing ->real_parent is not SMP-safe, it could\\n * change from under us. However, we can use a stale\\n * value of ->real_parent under rcu_read_lock(), see\\n * release_task()->call_rcu(delayed_put_task_struct).\\n */\\nSYSCALL_DEFINE0(getppid)\\n{\\n\\tint pid;\\n\\n\\trcu_read_lock();\\n\\tpid = task_tgid_vnr(rcu_dereference(current->real_parent));\\n\\trcu_read_unlock();\\n\\n\\treturn pid;\\n}\\n\\nSYSCALL_DEFINE0(getuid)\\n{\\n\\t/* Only we change this so SMP safe */\\n\\treturn from_kuid_munged(current_user_ns(), current_uid());\\n}\\n\\nSYSCALL_DEFINE0(geteuid)\\n{\\n\\t/* Only we change this so SMP safe */\\n\\treturn from_kuid_munged(current_user_ns(), current_euid());\\n}\\n\\nSYSCALL_DEFINE0(getgid)\\n{\\n\\t/* Only we change this so SMP safe */\\n\\treturn from_kgid_munged(current_user_ns(), current_gid());\\n}\\n\\nSYSCALL_DEFINE0(getegid)\\n{\\n\\t/* Only we change this so SMP safe */\\n\\treturn from_kgid_munged(current_user_ns(), current_egid());\\n}\\n\\nstatic void do_sys_times(struct tms *tms)\\n{\\n\\tu64 tgutime, tgstime, cutime, cstime;\\n\\n\\tthread_group_cputime_adjusted(current, &tgutime, &tgstime);\\n\\tcutime = current->signal->cutime;\\n\\tcstime = current->signal->cstime;\\n\\ttms->tms_utime = nsec_to_clock_t(tgutime);\\n\\ttms->tms_stime = nsec_to_clock_t(tgstime);\\n\\ttms->tms_cutime = nsec_to_clock_t(cutime);\\n\\ttms->tms_cstime = nsec_to_clock_t(cstime);\\n}\\n\\nSYSCALL_DEFINE1(times, struct tms __user *, tbuf)\\n{\\n\\tif (tbuf) {\\n\\t\\tstruct tms tmp;\\n\\n\\t\\tdo_sys_times(&tmp);\\n\\t\\tif (copy_to_user(tbuf, &tmp, sizeof(struct tms)))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\tforce_successful_syscall_return();\\n\\treturn (long) jiffies_64_to_clock_t(get_jiffies_64());\\n}\\n\\n#ifdef CONFIG_COMPAT\\nstatic compat_clock_t clock_t_to_compat_clock_t(clock_t x)\\n{\\n\\treturn compat_jiffies_to_clock_t(clock_t_to_jiffies(x));\\n}\\n\\nCOMPAT_SYSCALL_DEFINE1(times, struct compat_tms __user *, tbuf)\\n{\\n\\tif (tbuf) {\\n\\t\\tstruct tms tms;\\n\\t\\tstruct compat_tms tmp;\\n\\n\\t\\tdo_sys_times(&tms);\\n\\t\\t/* Convert our struct tms to the compat version. */\\n\\t\\ttmp.tms_utime = clock_t_to_compat_clock_t(tms.tms_utime);\\n\\t\\ttmp.tms_stime = clock_t_to_compat_clock_t(tms.tms_stime);\\n\\t\\ttmp.tms_cutime = clock_t_to_compat_clock_t(tms.tms_cutime);\\n\\t\\ttmp.tms_cstime = clock_t_to_compat_clock_t(tms.tms_cstime);\\n\\t\\tif (copy_to_user(tbuf, &tmp, sizeof(tmp)))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\tforce_successful_syscall_return();\\n\\treturn compat_jiffies_to_clock_t(jiffies);\\n}\\n#endif\\n\\n/*\\n * This needs some heavy checking ...\\n * I just haven\\'t the stomach for it. I also don\\'t fully\\n * understand sessions/pgrp etc. Let somebody who does explain it.\\n *\\n * OK, I think I have the protection semantics right.... this is really\\n * only important on a multi-user system anyway, to make sure one user\\n * can\\'t send a signal to a process owned by another.  -TYT, 12/12/91\\n *\\n * !PF_FORKNOEXEC check to conform completely to POSIX.\\n */\\nSYSCALL_DEFINE2(setpgid, pid_t, pid, pid_t, pgid)\\n{\\n\\tstruct task_struct *p;\\n\\tstruct task_struct *group_leader = current->group_leader;\\n\\tstruct pid *pgrp;\\n\\tint err;\\n\\n\\tif (!pid)\\n\\t\\tpid = task_pid_vnr(group_leader);\\n\\tif (!pgid)\\n\\t\\tpgid = pid;\\n\\tif (pgid < 0)\\n\\t\\treturn -EINVAL;\\n\\trcu_read_lock();\\n\\n\\t/* From this point forward we keep holding onto the tasklist lock\\n\\t * so that our parent does not change from under us. -DaveM\\n\\t */\\n\\twrite_lock_irq(&tasklist_lock);\\n\\n\\terr = -ESRCH;\\n\\tp = find_task_by_vpid(pid);\\n\\tif (!p)\\n\\t\\tgoto out;\\n\\n\\terr = -EINVAL;\\n\\tif (!thread_group_leader(p))\\n\\t\\tgoto out;\\n\\n\\tif (same_thread_group(p->real_parent, group_leader)) {\\n\\t\\terr = -EPERM;\\n\\t\\tif (task_session(p) != task_session(group_leader))\\n\\t\\t\\tgoto out;\\n\\t\\terr = -EACCES;\\n\\t\\tif (!(p->flags & PF_FORKNOEXEC))\\n\\t\\t\\tgoto out;\\n\\t} else {\\n\\t\\terr = -ESRCH;\\n\\t\\tif (p != group_leader)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\terr = -EPERM;\\n\\tif (p->signal->leader)\\n\\t\\tgoto out;\\n\\n\\tpgrp = task_pid(p);\\n\\tif (pgid != pid) {\\n\\t\\tstruct task_struct *g;\\n\\n\\t\\tpgrp = find_vpid(pgid);\\n\\t\\tg = pid_task(pgrp, PIDTYPE_PGID);\\n\\t\\tif (!g || task_session(g) != task_session(group_leader))\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\terr = security_task_setpgid(p, pgid);\\n\\tif (err)\\n\\t\\tgoto out;\\n\\n\\tif (task_pgrp(p) != pgrp)\\n\\t\\tchange_pid(p, PIDTYPE_PGID, pgrp);\\n\\n\\terr = 0;\\nout:\\n\\t/* All paths lead to here, thus we are safe. -DaveM */\\n\\twrite_unlock_irq(&tasklist_lock);\\n\\trcu_read_unlock();\\n\\treturn err;\\n}\\n\\nstatic int do_getpgid(pid_t pid)\\n{\\n\\tstruct task_struct *p;\\n\\tstruct pid *grp;\\n\\tint retval;\\n\\n\\trcu_read_lock();\\n\\tif (!pid)\\n\\t\\tgrp = task_pgrp(current);\\n\\telse {\\n\\t\\tretval = -ESRCH;\\n\\t\\tp = find_task_by_vpid(pid);\\n\\t\\tif (!p)\\n\\t\\t\\tgoto out;\\n\\t\\tgrp = task_pgrp(p);\\n\\t\\tif (!grp)\\n\\t\\t\\tgoto out;\\n\\n\\t\\tretval = security_task_getpgid(p);\\n\\t\\tif (retval)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\tretval = pid_vnr(grp);\\nout:\\n\\trcu_read_unlock();\\n\\treturn retval;\\n}\\n\\nSYSCALL_DEFINE1(getpgid, pid_t, pid)\\n{\\n\\treturn do_getpgid(pid);\\n}\\n\\n#ifdef __ARCH_WANT_SYS_GETPGRP\\n\\nSYSCALL_DEFINE0(getpgrp)\\n{\\n\\treturn do_getpgid(0);\\n}\\n\\n#endif\\n\\nSYSCALL_DEFINE1(getsid, pid_t, pid)\\n{\\n\\tstruct task_struct *p;\\n\\tstruct pid *sid;\\n\\tint retval;\\n\\n\\trcu_read_lock();\\n\\tif (!pid)\\n\\t\\tsid = task_session(current);\\n\\telse {\\n\\t\\tretval = -ESRCH;\\n\\t\\tp = find_task_by_vpid(pid);\\n\\t\\tif (!p)\\n\\t\\t\\tgoto out;\\n\\t\\tsid = task_session(p);\\n\\t\\tif (!sid)\\n\\t\\t\\tgoto out;\\n\\n\\t\\tretval = security_task_getsid(p);\\n\\t\\tif (retval)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\tretval = pid_vnr(sid);\\nout:\\n\\trcu_read_unlock();\\n\\treturn retval;\\n}\\n\\nstatic void set_special_pids(struct pid *pid)\\n{\\n\\tstruct task_struct *curr = current->group_leader;\\n\\n\\tif (task_session(curr) != pid)\\n\\t\\tchange_pid(curr, PIDTYPE_SID, pid);\\n\\n\\tif (task_pgrp(curr) != pid)\\n\\t\\tchange_pid(curr, PIDTYPE_PGID, pid);\\n}\\n\\nint ksys_setsid(void)\\n{\\n\\tstruct task_struct *group_leader = current->group_leader;\\n\\tstruct pid *sid = task_pid(group_leader);\\n\\tpid_t session = pid_vnr(sid);\\n\\tint err = -EPERM;\\n\\n\\twrite_lock_irq(&tasklist_lock);\\n\\t/* Fail if I am already a session leader */\\n\\tif (group_leader->signal->leader)\\n\\t\\tgoto out;\\n\\n\\t/* Fail if a process group id already exists that equals the\\n\\t * proposed session id.\\n\\t */\\n\\tif (pid_task(sid, PIDTYPE_PGID))\\n\\t\\tgoto out;\\n\\n\\tgroup_leader->signal->leader = 1;\\n\\tset_special_pids(sid);\\n\\n\\tproc_clear_tty(group_leader);\\n\\n\\terr = session;\\nout:\\n\\twrite_unlock_irq(&tasklist_lock);\\n\\tif (err > 0) {\\n\\t\\tproc_sid_connector(group_leader);\\n\\t\\tsched_autogroup_create_attach(group_leader);\\n\\t}\\n\\treturn err;\\n}\\n\\nSYSCALL_DEFINE0(setsid)\\n{\\n\\treturn ksys_setsid();\\n}\\n\\nDECLARE_RWSEM(uts_sem);\\n\\n#ifdef COMPAT_UTS_MACHINE\\n#define override_architecture(name) \\\\\\n\\t(personality(current->personality) == PER_LINUX32 && \\\\\\n\\t copy_to_user(name->machine, COMPAT_UTS_MACHINE, \\\\\\n\\t\\t      sizeof(COMPAT_UTS_MACHINE)))\\n#else\\n#define override_architecture(name)\\t0\\n#endif\\n\\n/*\\n * Work around broken programs that cannot handle \"Linux 3.0\".\\n * Instead we map 3.x to 2.6.40+x, so e.g. 3.0 would be 2.6.40\\n * And we map 4.x and later versions to 2.6.60+x, so 4.0/5.0/6.0/... would be\\n * 2.6.60.\\n */\\nstatic int override_release(char __user *release, size_t len)\\n{\\n\\tint ret = 0;\\n\\n\\tif (current->personality & UNAME26) {\\n\\t\\tconst char *rest = UTS_RELEASE;\\n\\t\\tchar buf[65] = { 0 };\\n\\t\\tint ndots = 0;\\n\\t\\tunsigned v;\\n\\t\\tsize_t copy;\\n\\n\\t\\twhile (*rest) {\\n\\t\\t\\tif (*rest == \\'.\\' && ++ndots >= 3)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tif (!isdigit(*rest) && *rest != \\'.\\')\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\trest++;\\n\\t\\t}\\n\\t\\tv = ((LINUX_VERSION_CODE >> 8) & 0xff) + 60;\\n\\t\\tcopy = clamp_t(size_t, len, 1, sizeof(buf));\\n\\t\\tcopy = scnprintf(buf, copy, \"2.6.%u%s\", v, rest);\\n\\t\\tret = copy_to_user(release, buf, copy + 1);\\n\\t}\\n\\treturn ret;\\n}\\n\\nSYSCALL_DEFINE1(newuname, struct new_utsname __user *, name)\\n{\\n\\tstruct new_utsname tmp;\\n\\n\\tdown_read(&uts_sem);\\n\\tmemcpy(&tmp, utsname(), sizeof(tmp));\\n\\tup_read(&uts_sem);\\n\\tif (copy_to_user(name, &tmp, sizeof(tmp)))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (override_release(name->release, sizeof(name->release)))\\n\\t\\treturn -EFAULT;\\n\\tif (override_architecture(name))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n\\n#ifdef __ARCH_WANT_SYS_OLD_UNAME\\n/*\\n * Old cruft\\n */\\nSYSCALL_DEFINE1(uname, struct old_utsname __user *, name)\\n{\\n\\tstruct old_utsname tmp;\\n\\n\\tif (!name)\\n\\t\\treturn -EFAULT;\\n\\n\\tdown_read(&uts_sem);\\n\\tmemcpy(&tmp, utsname(), sizeof(tmp));\\n\\tup_read(&uts_sem);\\n\\tif (copy_to_user(name, &tmp, sizeof(tmp)))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (override_release(name->release, sizeof(name->release)))\\n\\t\\treturn -EFAULT;\\n\\tif (override_architecture(name))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n\\nSYSCALL_DEFINE1(olduname, struct oldold_utsname __user *, name)\\n{\\n\\tstruct oldold_utsname tmp = {};\\n\\n\\tif (!name)\\n\\t\\treturn -EFAULT;\\n\\n\\tdown_read(&uts_sem);\\n\\tmemcpy(&tmp.sysname, &utsname()->sysname, __OLD_UTS_LEN);\\n\\tmemcpy(&tmp.nodename, &utsname()->nodename, __OLD_UTS_LEN);\\n\\tmemcpy(&tmp.release, &utsname()->release, __OLD_UTS_LEN);\\n\\tmemcpy(&tmp.version, &utsname()->version, __OLD_UTS_LEN);\\n\\tmemcpy(&tmp.machine, &utsname()->machine, __OLD_UTS_LEN);\\n\\tup_read(&uts_sem);\\n\\tif (copy_to_user(name, &tmp, sizeof(tmp)))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (override_architecture(name))\\n\\t\\treturn -EFAULT;\\n\\tif (override_release(name->release, sizeof(name->release)))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n#endif\\n\\nSYSCALL_DEFINE2(sethostname, char __user *, name, int, len)\\n{\\n\\tint errno;\\n\\tchar tmp[__NEW_UTS_LEN];\\n\\n\\tif (!ns_capable(current->nsproxy->uts_ns->user_ns, CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\tif (len < 0 || len > __NEW_UTS_LEN)\\n\\t\\treturn -EINVAL;\\n\\terrno = -EFAULT;\\n\\tif (!copy_from_user(tmp, name, len)) {\\n\\t\\tstruct new_utsname *u;\\n\\n\\t\\tdown_write(&uts_sem);\\n\\t\\tu = utsname();\\n\\t\\tmemcpy(u->nodename, tmp, len);\\n\\t\\tmemset(u->nodename + len, 0, sizeof(u->nodename) - len);\\n\\t\\terrno = 0;\\n\\t\\tuts_proc_notify(UTS_PROC_HOSTNAME);\\n\\t\\tup_write(&uts_sem);\\n\\t}\\n\\treturn errno;\\n}\\n\\n#ifdef __ARCH_WANT_SYS_GETHOSTNAME\\n\\nSYSCALL_DEFINE2(gethostname, char __user *, name, int, len)\\n{\\n\\tint i;\\n\\tstruct new_utsname *u;\\n\\tchar tmp[__NEW_UTS_LEN + 1];\\n\\n\\tif (len < 0)\\n\\t\\treturn -EINVAL;\\n\\tdown_read(&uts_sem);\\n\\tu = utsname();\\n\\ti = 1 + strlen(u->nodename);\\n\\tif (i > len)\\n\\t\\ti = len;\\n\\tmemcpy(tmp, u->nodename, i);\\n\\tup_read(&uts_sem);\\n\\tif (copy_to_user(name, tmp, i))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n\\n#endif\\n\\n/*\\n * Only setdomainname; getdomainname can be implemented by calling\\n * uname()\\n */\\nSYSCALL_DEFINE2(setdomainname, char __user *, name, int, len)\\n{\\n\\tint errno;\\n\\tchar tmp[__NEW_UTS_LEN];\\n\\n\\tif (!ns_capable(current->nsproxy->uts_ns->user_ns, CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\tif (len < 0 || len > __NEW_UTS_LEN)\\n\\t\\treturn -EINVAL;\\n\\n\\terrno = -EFAULT;\\n\\tif (!copy_from_user(tmp, name, len)) {\\n\\t\\tstruct new_utsname *u;\\n\\n\\t\\tdown_write(&uts_sem);\\n\\t\\tu = utsname();\\n\\t\\tmemcpy(u->domainname, tmp, len);\\n\\t\\tmemset(u->domainname + len, 0, sizeof(u->domainname) - len);\\n\\t\\terrno = 0;\\n\\t\\tuts_proc_notify(UTS_PROC_DOMAINNAME);\\n\\t\\tup_write(&uts_sem);\\n\\t}\\n\\treturn errno;\\n}\\n\\nSYSCALL_DEFINE2(getrlimit, unsigned int, resource, struct rlimit __user *, rlim)\\n{\\n\\tstruct rlimit value;\\n\\tint ret;\\n\\n\\tret = do_prlimit(current, resource, NULL, &value);\\n\\tif (!ret)\\n\\t\\tret = copy_to_user(rlim, &value, sizeof(*rlim)) ? -EFAULT : 0;\\n\\n\\treturn ret;\\n}\\n\\n#ifdef CONFIG_COMPAT\\n\\nCOMPAT_SYSCALL_DEFINE2(setrlimit, unsigned int, resource,\\n\\t\\t       struct compat_rlimit __user *, rlim)\\n{\\n\\tstruct rlimit r;\\n\\tstruct compat_rlimit r32;\\n\\n\\tif (copy_from_user(&r32, rlim, sizeof(struct compat_rlimit)))\\n\\t\\treturn -EFAULT;\\n\\n\\tif (r32.rlim_cur == COMPAT_RLIM_INFINITY)\\n\\t\\tr.rlim_cur = RLIM_INFINITY;\\n\\telse\\n\\t\\tr.rlim_cur = r32.rlim_cur;\\n\\tif (r32.rlim_max == COMPAT_RLIM_INFINITY)\\n\\t\\tr.rlim_max = RLIM_INFINITY;\\n\\telse\\n\\t\\tr.rlim_max = r32.rlim_max;\\n\\treturn do_prlimit(current, resource, &r, NULL);\\n}\\n\\nCOMPAT_SYSCALL_DEFINE2(getrlimit, unsigned int, resource,\\n\\t\\t       struct compat_rlimit __user *, rlim)\\n{\\n\\tstruct rlimit r;\\n\\tint ret;\\n\\n\\tret = do_prlimit(current, resource, NULL, &r);\\n\\tif (!ret) {\\n\\t\\tstruct compat_rlimit r32;\\n\\t\\tif (r.rlim_cur > COMPAT_RLIM_INFINITY)\\n\\t\\t\\tr32.rlim_cur = COMPAT_RLIM_INFINITY;\\n\\t\\telse\\n\\t\\t\\tr32.rlim_cur = r.rlim_cur;\\n\\t\\tif (r.rlim_max > COMPAT_RLIM_INFINITY)\\n\\t\\t\\tr32.rlim_max = COMPAT_RLIM_INFINITY;\\n\\t\\telse\\n\\t\\t\\tr32.rlim_max = r.rlim_max;\\n\\n\\t\\tif (copy_to_user(rlim, &r32, sizeof(struct compat_rlimit)))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\treturn ret;\\n}\\n\\n#endif\\n\\n#ifdef __ARCH_WANT_SYS_OLD_GETRLIMIT\\n\\n/*\\n *\\tBack compatibility for getrlimit. Needed for some apps.\\n */\\nSYSCALL_DEFINE2(old_getrlimit, unsigned int, resource,\\n\\t\\tstruct rlimit __user *, rlim)\\n{\\n\\tstruct rlimit x;\\n\\tif (resource >= RLIM_NLIMITS)\\n\\t\\treturn -EINVAL;\\n\\n\\tresource = array_index_nospec(resource, RLIM_NLIMITS);\\n\\ttask_lock(current->group_leader);\\n\\tx = current->signal->rlim[resource];\\n\\ttask_unlock(current->group_leader);\\n\\tif (x.rlim_cur > 0x7FFFFFFF)\\n\\t\\tx.rlim_cur = 0x7FFFFFFF;\\n\\tif (x.rlim_max > 0x7FFFFFFF)\\n\\t\\tx.rlim_max = 0x7FFFFFFF;\\n\\treturn copy_to_user(rlim, &x, sizeof(x)) ? -EFAULT : 0;\\n}\\n\\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE2(old_getrlimit, unsigned int, resource,\\n\\t\\t       struct compat_rlimit __user *, rlim)\\n{\\n\\tstruct rlimit r;\\n\\n\\tif (resource >= RLIM_NLIMITS)\\n\\t\\treturn -EINVAL;\\n\\n\\tresource = array_index_nospec(resource, RLIM_NLIMITS);\\n\\ttask_lock(current->group_leader);\\n\\tr = current->signal->rlim[resource];\\n\\ttask_unlock(current->group_leader);\\n\\tif (r.rlim_cur > 0x7FFFFFFF)\\n\\t\\tr.rlim_cur = 0x7FFFFFFF;\\n\\tif (r.rlim_max > 0x7FFFFFFF)\\n\\t\\tr.rlim_max = 0x7FFFFFFF;\\n\\n\\tif (put_user(r.rlim_cur, &rlim->rlim_cur) ||\\n\\t    put_user(r.rlim_max, &rlim->rlim_max))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n#endif\\n\\n#endif\\n\\nstatic inline bool rlim64_is_infinity(__u64 rlim64)\\n{\\n#if BITS_PER_LONG < 64\\n\\treturn rlim64 >= ULONG_MAX;\\n#else\\n\\treturn rlim64 == RLIM64_INFINITY;\\n#endif\\n}\\n\\nstatic void rlim_to_rlim64(const struct rlimit *rlim, struct rlimit64 *rlim64)\\n{\\n\\tif (rlim->rlim_cur == RLIM_INFINITY)\\n\\t\\trlim64->rlim_cur = RLIM64_INFINITY;\\n\\telse\\n\\t\\trlim64->rlim_cur = rlim->rlim_cur;\\n\\tif (rlim->rlim_max == RLIM_INFINITY)\\n\\t\\trlim64->rlim_max = RLIM64_INFINITY;\\n\\telse\\n\\t\\trlim64->rlim_max = rlim->rlim_max;\\n}\\n\\nstatic void rlim64_to_rlim(const struct rlimit64 *rlim64, struct rlimit *rlim)\\n{\\n\\tif (rlim64_is_infinity(rlim64->rlim_cur))\\n\\t\\trlim->rlim_cur = RLIM_INFINITY;\\n\\telse\\n\\t\\trlim->rlim_cur = (unsigned long)rlim64->rlim_cur;\\n\\tif (rlim64_is_infinity(rlim64->rlim_max))\\n\\t\\trlim->rlim_max = RLIM_INFINITY;\\n\\telse\\n\\t\\trlim->rlim_max = (unsigned long)rlim64->rlim_max;\\n}\\n\\n/* make sure you are allowed to change @tsk limits before calling this */\\nint do_prlimit(struct task_struct *tsk, unsigned int resource,\\n\\t\\tstruct rlimit *new_rlim, struct rlimit *old_rlim)\\n{\\n\\tstruct rlimit *rlim;\\n\\tint retval = 0;\\n\\n\\tif (resource >= RLIM_NLIMITS)\\n\\t\\treturn -EINVAL;\\n\\tif (new_rlim) {\\n\\t\\tif (new_rlim->rlim_cur > new_rlim->rlim_max)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (resource == RLIMIT_NOFILE &&\\n\\t\\t\\t\\tnew_rlim->rlim_max > sysctl_nr_open)\\n\\t\\t\\treturn -EPERM;\\n\\t}\\n\\n\\t/* protect tsk->signal and tsk->sighand from disappearing */\\n\\tread_lock(&tasklist_lock);\\n\\tif (!tsk->sighand) {\\n\\t\\tretval = -ESRCH;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\trlim = tsk->signal->rlim + resource;\\n\\ttask_lock(tsk->group_leader);\\n\\tif (new_rlim) {\\n\\t\\t/* Keep the capable check against init_user_ns until\\n\\t\\t   cgroups can contain all limits */\\n\\t\\tif (new_rlim->rlim_max > rlim->rlim_max &&\\n\\t\\t\\t\\t!capable(CAP_SYS_RESOURCE))\\n\\t\\t\\tretval = -EPERM;\\n\\t\\tif (!retval)\\n\\t\\t\\tretval = security_task_setrlimit(tsk, resource, new_rlim);\\n\\t\\tif (resource == RLIMIT_CPU && new_rlim->rlim_cur == 0) {\\n\\t\\t\\t/*\\n\\t\\t\\t * The caller is asking for an immediate RLIMIT_CPU\\n\\t\\t\\t * expiry.  But we use the zero value to mean \"it was\\n\\t\\t\\t * never set\".  So let\\'s cheat and make it one second\\n\\t\\t\\t * instead\\n\\t\\t\\t */\\n\\t\\t\\tnew_rlim->rlim_cur = 1;\\n\\t\\t}\\n\\t}\\n\\tif (!retval) {\\n\\t\\tif (old_rlim)\\n\\t\\t\\t*old_rlim = *rlim;\\n\\t\\tif (new_rlim)\\n\\t\\t\\t*rlim = *new_rlim;\\n\\t}\\n\\ttask_unlock(tsk->group_leader);\\n\\n\\t/*\\n\\t * RLIMIT_CPU handling.   Note that the kernel fails to return an error\\n\\t * code if it rejected the user\\'s attempt to set RLIMIT_CPU.  This is a\\n\\t * very long-standing error, and fixing it now risks breakage of\\n\\t * applications, so we live with it\\n\\t */\\n\\t if (!retval && new_rlim && resource == RLIMIT_CPU &&\\n\\t     new_rlim->rlim_cur != RLIM_INFINITY &&\\n\\t     IS_ENABLED(CONFIG_POSIX_TIMERS))\\n\\t\\tupdate_rlimit_cpu(tsk, new_rlim->rlim_cur);\\nout:\\n\\tread_unlock(&tasklist_lock);\\n\\treturn retval;\\n}\\n\\n/* rcu lock must be held */\\nstatic int check_prlimit_permission(struct task_struct *task,\\n\\t\\t\\t\\t    unsigned int flags)\\n{\\n\\tconst struct cred *cred = current_cred(), *tcred;\\n\\tbool id_match;\\n\\n\\tif (current == task)\\n\\t\\treturn 0;\\n\\n\\ttcred = __task_cred(task);\\n\\tid_match = (uid_eq(cred->uid, tcred->euid) &&\\n\\t\\t    uid_eq(cred->uid, tcred->suid) &&\\n\\t\\t    uid_eq(cred->uid, tcred->uid)  &&\\n\\t\\t    gid_eq(cred->gid, tcred->egid) &&\\n\\t\\t    gid_eq(cred->gid, tcred->sgid) &&\\n\\t\\t    gid_eq(cred->gid, tcred->gid));\\n\\tif (!id_match && !ns_capable(tcred->user_ns, CAP_SYS_RESOURCE))\\n\\t\\treturn -EPERM;\\n\\n\\treturn security_task_prlimit(cred, tcred, flags);\\n}\\n\\nSYSCALL_DEFINE4(prlimit64, pid_t, pid, unsigned int, resource,\\n\\t\\tconst struct rlimit64 __user *, new_rlim,\\n\\t\\tstruct rlimit64 __user *, old_rlim)\\n{\\n\\tstruct rlimit64 old64, new64;\\n\\tstruct rlimit old, new;\\n\\tstruct task_struct *tsk;\\n\\tunsigned int checkflags = 0;\\n\\tint ret;\\n\\n\\tif (old_rlim)\\n\\t\\tcheckflags |= LSM_PRLIMIT_READ;\\n\\n\\tif (new_rlim) {\\n\\t\\tif (copy_from_user(&new64, new_rlim, sizeof(new64)))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\trlim64_to_rlim(&new64, &new);\\n\\t\\tcheckflags |= LSM_PRLIMIT_WRITE;\\n\\t}\\n\\n\\trcu_read_lock();\\n\\ttsk = pid ? find_task_by_vpid(pid) : current;\\n\\tif (!tsk) {\\n\\t\\trcu_read_unlock();\\n\\t\\treturn -ESRCH;\\n\\t}\\n\\tret = check_prlimit_permission(tsk, checkflags);\\n\\tif (ret) {\\n\\t\\trcu_read_unlock();\\n\\t\\treturn ret;\\n\\t}\\n\\tget_task_struct(tsk);\\n\\trcu_read_unlock();\\n\\n\\tret = do_prlimit(tsk, resource, new_rlim ? &new : NULL,\\n\\t\\t\\told_rlim ? &old : NULL);\\n\\n\\tif (!ret && old_rlim) {\\n\\t\\trlim_to_rlim64(&old, &old64);\\n\\t\\tif (copy_to_user(old_rlim, &old64, sizeof(old64)))\\n\\t\\t\\tret = -EFAULT;\\n\\t}\\n\\n\\tput_task_struct(tsk);\\n\\treturn ret;\\n}\\n\\nSYSCALL_DEFINE2(setrlimit, unsigned int, resource, struct rlimit __user *, rlim)\\n{\\n\\tstruct rlimit new_rlim;\\n\\n\\tif (copy_from_user(&new_rlim, rlim, sizeof(*rlim)))\\n\\t\\treturn -EFAULT;\\n\\treturn do_prlimit(current, resource, &new_rlim, NULL);\\n}\\n\\n/*\\n * It would make sense to put struct rusage in the task_struct,\\n * except that would make the task_struct be *really big*.  After\\n * task_struct gets moved into malloc\\'ed memory, it would\\n * make sense to do this.  It will make moving the rest of the information\\n * a lot simpler!  (Which we\\'re not doing right now because we\\'re not\\n * measuring them yet).\\n *\\n * When sampling multiple threads for RUSAGE_SELF, under SMP we might have\\n * races with threads incrementing their own counters.  But since word\\n * reads are atomic, we either get new values or old values and we don\\'t\\n * care which for the sums.  We always take the siglock to protect reading\\n * the c* fields from p->signal from races with exit.c updating those\\n * fields when reaping, so a sample either gets all the additions of a\\n * given child after it\\'s reaped, or none so this sample is before reaping.\\n *\\n * Locking:\\n * We need to take the siglock for CHILDEREN, SELF and BOTH\\n * for  the cases current multithreaded, non-current single threaded\\n * non-current multithreaded.  Thread traversal is now safe with\\n * the siglock held.\\n * Strictly speaking, we donot need to take the siglock if we are current and\\n * single threaded,  as no one else can take our signal_struct away, no one\\n * else can  reap the  children to update signal->c* counters, and no one else\\n * can race with the signal-> fields. If we do not take any lock, the\\n * signal-> fields could be read out of order while another thread was just\\n * exiting. So we should  place a read memory barrier when we avoid the lock.\\n * On the writer side,  write memory barrier is implied in  __exit_signal\\n * as __exit_signal releases  the siglock spinlock after updating the signal->\\n * fields. But we don\\'t do this yet to keep things simple.\\n *\\n */\\n\\nstatic void accumulate_thread_rusage(struct task_struct *t, struct rusage *r)\\n{\\n\\tr->ru_nvcsw += t->nvcsw;\\n\\tr->ru_nivcsw += t->nivcsw;\\n\\tr->ru_minflt += t->min_flt;\\n\\tr->ru_majflt += t->maj_flt;\\n\\tr->ru_inblock += task_io_get_inblock(t);\\n\\tr->ru_oublock += task_io_get_oublock(t);\\n}\\n\\nvoid getrusage(struct task_struct *p, int who, struct rusage *r)\\n{\\n\\tstruct task_struct *t;\\n\\tunsigned long flags;\\n\\tu64 tgutime, tgstime, utime, stime;\\n\\tunsigned long maxrss = 0;\\n\\n\\tmemset((char *)r, 0, sizeof (*r));\\n\\tutime = stime = 0;\\n\\n\\tif (who == RUSAGE_THREAD) {\\n\\t\\ttask_cputime_adjusted(current, &utime, &stime);\\n\\t\\taccumulate_thread_rusage(p, r);\\n\\t\\tmaxrss = p->signal->maxrss;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tif (!lock_task_sighand(p, &flags))\\n\\t\\treturn;\\n\\n\\tswitch (who) {\\n\\tcase RUSAGE_BOTH:\\n\\tcase RUSAGE_CHILDREN:\\n\\t\\tutime = p->signal->cutime;\\n\\t\\tstime = p->signal->cstime;\\n\\t\\tr->ru_nvcsw = p->signal->cnvcsw;\\n\\t\\tr->ru_nivcsw = p->signal->cnivcsw;\\n\\t\\tr->ru_minflt = p->signal->cmin_flt;\\n\\t\\tr->ru_majflt = p->signal->cmaj_flt;\\n\\t\\tr->ru_inblock = p->signal->cinblock;\\n\\t\\tr->ru_oublock = p->signal->coublock;\\n\\t\\tmaxrss = p->signal->cmaxrss;\\n\\n\\t\\tif (who == RUSAGE_CHILDREN)\\n\\t\\t\\tbreak;\\n\\n\\tcase RUSAGE_SELF:\\n\\t\\tthread_group_cputime_adjusted(p, &tgutime, &tgstime);\\n\\t\\tutime += tgutime;\\n\\t\\tstime += tgstime;\\n\\t\\tr->ru_nvcsw += p->signal->nvcsw;\\n\\t\\tr->ru_nivcsw += p->signal->nivcsw;\\n\\t\\tr->ru_minflt += p->signal->min_flt;\\n\\t\\tr->ru_majflt += p->signal->maj_flt;\\n\\t\\tr->ru_inblock += p->signal->inblock;\\n\\t\\tr->ru_oublock += p->signal->oublock;\\n\\t\\tif (maxrss < p->signal->maxrss)\\n\\t\\t\\tmaxrss = p->signal->maxrss;\\n\\t\\tt = p;\\n\\t\\tdo {\\n\\t\\t\\taccumulate_thread_rusage(t, r);\\n\\t\\t} while_each_thread(p, t);\\n\\t\\tbreak;\\n\\n\\tdefault:\\n\\t\\tBUG();\\n\\t}\\n\\tunlock_task_sighand(p, &flags);\\n\\nout:\\n\\tr->ru_utime = ns_to_timeval(utime);\\n\\tr->ru_stime = ns_to_timeval(stime);\\n\\n\\tif (who != RUSAGE_CHILDREN) {\\n\\t\\tstruct mm_struct *mm = get_task_mm(p);\\n\\n\\t\\tif (mm) {\\n\\t\\t\\tsetmax_mm_hiwater_rss(&maxrss, mm);\\n\\t\\t\\tmmput(mm);\\n\\t\\t}\\n\\t}\\n\\tr->ru_maxrss = maxrss * (PAGE_SIZE / 1024); /* convert pages to KBs */\\n}\\n\\nSYSCALL_DEFINE2(getrusage, int, who, struct rusage __user *, ru)\\n{\\n\\tstruct rusage r;\\n\\n\\tif (who != RUSAGE_SELF && who != RUSAGE_CHILDREN &&\\n\\t    who != RUSAGE_THREAD)\\n\\t\\treturn -EINVAL;\\n\\n\\tgetrusage(current, who, &r);\\n\\treturn copy_to_user(ru, &r, sizeof(r)) ? -EFAULT : 0;\\n}\\n\\n#ifdef CONFIG_COMPAT\\nCOMPAT_SYSCALL_DEFINE2(getrusage, int, who, struct compat_rusage __user *, ru)\\n{\\n\\tstruct rusage r;\\n\\n\\tif (who != RUSAGE_SELF && who != RUSAGE_CHILDREN &&\\n\\t    who != RUSAGE_THREAD)\\n\\t\\treturn -EINVAL;\\n\\n\\tgetrusage(current, who, &r);\\n\\treturn put_compat_rusage(&r, ru);\\n}\\n#endif\\n\\nSYSCALL_DEFINE1(umask, int, mask)\\n{\\n\\tmask = xchg(&current->fs->umask, mask & S_IRWXUGO);\\n\\treturn mask;\\n}\\n\\nstatic int prctl_set_mm_exe_file(struct mm_struct *mm, unsigned int fd)\\n{\\n\\tstruct fd exe;\\n\\tstruct file *old_exe, *exe_file;\\n\\tstruct inode *inode;\\n\\tint err;\\n\\n\\texe = fdget(fd);\\n\\tif (!exe.file)\\n\\t\\treturn -EBADF;\\n\\n\\tinode = file_inode(exe.file);\\n\\n\\t/*\\n\\t * Because the original mm->exe_file points to executable file, make\\n\\t * sure that this one is executable as well, to avoid breaking an\\n\\t * overall picture.\\n\\t */\\n\\terr = -EACCES;\\n\\tif (!S_ISREG(inode->i_mode) || path_noexec(&exe.file->f_path))\\n\\t\\tgoto exit;\\n\\n\\terr = inode_permission(inode, MAY_EXEC);\\n\\tif (err)\\n\\t\\tgoto exit;\\n\\n\\t/*\\n\\t * Forbid mm->exe_file change if old file still mapped.\\n\\t */\\n\\texe_file = get_mm_exe_file(mm);\\n\\terr = -EBUSY;\\n\\tif (exe_file) {\\n\\t\\tstruct vm_area_struct *vma;\\n\\n\\t\\tdown_read(&mm->mmap_sem);\\n\\t\\tfor (vma = mm->mmap; vma; vma = vma->vm_next) {\\n\\t\\t\\tif (!vma->vm_file)\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\tif (path_equal(&vma->vm_file->f_path,\\n\\t\\t\\t\\t       &exe_file->f_path))\\n\\t\\t\\t\\tgoto exit_err;\\n\\t\\t}\\n\\n\\t\\tup_read(&mm->mmap_sem);\\n\\t\\tfput(exe_file);\\n\\t}\\n\\n\\terr = 0;\\n\\t/* set the new file, lockless */\\n\\tget_file(exe.file);\\n\\told_exe = xchg(&mm->exe_file, exe.file);\\n\\tif (old_exe)\\n\\t\\tfput(old_exe);\\nexit:\\n\\tfdput(exe);\\n\\treturn err;\\nexit_err:\\n\\tup_read(&mm->mmap_sem);\\n\\tfput(exe_file);\\n\\tgoto exit;\\n}\\n\\n/*\\n * WARNING: we don\\'t require any capability here so be very careful\\n * in what is allowed for modification from userspace.\\n */\\nstatic int validate_prctl_map(struct prctl_mm_map *prctl_map)\\n{\\n\\tunsigned long mmap_max_addr = TASK_SIZE;\\n\\tstruct mm_struct *mm = current->mm;\\n\\tint error = -EINVAL, i;\\n\\n\\tstatic const unsigned char offsets[] = {\\n\\t\\toffsetof(struct prctl_mm_map, start_code),\\n\\t\\toffsetof(struct prctl_mm_map, end_code),\\n\\t\\toffsetof(struct prctl_mm_map, start_data),\\n\\t\\toffsetof(struct prctl_mm_map, end_data),\\n\\t\\toffsetof(struct prctl_mm_map, start_brk),\\n\\t\\toffsetof(struct prctl_mm_map, brk),\\n\\t\\toffsetof(struct prctl_mm_map, start_stack),\\n\\t\\toffsetof(struct prctl_mm_map, arg_start),\\n\\t\\toffsetof(struct prctl_mm_map, arg_end),\\n\\t\\toffsetof(struct prctl_mm_map, env_start),\\n\\t\\toffsetof(struct prctl_mm_map, env_end),\\n\\t};\\n\\n\\t/*\\n\\t * Make sure the members are not somewhere outside\\n\\t * of allowed address space.\\n\\t */\\n\\tfor (i = 0; i < ARRAY_SIZE(offsets); i++) {\\n\\t\\tu64 val = *(u64 *)((char *)prctl_map + offsets[i]);\\n\\n\\t\\tif ((unsigned long)val >= mmap_max_addr ||\\n\\t\\t    (unsigned long)val < mmap_min_addr)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\t/*\\n\\t * Make sure the pairs are ordered.\\n\\t */\\n#define __prctl_check_order(__m1, __op, __m2)\\t\\t\\t\\t\\\\\\n\\t((unsigned long)prctl_map->__m1 __op\\t\\t\\t\\t\\\\\\n\\t (unsigned long)prctl_map->__m2) ? 0 : -EINVAL\\n\\terror  = __prctl_check_order(start_code, <, end_code);\\n\\terror |= __prctl_check_order(start_data, <, end_data);\\n\\terror |= __prctl_check_order(start_brk, <=, brk);\\n\\terror |= __prctl_check_order(arg_start, <=, arg_end);\\n\\terror |= __prctl_check_order(env_start, <=, env_end);\\n\\tif (error)\\n\\t\\tgoto out;\\n#undef __prctl_check_order\\n\\n\\terror = -EINVAL;\\n\\n\\t/*\\n\\t * @brk should be after @end_data in traditional maps.\\n\\t */\\n\\tif (prctl_map->start_brk <= prctl_map->end_data ||\\n\\t    prctl_map->brk <= prctl_map->end_data)\\n\\t\\tgoto out;\\n\\n\\t/*\\n\\t * Neither we should allow to override limits if they set.\\n\\t */\\n\\tif (check_data_rlimit(rlimit(RLIMIT_DATA), prctl_map->brk,\\n\\t\\t\\t      prctl_map->start_brk, prctl_map->end_data,\\n\\t\\t\\t      prctl_map->start_data))\\n\\t\\t\\tgoto out;\\n\\n\\t/*\\n\\t * Someone is trying to cheat the auxv vector.\\n\\t */\\n\\tif (prctl_map->auxv_size) {\\n\\t\\tif (!prctl_map->auxv || prctl_map->auxv_size > sizeof(mm->saved_auxv))\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\t/*\\n\\t * Finally, make sure the caller has the rights to\\n\\t * change /proc/pid/exe link: only local sys admin should\\n\\t * be allowed to.\\n\\t */\\n\\tif (prctl_map->exe_fd != (u32)-1) {\\n\\t\\tif (!ns_capable(current_user_ns(), CAP_SYS_ADMIN))\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\terror = 0;\\nout:\\n\\treturn error;\\n}\\n\\n#ifdef CONFIG_CHECKPOINT_RESTORE\\nstatic int prctl_set_mm_map(int opt, const void __user *addr, unsigned long data_size)\\n{\\n\\tstruct prctl_mm_map prctl_map = { .exe_fd = (u32)-1, };\\n\\tunsigned long user_auxv[AT_VECTOR_SIZE];\\n\\tstruct mm_struct *mm = current->mm;\\n\\tint error;\\n\\n\\tBUILD_BUG_ON(sizeof(user_auxv) != sizeof(mm->saved_auxv));\\n\\tBUILD_BUG_ON(sizeof(struct prctl_mm_map) > 256);\\n\\n\\tif (opt == PR_SET_MM_MAP_SIZE)\\n\\t\\treturn put_user((unsigned int)sizeof(prctl_map),\\n\\t\\t\\t\\t(unsigned int __user *)addr);\\n\\n\\tif (data_size != sizeof(prctl_map))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (copy_from_user(&prctl_map, addr, sizeof(prctl_map)))\\n\\t\\treturn -EFAULT;\\n\\n\\terror = validate_prctl_map(&prctl_map);\\n\\tif (error)\\n\\t\\treturn error;\\n\\n\\tif (prctl_map.auxv_size) {\\n\\t\\tmemset(user_auxv, 0, sizeof(user_auxv));\\n\\t\\tif (copy_from_user(user_auxv,\\n\\t\\t\\t\\t   (const void __user *)prctl_map.auxv,\\n\\t\\t\\t\\t   prctl_map.auxv_size))\\n\\t\\t\\treturn -EFAULT;\\n\\n\\t\\t/* Last entry must be AT_NULL as specification requires */\\n\\t\\tuser_auxv[AT_VECTOR_SIZE - 2] = AT_NULL;\\n\\t\\tuser_auxv[AT_VECTOR_SIZE - 1] = AT_NULL;\\n\\t}\\n\\n\\tif (prctl_map.exe_fd != (u32)-1) {\\n\\t\\terror = prctl_set_mm_exe_file(mm, prctl_map.exe_fd);\\n\\t\\tif (error)\\n\\t\\t\\treturn error;\\n\\t}\\n\\n\\t/*\\n\\t * arg_lock protects concurent updates but we still need mmap_sem for\\n\\t * read to exclude races with sys_brk.\\n\\t */\\n\\tdown_read(&mm->mmap_sem);\\n\\n\\t/*\\n\\t * We don\\'t validate if these members are pointing to\\n\\t * real present VMAs because application may have correspond\\n\\t * VMAs already unmapped and kernel uses these members for statistics\\n\\t * output in procfs mostly, except\\n\\t *\\n\\t *  - @start_brk/@brk which are used in do_brk but kernel lookups\\n\\t *    for VMAs when updating these memvers so anything wrong written\\n\\t *    here cause kernel to swear at userspace program but won\\'t lead\\n\\t *    to any problem in kernel itself\\n\\t */\\n\\n\\tspin_lock(&mm->arg_lock);\\n\\tmm->start_code\\t= prctl_map.start_code;\\n\\tmm->end_code\\t= prctl_map.end_code;\\n\\tmm->start_data\\t= prctl_map.start_data;\\n\\tmm->end_data\\t= prctl_map.end_data;\\n\\tmm->start_brk\\t= prctl_map.start_brk;\\n\\tmm->brk\\t\\t= prctl_map.brk;\\n\\tmm->start_stack\\t= prctl_map.start_stack;\\n\\tmm->arg_start\\t= prctl_map.arg_start;\\n\\tmm->arg_end\\t= prctl_map.arg_end;\\n\\tmm->env_start\\t= prctl_map.env_start;\\n\\tmm->env_end\\t= prctl_map.env_end;\\n\\tspin_unlock(&mm->arg_lock);\\n\\n\\t/*\\n\\t * Note this update of @saved_auxv is lockless thus\\n\\t * if someone reads this member in procfs while we\\'re\\n\\t * updating -- it may get partly updated results. It\\'s\\n\\t * known and acceptable trade off: we leave it as is to\\n\\t * not introduce additional locks here making the kernel\\n\\t * more complex.\\n\\t */\\n\\tif (prctl_map.auxv_size)\\n\\t\\tmemcpy(mm->saved_auxv, user_auxv, sizeof(user_auxv));\\n\\n\\tup_read(&mm->mmap_sem);\\n\\treturn 0;\\n}\\n#endif /* CONFIG_CHECKPOINT_RESTORE */\\n\\nstatic int prctl_set_auxv(struct mm_struct *mm, unsigned long addr,\\n\\t\\t\\t  unsigned long len)\\n{\\n\\t/*\\n\\t * This doesn\\'t move the auxiliary vector itself since it\\'s pinned to\\n\\t * mm_struct, but it permits filling the vector with new values.  It\\'s\\n\\t * up to the caller to provide sane values here, otherwise userspace\\n\\t * tools which use this vector might be unhappy.\\n\\t */\\n\\tunsigned long user_auxv[AT_VECTOR_SIZE];\\n\\n\\tif (len > sizeof(user_auxv))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (copy_from_user(user_auxv, (const void __user *)addr, len))\\n\\t\\treturn -EFAULT;\\n\\n\\t/* Make sure the last entry is always AT_NULL */\\n\\tuser_auxv[AT_VECTOR_SIZE - 2] = 0;\\n\\tuser_auxv[AT_VECTOR_SIZE - 1] = 0;\\n\\n\\tBUILD_BUG_ON(sizeof(user_auxv) != sizeof(mm->saved_auxv));\\n\\n\\ttask_lock(current);\\n\\tmemcpy(mm->saved_auxv, user_auxv, len);\\n\\ttask_unlock(current);\\n\\n\\treturn 0;\\n}\\n\\nstatic int prctl_set_mm(int opt, unsigned long addr,\\n\\t\\t\\tunsigned long arg4, unsigned long arg5)\\n{\\n\\tstruct mm_struct *mm = current->mm;\\n\\tstruct prctl_mm_map prctl_map;\\n\\tstruct vm_area_struct *vma;\\n\\tint error;\\n\\n\\tif (arg5 || (arg4 && (opt != PR_SET_MM_AUXV &&\\n\\t\\t\\t      opt != PR_SET_MM_MAP &&\\n\\t\\t\\t      opt != PR_SET_MM_MAP_SIZE)))\\n\\t\\treturn -EINVAL;\\n\\n#ifdef CONFIG_CHECKPOINT_RESTORE\\n\\tif (opt == PR_SET_MM_MAP || opt == PR_SET_MM_MAP_SIZE)\\n\\t\\treturn prctl_set_mm_map(opt, (const void __user *)addr, arg4);\\n#endif\\n\\n\\tif (!capable(CAP_SYS_RESOURCE))\\n\\t\\treturn -EPERM;\\n\\n\\tif (opt == PR_SET_MM_EXE_FILE)\\n\\t\\treturn prctl_set_mm_exe_file(mm, (unsigned int)addr);\\n\\n\\tif (opt == PR_SET_MM_AUXV)\\n\\t\\treturn prctl_set_auxv(mm, addr, arg4);\\n\\n\\tif (addr >= TASK_SIZE || addr < mmap_min_addr)\\n\\t\\treturn -EINVAL;\\n\\n\\terror = -EINVAL;\\n\\n\\tdown_write(&mm->mmap_sem);\\n\\tvma = find_vma(mm, addr);\\n\\n\\tprctl_map.start_code\\t= mm->start_code;\\n\\tprctl_map.end_code\\t= mm->end_code;\\n\\tprctl_map.start_data\\t= mm->start_data;\\n\\tprctl_map.end_data\\t= mm->end_data;\\n\\tprctl_map.start_brk\\t= mm->start_brk;\\n\\tprctl_map.brk\\t\\t= mm->brk;\\n\\tprctl_map.start_stack\\t= mm->start_stack;\\n\\tprctl_map.arg_start\\t= mm->arg_start;\\n\\tprctl_map.arg_end\\t= mm->arg_end;\\n\\tprctl_map.env_start\\t= mm->env_start;\\n\\tprctl_map.env_end\\t= mm->env_end;\\n\\tprctl_map.auxv\\t\\t= NULL;\\n\\tprctl_map.auxv_size\\t= 0;\\n\\tprctl_map.exe_fd\\t= -1;\\n\\n\\tswitch (opt) {\\n\\tcase PR_SET_MM_START_CODE:\\n\\t\\tprctl_map.start_code = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_END_CODE:\\n\\t\\tprctl_map.end_code = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_START_DATA:\\n\\t\\tprctl_map.start_data = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_END_DATA:\\n\\t\\tprctl_map.end_data = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_START_STACK:\\n\\t\\tprctl_map.start_stack = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_START_BRK:\\n\\t\\tprctl_map.start_brk = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_BRK:\\n\\t\\tprctl_map.brk = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_ARG_START:\\n\\t\\tprctl_map.arg_start = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_ARG_END:\\n\\t\\tprctl_map.arg_end = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_ENV_START:\\n\\t\\tprctl_map.env_start = addr;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM_ENV_END:\\n\\t\\tprctl_map.env_end = addr;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tgoto out;\\n\\t}\\n\\n\\terror = validate_prctl_map(&prctl_map);\\n\\tif (error)\\n\\t\\tgoto out;\\n\\n\\tswitch (opt) {\\n\\t/*\\n\\t * If command line arguments and environment\\n\\t * are placed somewhere else on stack, we can\\n\\t * set them up here, ARG_START/END to setup\\n\\t * command line argumets and ENV_START/END\\n\\t * for environment.\\n\\t */\\n\\tcase PR_SET_MM_START_STACK:\\n\\tcase PR_SET_MM_ARG_START:\\n\\tcase PR_SET_MM_ARG_END:\\n\\tcase PR_SET_MM_ENV_START:\\n\\tcase PR_SET_MM_ENV_END:\\n\\t\\tif (!vma) {\\n\\t\\t\\terror = -EFAULT;\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\n\\n\\tmm->start_code\\t= prctl_map.start_code;\\n\\tmm->end_code\\t= prctl_map.end_code;\\n\\tmm->start_data\\t= prctl_map.start_data;\\n\\tmm->end_data\\t= prctl_map.end_data;\\n\\tmm->start_brk\\t= prctl_map.start_brk;\\n\\tmm->brk\\t\\t= prctl_map.brk;\\n\\tmm->start_stack\\t= prctl_map.start_stack;\\n\\tmm->arg_start\\t= prctl_map.arg_start;\\n\\tmm->arg_end\\t= prctl_map.arg_end;\\n\\tmm->env_start\\t= prctl_map.env_start;\\n\\tmm->env_end\\t= prctl_map.env_end;\\n\\n\\terror = 0;\\nout:\\n\\tup_write(&mm->mmap_sem);\\n\\treturn error;\\n}\\n\\n#ifdef CONFIG_CHECKPOINT_RESTORE\\nstatic int prctl_get_tid_address(struct task_struct *me, int __user **tid_addr)\\n{\\n\\treturn put_user(me->clear_child_tid, tid_addr);\\n}\\n#else\\nstatic int prctl_get_tid_address(struct task_struct *me, int __user **tid_addr)\\n{\\n\\treturn -EINVAL;\\n}\\n#endif\\n\\nstatic int propagate_has_child_subreaper(struct task_struct *p, void *data)\\n{\\n\\t/*\\n\\t * If task has has_child_subreaper - all its decendants\\n\\t * already have these flag too and new decendants will\\n\\t * inherit it on fork, skip them.\\n\\t *\\n\\t * If we\\'ve found child_reaper - skip descendants in\\n\\t * it\\'s subtree as they will never get out pidns.\\n\\t */\\n\\tif (p->signal->has_child_subreaper ||\\n\\t    is_child_reaper(task_pid(p)))\\n\\t\\treturn 0;\\n\\n\\tp->signal->has_child_subreaper = 1;\\n\\treturn 1;\\n}\\n\\nint __weak arch_prctl_spec_ctrl_get(struct task_struct *t, unsigned long which)\\n{\\n\\treturn -EINVAL;\\n}\\n\\nint __weak arch_prctl_spec_ctrl_set(struct task_struct *t, unsigned long which,\\n\\t\\t\\t\\t    unsigned long ctrl)\\n{\\n\\treturn -EINVAL;\\n}\\n\\nSYSCALL_DEFINE5(prctl, int, option, unsigned long, arg2, unsigned long, arg3,\\n\\t\\tunsigned long, arg4, unsigned long, arg5)\\n{\\n\\tstruct task_struct *me = current;\\n\\tunsigned char comm[sizeof(me->comm)];\\n\\tlong error;\\n\\n\\terror = security_task_prctl(option, arg2, arg3, arg4, arg5);\\n\\tif (error != -ENOSYS)\\n\\t\\treturn error;\\n\\n\\terror = 0;\\n\\tswitch (option) {\\n\\tcase PR_SET_PDEATHSIG:\\n\\t\\tif (!valid_signal(arg2)) {\\n\\t\\t\\terror = -EINVAL;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tme->pdeath_signal = arg2;\\n\\t\\tbreak;\\n\\tcase PR_GET_PDEATHSIG:\\n\\t\\terror = put_user(me->pdeath_signal, (int __user *)arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_DUMPABLE:\\n\\t\\terror = get_dumpable(me->mm);\\n\\t\\tbreak;\\n\\tcase PR_SET_DUMPABLE:\\n\\t\\tif (arg2 != SUID_DUMP_DISABLE && arg2 != SUID_DUMP_USER) {\\n\\t\\t\\terror = -EINVAL;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tset_dumpable(me->mm, arg2);\\n\\t\\tbreak;\\n\\n\\tcase PR_SET_UNALIGN:\\n\\t\\terror = SET_UNALIGN_CTL(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_UNALIGN:\\n\\t\\terror = GET_UNALIGN_CTL(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_FPEMU:\\n\\t\\terror = SET_FPEMU_CTL(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_FPEMU:\\n\\t\\terror = GET_FPEMU_CTL(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_FPEXC:\\n\\t\\terror = SET_FPEXC_CTL(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_FPEXC:\\n\\t\\terror = GET_FPEXC_CTL(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_TIMING:\\n\\t\\terror = PR_TIMING_STATISTICAL;\\n\\t\\tbreak;\\n\\tcase PR_SET_TIMING:\\n\\t\\tif (arg2 != PR_TIMING_STATISTICAL)\\n\\t\\t\\terror = -EINVAL;\\n\\t\\tbreak;\\n\\tcase PR_SET_NAME:\\n\\t\\tcomm[sizeof(me->comm) - 1] = 0;\\n\\t\\tif (strncpy_from_user(comm, (char __user *)arg2,\\n\\t\\t\\t\\t      sizeof(me->comm) - 1) < 0)\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tset_task_comm(me, comm);\\n\\t\\tproc_comm_connector(me);\\n\\t\\tbreak;\\n\\tcase PR_GET_NAME:\\n\\t\\tget_task_comm(comm, me);\\n\\t\\tif (copy_to_user((char __user *)arg2, comm, sizeof(comm)))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tbreak;\\n\\tcase PR_GET_ENDIAN:\\n\\t\\terror = GET_ENDIAN(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_ENDIAN:\\n\\t\\terror = SET_ENDIAN(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_SECCOMP:\\n\\t\\terror = prctl_get_seccomp();\\n\\t\\tbreak;\\n\\tcase PR_SET_SECCOMP:\\n\\t\\terror = prctl_set_seccomp(arg2, (char __user *)arg3);\\n\\t\\tbreak;\\n\\tcase PR_GET_TSC:\\n\\t\\terror = GET_TSC_CTL(arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_TSC:\\n\\t\\terror = SET_TSC_CTL(arg2);\\n\\t\\tbreak;\\n\\tcase PR_TASK_PERF_EVENTS_DISABLE:\\n\\t\\terror = perf_event_task_disable();\\n\\t\\tbreak;\\n\\tcase PR_TASK_PERF_EVENTS_ENABLE:\\n\\t\\terror = perf_event_task_enable();\\n\\t\\tbreak;\\n\\tcase PR_GET_TIMERSLACK:\\n\\t\\tif (current->timer_slack_ns > ULONG_MAX)\\n\\t\\t\\terror = ULONG_MAX;\\n\\t\\telse\\n\\t\\t\\terror = current->timer_slack_ns;\\n\\t\\tbreak;\\n\\tcase PR_SET_TIMERSLACK:\\n\\t\\tif (arg2 <= 0)\\n\\t\\t\\tcurrent->timer_slack_ns =\\n\\t\\t\\t\\t\\tcurrent->default_timer_slack_ns;\\n\\t\\telse\\n\\t\\t\\tcurrent->timer_slack_ns = arg2;\\n\\t\\tbreak;\\n\\tcase PR_MCE_KILL:\\n\\t\\tif (arg4 | arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tswitch (arg2) {\\n\\t\\tcase PR_MCE_KILL_CLEAR:\\n\\t\\t\\tif (arg3 != 0)\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\tcurrent->flags &= ~PF_MCE_PROCESS;\\n\\t\\t\\tbreak;\\n\\t\\tcase PR_MCE_KILL_SET:\\n\\t\\t\\tcurrent->flags |= PF_MCE_PROCESS;\\n\\t\\t\\tif (arg3 == PR_MCE_KILL_EARLY)\\n\\t\\t\\t\\tcurrent->flags |= PF_MCE_EARLY;\\n\\t\\t\\telse if (arg3 == PR_MCE_KILL_LATE)\\n\\t\\t\\t\\tcurrent->flags &= ~PF_MCE_EARLY;\\n\\t\\t\\telse if (arg3 == PR_MCE_KILL_DEFAULT)\\n\\t\\t\\t\\tcurrent->flags &=\\n\\t\\t\\t\\t\\t\\t~(PF_MCE_EARLY|PF_MCE_PROCESS);\\n\\t\\t\\telse\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\t\\tbreak;\\n\\tcase PR_MCE_KILL_GET:\\n\\t\\tif (arg2 | arg3 | arg4 | arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (current->flags & PF_MCE_PROCESS)\\n\\t\\t\\terror = (current->flags & PF_MCE_EARLY) ?\\n\\t\\t\\t\\tPR_MCE_KILL_EARLY : PR_MCE_KILL_LATE;\\n\\t\\telse\\n\\t\\t\\terror = PR_MCE_KILL_DEFAULT;\\n\\t\\tbreak;\\n\\tcase PR_SET_MM:\\n\\t\\terror = prctl_set_mm(arg2, arg3, arg4, arg5);\\n\\t\\tbreak;\\n\\tcase PR_GET_TID_ADDRESS:\\n\\t\\terror = prctl_get_tid_address(me, (int __user **)arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_CHILD_SUBREAPER:\\n\\t\\tme->signal->is_child_subreaper = !!arg2;\\n\\t\\tif (!arg2)\\n\\t\\t\\tbreak;\\n\\n\\t\\twalk_process_tree(me, propagate_has_child_subreaper, NULL);\\n\\t\\tbreak;\\n\\tcase PR_GET_CHILD_SUBREAPER:\\n\\t\\terror = put_user(me->signal->is_child_subreaper,\\n\\t\\t\\t\\t (int __user *)arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_NO_NEW_PRIVS:\\n\\t\\tif (arg2 != 1 || arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\n\\t\\ttask_set_no_new_privs(current);\\n\\t\\tbreak;\\n\\tcase PR_GET_NO_NEW_PRIVS:\\n\\t\\tif (arg2 || arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\treturn task_no_new_privs(current) ? 1 : 0;\\n\\tcase PR_GET_THP_DISABLE:\\n\\t\\tif (arg2 || arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = !!test_bit(MMF_DISABLE_THP, &me->mm->flags);\\n\\t\\tbreak;\\n\\tcase PR_SET_THP_DISABLE:\\n\\t\\tif (arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tif (down_write_killable(&me->mm->mmap_sem))\\n\\t\\t\\treturn -EINTR;\\n\\t\\tif (arg2)\\n\\t\\t\\tset_bit(MMF_DISABLE_THP, &me->mm->flags);\\n\\t\\telse\\n\\t\\t\\tclear_bit(MMF_DISABLE_THP, &me->mm->flags);\\n\\t\\tup_write(&me->mm->mmap_sem);\\n\\t\\tbreak;\\n\\tcase PR_MPX_ENABLE_MANAGEMENT:\\n\\t\\tif (arg2 || arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = MPX_ENABLE_MANAGEMENT();\\n\\t\\tbreak;\\n\\tcase PR_MPX_DISABLE_MANAGEMENT:\\n\\t\\tif (arg2 || arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = MPX_DISABLE_MANAGEMENT();\\n\\t\\tbreak;\\n\\tcase PR_SET_FP_MODE:\\n\\t\\terror = SET_FP_MODE(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_GET_FP_MODE:\\n\\t\\terror = GET_FP_MODE(me);\\n\\t\\tbreak;\\n\\tcase PR_SVE_SET_VL:\\n\\t\\terror = SVE_SET_VL(arg2);\\n\\t\\tbreak;\\n\\tcase PR_SVE_GET_VL:\\n\\t\\terror = SVE_GET_VL();\\n\\t\\tbreak;\\n\\tcase PR_GET_SPECULATION_CTRL:\\n\\t\\tif (arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = arch_prctl_spec_ctrl_get(me, arg2);\\n\\t\\tbreak;\\n\\tcase PR_SET_SPECULATION_CTRL:\\n\\t\\tif (arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = arch_prctl_spec_ctrl_set(me, arg2, arg3);\\n\\t\\tbreak;\\n\\tcase PR_PAC_RESET_KEYS:\\n\\t\\tif (arg3 || arg4 || arg5)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\terror = PAC_RESET_KEYS(me, arg2);\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\terror = -EINVAL;\\n\\t\\tbreak;\\n\\t}\\n\\treturn error;\\n}\\n\\nSYSCALL_DEFINE3(getcpu, unsigned __user *, cpup, unsigned __user *, nodep,\\n\\t\\tstruct getcpu_cache __user *, unused)\\n{\\n\\tint err = 0;\\n\\tint cpu = raw_smp_processor_id();\\n\\n\\tif (cpup)\\n\\t\\terr |= put_user(cpu, cpup);\\n\\tif (nodep)\\n\\t\\terr |= put_user(cpu_to_node(cpu), nodep);\\n\\treturn err ? -EFAULT : 0;\\n}\\n\\n/**\\n * do_sysinfo - fill in sysinfo struct\\n * @info: pointer to buffer to fill\\n */\\nstatic int do_sysinfo(struct sysinfo *info)\\n{\\n\\tunsigned long mem_total, sav_total;\\n\\tunsigned int mem_unit, bitcount;\\n\\tstruct timespec64 tp;\\n\\n\\tmemset(info, 0, sizeof(struct sysinfo));\\n\\n\\tktime_get_boottime_ts64(&tp);\\n\\tinfo->uptime = tp.tv_sec + (tp.tv_nsec ? 1 : 0);\\n\\n\\tget_avenrun(info->loads, 0, SI_LOAD_SHIFT - FSHIFT);\\n\\n\\tinfo->procs = nr_threads;\\n\\n\\tsi_meminfo(info);\\n\\tsi_swapinfo(info);\\n\\n\\t/*\\n\\t * If the sum of all the available memory (i.e. ram + swap)\\n\\t * is less than can be stored in a 32 bit unsigned long then\\n\\t * we can be binary compatible with 2.2.x kernels.  If not,\\n\\t * well, in that case 2.2.x was broken anyways...\\n\\t *\\n\\t *  -Erik Andersen <andersee@debian.org>\\n\\t */\\n\\n\\tmem_total = info->totalram + info->totalswap;\\n\\tif (mem_total < info->totalram || mem_total < info->totalswap)\\n\\t\\tgoto out;\\n\\tbitcount = 0;\\n\\tmem_unit = info->mem_unit;\\n\\twhile (mem_unit > 1) {\\n\\t\\tbitcount++;\\n\\t\\tmem_unit >>= 1;\\n\\t\\tsav_total = mem_total;\\n\\t\\tmem_total <<= 1;\\n\\t\\tif (mem_total < sav_total)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\t/*\\n\\t * If mem_total did not overflow, multiply all memory values by\\n\\t * info->mem_unit and set it to 1.  This leaves things compatible\\n\\t * with 2.2.x, and also retains compatibility with earlier 2.4.x\\n\\t * kernels...\\n\\t */\\n\\n\\tinfo->mem_unit = 1;\\n\\tinfo->totalram <<= bitcount;\\n\\tinfo->freeram <<= bitcount;\\n\\tinfo->sharedram <<= bitcount;\\n\\tinfo->bufferram <<= bitcount;\\n\\tinfo->totalswap <<= bitcount;\\n\\tinfo->freeswap <<= bitcount;\\n\\tinfo->totalhigh <<= bitcount;\\n\\tinfo->freehigh <<= bitcount;\\n\\nout:\\n\\treturn 0;\\n}\\n\\nSYSCALL_DEFINE1(sysinfo, struct sysinfo __user *, info)\\n{\\n\\tstruct sysinfo val;\\n\\n\\tdo_sysinfo(&val);\\n\\n\\tif (copy_to_user(info, &val, sizeof(struct sysinfo)))\\n\\t\\treturn -EFAULT;\\n\\n\\treturn 0;\\n}\\n\\n#ifdef CONFIG_COMPAT\\nstruct compat_sysinfo {\\n\\ts32 uptime;\\n\\tu32 loads[3];\\n\\tu32 totalram;\\n\\tu32 freeram;\\n\\tu32 sharedram;\\n\\tu32 bufferram;\\n\\tu32 totalswap;\\n\\tu32 freeswap;\\n\\tu16 procs;\\n\\tu16 pad;\\n\\tu32 totalhigh;\\n\\tu32 freehigh;\\n\\tu32 mem_unit;\\n\\tchar _f[20-2*sizeof(u32)-sizeof(int)];\\n};\\n\\nCOMPAT_SYSCALL_DEFINE1(sysinfo, struct compat_sysinfo __user *, info)\\n{\\n\\tstruct sysinfo s;\\n\\n\\tdo_sysinfo(&s);\\n\\n\\t/* Check to see if any memory value is too large for 32-bit and scale\\n\\t *  down if needed\\n\\t */\\n\\tif (upper_32_bits(s.totalram) || upper_32_bits(s.totalswap)) {\\n\\t\\tint bitcount = 0;\\n\\n\\t\\twhile (s.mem_unit < PAGE_SIZE) {\\n\\t\\t\\ts.mem_unit <<= 1;\\n\\t\\t\\tbitcount++;\\n\\t\\t}\\n\\n\\t\\ts.totalram >>= bitcount;\\n\\t\\ts.freeram >>= bitcount;\\n\\t\\ts.sharedram >>= bitcount;\\n\\t\\ts.bufferram >>= bitcount;\\n\\t\\ts.totalswap >>= bitcount;\\n\\t\\ts.freeswap >>= bitcount;\\n\\t\\ts.totalhigh >>= bitcount;\\n\\t\\ts.freehigh >>= bitcount;\\n\\t}\\n\\n\\tif (!access_ok(info, sizeof(struct compat_sysinfo)) ||\\n\\t    __put_user(s.uptime, &info->uptime) ||\\n\\t    __put_user(s.loads[0], &info->loads[0]) ||\\n\\t    __put_user(s.loads[1], &info->loads[1]) ||\\n\\t    __put_user(s.loads[2], &info->loads[2]) ||\\n\\t    __put_user(s.totalram, &info->totalram) ||\\n\\t    __put_user(s.freeram, &info->freeram) ||\\n\\t    __put_user(s.sharedram, &info->sharedram) ||\\n\\t    __put_user(s.bufferram, &info->bufferram) ||\\n\\t    __put_user(s.totalswap, &info->totalswap) ||\\n\\t    __put_user(s.freeswap, &info->freeswap) ||\\n\\t    __put_user(s.procs, &info->procs) ||\\n\\t    __put_user(s.totalhigh, &info->totalhigh) ||\\n\\t    __put_user(s.freehigh, &info->freehigh) ||\\n\\t    __put_user(s.mem_unit, &info->mem_unit))\\n\\t\\treturn -EFAULT;\\n\\n\\treturn 0;\\n}\\n#endif /* CONFIG_COMPAT */\\n\\n/*\\n *  linux/kernel/compat.c\\n *\\n *  Kernel compatibililty routines for e.g. 32 bit syscall support\\n *  on 64 bit kernels.\\n *\\n *  Copyright (C) 2002-2003 Stephen Rothwell, IBM Corporation\\n *\\n *  This program is free software; you can redistribute it and/or modify\\n *  it under the terms of the GNU General Public License version 2 as\\n *  published by the Free Software Foundation.\\n */\\n\\n#include <linux/linkage.h>\\n#include <linux/compat.h>\\n#include <linux/errno.h>\\n#include <linux/time.h>\\n#include <linux/signal.h>\\n#include <linux/sched.h>\\t/* for MAX_SCHEDULE_TIMEOUT */\\n#include <linux/syscalls.h>\\n#include <linux/unistd.h>\\n#include <linux/security.h>\\n#include <linux/timex.h>\\n#include <linux/export.h>\\n#include <linux/migrate.h>\\n#include <linux/posix-timers.h>\\n#include <linux/times.h>\\n#include <linux/ptrace.h>\\n#include <linux/gfp.h>\\n\\n#include <linux/uaccess.h>\\n\\nint compat_get_timex(struct timex *txc, const struct compat_timex __user *utp)\\n{\\n\\tstruct compat_timex tx32;\\n\\n\\tmemset(txc, 0, sizeof(struct timex));\\n\\tif (copy_from_user(&tx32, utp, sizeof(struct compat_timex)))\\n\\t\\treturn -EFAULT;\\n\\n\\ttxc->modes = tx32.modes;\\n\\ttxc->offset = tx32.offset;\\n\\ttxc->freq = tx32.freq;\\n\\ttxc->maxerror = tx32.maxerror;\\n\\ttxc->esterror = tx32.esterror;\\n\\ttxc->status = tx32.status;\\n\\ttxc->constant = tx32.constant;\\n\\ttxc->precision = tx32.precision;\\n\\ttxc->tolerance = tx32.tolerance;\\n\\ttxc->time.tv_sec = tx32.time.tv_sec;\\n\\ttxc->time.tv_usec = tx32.time.tv_usec;\\n\\ttxc->tick = tx32.tick;\\n\\ttxc->ppsfreq = tx32.ppsfreq;\\n\\ttxc->jitter = tx32.jitter;\\n\\ttxc->shift = tx32.shift;\\n\\ttxc->stabil = tx32.stabil;\\n\\ttxc->jitcnt = tx32.jitcnt;\\n\\ttxc->calcnt = tx32.calcnt;\\n\\ttxc->errcnt = tx32.errcnt;\\n\\ttxc->stbcnt = tx32.stbcnt;\\n\\n\\treturn 0;\\n}\\n\\nint compat_put_timex(struct compat_timex __user *utp, const struct timex *txc)\\n{\\n\\tstruct compat_timex tx32;\\n\\n\\tmemset(&tx32, 0, sizeof(struct compat_timex));\\n\\ttx32.modes = txc->modes;\\n\\ttx32.offset = txc->offset;\\n\\ttx32.freq = txc->freq;\\n\\ttx32.maxerror = txc->maxerror;\\n\\ttx32.esterror = txc->esterror;\\n\\ttx32.status = txc->status;\\n\\ttx32.constant = txc->constant;\\n\\ttx32.precision = txc->precision;\\n\\ttx32.tolerance = txc->tolerance;\\n\\ttx32.time.tv_sec = txc->time.tv_sec;\\n\\ttx32.time.tv_usec = txc->time.tv_usec;\\n\\ttx32.tick = txc->tick;\\n\\ttx32.ppsfreq = txc->ppsfreq;\\n\\ttx32.jitter = txc->jitter;\\n\\ttx32.shift = txc->shift;\\n\\ttx32.stabil = txc->stabil;\\n\\ttx32.jitcnt = txc->jitcnt;\\n\\ttx32.calcnt = txc->calcnt;\\n\\ttx32.errcnt = txc->errcnt;\\n\\ttx32.stbcnt = txc->stbcnt;\\n\\ttx32.tai = txc->tai;\\n\\tif (copy_to_user(utp, &tx32, sizeof(struct compat_timex)))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n\\nstatic int __compat_get_timeval(struct timeval *tv, const struct old_timeval32 __user *ctv)\\n{\\n\\treturn (!access_ok(ctv, sizeof(*ctv)) ||\\n\\t\\t\\t__get_user(tv->tv_sec, &ctv->tv_sec) ||\\n\\t\\t\\t__get_user(tv->tv_usec, &ctv->tv_usec)) ? -EFAULT : 0;\\n}\\n\\nstatic int __compat_put_timeval(const struct timeval *tv, struct old_timeval32 __user *ctv)\\n{\\n\\treturn (!access_ok(ctv, sizeof(*ctv)) ||\\n\\t\\t\\t__put_user(tv->tv_sec, &ctv->tv_sec) ||\\n\\t\\t\\t__put_user(tv->tv_usec, &ctv->tv_usec)) ? -EFAULT : 0;\\n}\\n\\nstatic int __compat_get_timespec(struct timespec *ts, const struct old_timespec32 __user *cts)\\n{\\n\\treturn (!access_ok(cts, sizeof(*cts)) ||\\n\\t\\t\\t__get_user(ts->tv_sec, &cts->tv_sec) ||\\n\\t\\t\\t__get_user(ts->tv_nsec, &cts->tv_nsec)) ? -EFAULT : 0;\\n}\\n\\nstatic int __compat_put_timespec(const struct timespec *ts, struct old_timespec32 __user *cts)\\n{\\n\\treturn (!access_ok(cts, sizeof(*cts)) ||\\n\\t\\t\\t__put_user(ts->tv_sec, &cts->tv_sec) ||\\n\\t\\t\\t__put_user(ts->tv_nsec, &cts->tv_nsec)) ? -EFAULT : 0;\\n}\\n\\nint compat_get_timeval(struct timeval *tv, const void __user *utv)\\n{\\n\\tif (COMPAT_USE_64BIT_TIME)\\n\\t\\treturn copy_from_user(tv, utv, sizeof(*tv)) ? -EFAULT : 0;\\n\\telse\\n\\t\\treturn __compat_get_timeval(tv, utv);\\n}\\nEXPORT_SYMBOL_GPL(compat_get_timeval);\\n\\nint compat_put_timeval(const struct timeval *tv, void __user *utv)\\n{\\n\\tif (COMPAT_USE_64BIT_TIME)\\n\\t\\treturn copy_to_user(utv, tv, sizeof(*tv)) ? -EFAULT : 0;\\n\\telse\\n\\t\\treturn __compat_put_timeval(tv, utv);\\n}\\nEXPORT_SYMBOL_GPL(compat_put_timeval);\\n\\nint compat_get_timespec(struct timespec *ts, const void __user *uts)\\n{\\n\\tif (COMPAT_USE_64BIT_TIME)\\n\\t\\treturn copy_from_user(ts, uts, sizeof(*ts)) ? -EFAULT : 0;\\n\\telse\\n\\t\\treturn __compat_get_timespec(ts, uts);\\n}\\nEXPORT_SYMBOL_GPL(compat_get_timespec);\\n\\nint compat_put_timespec(const struct timespec *ts, void __user *uts)\\n{\\n\\tif (COMPAT_USE_64BIT_TIME)\\n\\t\\treturn copy_to_user(uts, ts, sizeof(*ts)) ? -EFAULT : 0;\\n\\telse\\n\\t\\treturn __compat_put_timespec(ts, uts);\\n}\\nEXPORT_SYMBOL_GPL(compat_put_timespec);\\n\\nint get_compat_itimerval(struct itimerval *o, const struct compat_itimerval __user *i)\\n{\\n\\tstruct compat_itimerval v32;\\n\\n\\tif (copy_from_user(&v32, i, sizeof(struct compat_itimerval)))\\n\\t\\treturn -EFAULT;\\n\\to->it_interval.tv_sec = v32.it_interval.tv_sec;\\n\\to->it_interval.tv_usec = v32.it_interval.tv_usec;\\n\\to->it_value.tv_sec = v32.it_value.tv_sec;\\n\\to->it_value.tv_usec = v32.it_value.tv_usec;\\n\\treturn 0;\\n}\\n\\nint put_compat_itimerval(struct compat_itimerval __user *o, const struct itimerval *i)\\n{\\n\\tstruct compat_itimerval v32;\\n\\n\\tv32.it_interval.tv_sec = i->it_interval.tv_sec;\\n\\tv32.it_interval.tv_usec = i->it_interval.tv_usec;\\n\\tv32.it_value.tv_sec = i->it_value.tv_sec;\\n\\tv32.it_value.tv_usec = i->it_value.tv_usec;\\n\\treturn copy_to_user(o, &v32, sizeof(struct compat_itimerval)) ? -EFAULT : 0;\\n}\\n\\n#ifdef __ARCH_WANT_SYS_SIGPROCMASK\\n\\n/*\\n * sys_sigprocmask SIG_SETMASK sets the first (compat) word of the\\n * blocked set of signals to the supplied signal set\\n */\\nstatic inline void compat_sig_setmask(sigset_t *blocked, compat_sigset_word set)\\n{\\n\\tmemcpy(blocked->sig, &set, sizeof(set));\\n}\\n\\nCOMPAT_SYSCALL_DEFINE3(sigprocmask, int, how,\\n\\t\\t       compat_old_sigset_t __user *, nset,\\n\\t\\t       compat_old_sigset_t __user *, oset)\\n{\\n\\told_sigset_t old_set, new_set;\\n\\tsigset_t new_blocked;\\n\\n\\told_set = current->blocked.sig[0];\\n\\n\\tif (nset) {\\n\\t\\tif (get_user(new_set, nset))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\tnew_set &= ~(sigmask(SIGKILL) | sigmask(SIGSTOP));\\n\\n\\t\\tnew_blocked = current->blocked;\\n\\n\\t\\tswitch (how) {\\n\\t\\tcase SIG_BLOCK:\\n\\t\\t\\tsigaddsetmask(&new_blocked, new_set);\\n\\t\\t\\tbreak;\\n\\t\\tcase SIG_UNBLOCK:\\n\\t\\t\\tsigdelsetmask(&new_blocked, new_set);\\n\\t\\t\\tbreak;\\n\\t\\tcase SIG_SETMASK:\\n\\t\\t\\tcompat_sig_setmask(&new_blocked, new_set);\\n\\t\\t\\tbreak;\\n\\t\\tdefault:\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t}\\n\\n\\t\\tset_current_blocked(&new_blocked);\\n\\t}\\n\\n\\tif (oset) {\\n\\t\\tif (put_user(old_set, oset))\\n\\t\\t\\treturn -EFAULT;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\n#endif\\n\\nint put_compat_rusage(const struct rusage *r, struct compat_rusage __user *ru)\\n{\\n\\tstruct compat_rusage r32;\\n\\tmemset(&r32, 0, sizeof(r32));\\n\\tr32.ru_utime.tv_sec = r->ru_utime.tv_sec;\\n\\tr32.ru_utime.tv_usec = r->ru_utime.tv_usec;\\n\\tr32.ru_stime.tv_sec = r->ru_stime.tv_sec;\\n\\tr32.ru_stime.tv_usec = r->ru_stime.tv_usec;\\n\\tr32.ru_maxrss = r->ru_maxrss;\\n\\tr32.ru_ixrss = r->ru_ixrss;\\n\\tr32.ru_idrss = r->ru_idrss;\\n\\tr32.ru_isrss = r->ru_isrss;\\n\\tr32.ru_minflt = r->ru_minflt;\\n\\tr32.ru_majflt = r->ru_majflt;\\n\\tr32.ru_nswap = r->ru_nswap;\\n\\tr32.ru_inblock = r->ru_inblock;\\n\\tr32.ru_oublock = r->ru_oublock;\\n\\tr32.ru_msgsnd = r->ru_msgsnd;\\n\\tr32.ru_msgrcv = r->ru_msgrcv;\\n\\tr32.ru_nsignals = r->ru_nsignals;\\n\\tr32.ru_nvcsw = r->ru_nvcsw;\\n\\tr32.ru_nivcsw = r->ru_nivcsw;\\n\\tif (copy_to_user(ru, &r32, sizeof(r32)))\\n\\t\\treturn -EFAULT;\\n\\treturn 0;\\n}\\n\\nstatic int compat_get_user_cpu_mask(compat_ulong_t __user *user_mask_ptr,\\n\\t\\t\\t\\t    unsigned len, struct cpumask *new_mask)\\n{\\n\\tunsigned long *k;\\n\\n\\tif (len < cpumask_size())\\n\\t\\tmemset(new_mask, 0, cpumask_size());\\n\\telse if (len > cpumask_size())\\n\\t\\tlen = cpumask_size();\\n\\n\\tk = cpumask_bits(new_mask);\\n\\treturn compat_get_bitmap(k, user_mask_ptr, len * 8);\\n}\\n\\nCOMPAT_SYSCALL_DEFINE3(sched_setaffinity, compat_pid_t, pid,\\n\\t\\t       unsigned int, len,\\n\\t\\t       compat_ulong_t __user *, user_mask_ptr)\\n{\\n\\tcpumask_var_t new_mask;\\n\\tint retval;\\n\\n\\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL))\\n\\t\\treturn -ENOMEM;\\n\\n\\tretval = compat_get_user_cpu_mask(user_mask_ptr, len, new_mask);\\n\\tif (retval)\\n\\t\\tgoto out;\\n\\n\\tretval = sched_setaffinity(pid, new_mask);\\nout:\\n\\tfree_cpumask_var(new_mask);\\n\\treturn retval;\\n}\\n\\nCOMPAT_SYSCALL_DEFINE3(sched_getaffinity, compat_pid_t,  pid, unsigned int, len,\\n\\t\\t       compat_ulong_t __user *, user_mask_ptr)\\n{\\n\\tint ret;\\n\\tcpumask_var_t mask;\\n\\n\\tif ((len * BITS_PER_BYTE) < nr_cpu_ids)\\n\\t\\treturn -EINVAL;\\n\\tif (len & (sizeof(compat_ulong_t)-1))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (!alloc_cpumask_var(&mask, GFP_KERNEL))\\n\\t\\treturn -ENOMEM;\\n\\n\\tret = sched_getaffinity(pid, mask);\\n\\tif (ret == 0) {\\n\\t\\tunsigned int retlen = min(len, cpumask_size());\\n\\n\\t\\tif (compat_put_bitmap(user_mask_ptr, cpumask_bits(mask), retlen * 8))\\n\\t\\t\\tret = -EFAULT;\\n\\t\\telse\\n\\t\\t\\tret = retlen;\\n\\t}\\n\\tfree_cpumask_var(mask);\\n\\n\\treturn ret;\\n}\\n\\n/*\\n * We currently only need the following fields from the sigevent\\n * structure: sigev_value, sigev_signo, sig_notify and (sometimes\\n * sigev_notify_thread_id).  The others are handled in user mode.\\n * We also assume that copying sigev_value.sival_int is sufficient\\n * to keep all the bits of sigev_value.sival_ptr intact.\\n */\\nint get_compat_sigevent(struct sigevent *event,\\n\\t\\tconst struct compat_sigevent __user *u_event)\\n{\\n\\tmemset(event, 0, sizeof(*event));\\n\\treturn (!access_ok(u_event, sizeof(*u_event)) ||\\n\\t\\t__get_user(event->sigev_value.sival_int,\\n\\t\\t\\t&u_event->sigev_value.sival_int) ||\\n\\t\\t__get_user(event->sigev_signo, &u_event->sigev_signo) ||\\n\\t\\t__get_user(event->sigev_notify, &u_event->sigev_notify) ||\\n\\t\\t__get_user(event->sigev_notify_thread_id,\\n\\t\\t\\t&u_event->sigev_notify_thread_id))\\n\\t\\t? -EFAULT : 0;\\n}\\n\\nlong compat_get_bitmap(unsigned long *mask, const compat_ulong_t __user *umask,\\n\\t\\t       unsigned long bitmap_size)\\n{\\n\\tunsigned long nr_compat_longs;\\n\\n\\t/* align bitmap up to nearest compat_long_t boundary */\\n\\tbitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);\\n\\tnr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);\\n\\n\\tif (!user_access_begin(umask, bitmap_size / 8))\\n\\t\\treturn -EFAULT;\\n\\n\\twhile (nr_compat_longs > 1) {\\n\\t\\tcompat_ulong_t l1, l2;\\n\\t\\tunsafe_get_user(l1, umask++, Efault);\\n\\t\\tunsafe_get_user(l2, umask++, Efault);\\n\\t\\t*mask++ = ((unsigned long)l2 << BITS_PER_COMPAT_LONG) | l1;\\n\\t\\tnr_compat_longs -= 2;\\n\\t}\\n\\tif (nr_compat_longs)\\n\\t\\tunsafe_get_user(*mask, umask++, Efault);\\n\\tuser_access_end();\\n\\treturn 0;\\n\\nEfault:\\n\\tuser_access_end();\\n\\treturn -EFAULT;\\n}\\n\\nlong compat_put_bitmap(compat_ulong_t __user *umask, unsigned long *mask,\\n\\t\\t       unsigned long bitmap_size)\\n{\\n\\tunsigned long nr_compat_longs;\\n\\n\\t/* align bitmap up to nearest compat_long_t boundary */\\n\\tbitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);\\n\\tnr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);\\n\\n\\tif (!user_access_begin(umask, bitmap_size / 8))\\n\\t\\treturn -EFAULT;\\n\\n\\twhile (nr_compat_longs > 1) {\\n\\t\\tunsigned long m = *mask++;\\n\\t\\tunsafe_put_user((compat_ulong_t)m, umask++, Efault);\\n\\t\\tunsafe_put_user(m >> BITS_PER_COMPAT_LONG, umask++, Efault);\\n\\t\\tnr_compat_longs -= 2;\\n\\t}\\n\\tif (nr_compat_longs)\\n\\t\\tunsafe_put_user((compat_ulong_t)*mask, umask++, Efault);\\n\\tuser_access_end();\\n\\treturn 0;\\nEfault:\\n\\tuser_access_end();\\n\\treturn -EFAULT;\\n}\\n\\nint\\nget_compat_sigset(sigset_t *set, const compat_sigset_t __user *compat)\\n{\\n#ifdef __BIG_ENDIAN\\n\\tcompat_sigset_t v;\\n\\tif (copy_from_user(&v, compat, sizeof(compat_sigset_t)))\\n\\t\\treturn -EFAULT;\\n\\tswitch (_NSIG_WORDS) {\\n\\tcase 4: set->sig[3] = v.sig[6] | (((long)v.sig[7]) << 32 );\\n\\tcase 3: set->sig[2] = v.sig[4] | (((long)v.sig[5]) << 32 );\\n\\tcase 2: set->sig[1] = v.sig[2] | (((long)v.sig[3]) << 32 );\\n\\tcase 1: set->sig[0] = v.sig[0] | (((long)v.sig[1]) << 32 );\\n\\t}\\n#else\\n\\tif (copy_from_user(set, compat, sizeof(compat_sigset_t)))\\n\\t\\treturn -EFAULT;\\n#endif\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(get_compat_sigset);\\n\\n/*\\n * Allocate user-space memory for the duration of a single system call,\\n * in order to marshall parameters inside a compat thunk.\\n */\\nvoid __user *compat_alloc_user_space(unsigned long len)\\n{\\n\\tvoid __user *ptr;\\n\\n\\t/* If len would occupy more than half of the entire compat space... */\\n\\tif (unlikely(len > (((compat_uptr_t)~0) >> 1)))\\n\\t\\treturn NULL;\\n\\n\\tptr = arch_compat_alloc_user_space(len);\\n\\n\\tif (unlikely(!access_ok(ptr, len)))\\n\\t\\treturn NULL;\\n\\n\\treturn ptr;\\n}\\nEXPORT_SYMBOL_GPL(compat_alloc_user_space);\\n\\n/* audit_watch.c -- watching inodes\\n *\\n * Copyright 2003-2009 Red Hat, Inc.\\n * Copyright 2005 Hewlett-Packard Development Company, L.P.\\n * Copyright 2005 IBM Corporation\\n *\\n * This program is free software; you can redistribute it and/or modify\\n * it under the terms of the GNU General Public License as published by\\n * the Free Software Foundation; either version 2 of the License, or\\n * (at your option) any later version.\\n *\\n * This program is distributed in the hope that it will be useful,\\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n * GNU General Public License for more details.\\n *\\n * You should have received a copy of the GNU General Public License\\n * along with this program; if not, write to the Free Software\\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA\\n */\\n\\n#include <linux/file.h>\\n#include <linux/kernel.h>\\n#include <linux/audit.h>\\n#include <linux/kthread.h>\\n#include <linux/mutex.h>\\n#include <linux/fs.h>\\n#include <linux/fsnotify_backend.h>\\n#include <linux/namei.h>\\n#include <linux/netlink.h>\\n#include <linux/refcount.h>\\n#include <linux/sched.h>\\n#include <linux/slab.h>\\n#include <linux/security.h>\\n#include \"audit.h\"\\n\\n/*\\n * Reference counting:\\n *\\n * audit_parent: lifetime is from audit_init_parent() to receipt of an FS_IGNORED\\n * \\tevent.  Each audit_watch holds a reference to its associated parent.\\n *\\n * audit_watch: if added to lists, lifetime is from audit_init_watch() to\\n * \\taudit_remove_watch().  Additionally, an audit_watch may exist\\n * \\ttemporarily to assist in searching existing filter data.  Each\\n * \\taudit_krule holds a reference to its associated watch.\\n */\\n\\nstruct audit_watch {\\n\\trefcount_t\\t\\tcount;\\t/* reference count */\\n\\tdev_t\\t\\t\\tdev;\\t/* associated superblock device */\\n\\tchar\\t\\t\\t*path;\\t/* insertion path */\\n\\tunsigned long\\t\\tino;\\t/* associated inode number */\\n\\tstruct audit_parent\\t*parent; /* associated parent */\\n\\tstruct list_head\\twlist;\\t/* entry in parent->watches list */\\n\\tstruct list_head\\trules;\\t/* anchor for krule->rlist */\\n};\\n\\nstruct audit_parent {\\n\\tstruct list_head\\twatches; /* anchor for audit_watch->wlist */\\n\\tstruct fsnotify_mark mark; /* fsnotify mark on the inode */\\n};\\n\\n/* fsnotify handle. */\\nstatic struct fsnotify_group *audit_watch_group;\\n\\n/* fsnotify events we care about. */\\n#define AUDIT_FS_WATCH (FS_MOVE | FS_CREATE | FS_DELETE | FS_DELETE_SELF |\\\\\\n\\t\\t\\tFS_MOVE_SELF | FS_EVENT_ON_CHILD | FS_UNMOUNT)\\n\\nstatic void audit_free_parent(struct audit_parent *parent)\\n{\\n\\tWARN_ON(!list_empty(&parent->watches));\\n\\tkfree(parent);\\n}\\n\\nstatic void audit_watch_free_mark(struct fsnotify_mark *entry)\\n{\\n\\tstruct audit_parent *parent;\\n\\n\\tparent = container_of(entry, struct audit_parent, mark);\\n\\taudit_free_parent(parent);\\n}\\n\\nstatic void audit_get_parent(struct audit_parent *parent)\\n{\\n\\tif (likely(parent))\\n\\t\\tfsnotify_get_mark(&parent->mark);\\n}\\n\\nstatic void audit_put_parent(struct audit_parent *parent)\\n{\\n\\tif (likely(parent))\\n\\t\\tfsnotify_put_mark(&parent->mark);\\n}\\n\\n/*\\n * Find and return the audit_parent on the given inode.  If found a reference\\n * is taken on this parent.\\n */\\nstatic inline struct audit_parent *audit_find_parent(struct inode *inode)\\n{\\n\\tstruct audit_parent *parent = NULL;\\n\\tstruct fsnotify_mark *entry;\\n\\n\\tentry = fsnotify_find_mark(&inode->i_fsnotify_marks, audit_watch_group);\\n\\tif (entry)\\n\\t\\tparent = container_of(entry, struct audit_parent, mark);\\n\\n\\treturn parent;\\n}\\n\\nvoid audit_get_watch(struct audit_watch *watch)\\n{\\n\\trefcount_inc(&watch->count);\\n}\\n\\nvoid audit_put_watch(struct audit_watch *watch)\\n{\\n\\tif (refcount_dec_and_test(&watch->count)) {\\n\\t\\tWARN_ON(watch->parent);\\n\\t\\tWARN_ON(!list_empty(&watch->rules));\\n\\t\\tkfree(watch->path);\\n\\t\\tkfree(watch);\\n\\t}\\n}\\n\\nstatic void audit_remove_watch(struct audit_watch *watch)\\n{\\n\\tlist_del(&watch->wlist);\\n\\taudit_put_parent(watch->parent);\\n\\twatch->parent = NULL;\\n\\taudit_put_watch(watch); /* match initial get */\\n}\\n\\nchar *audit_watch_path(struct audit_watch *watch)\\n{\\n\\treturn watch->path;\\n}\\n\\nint audit_watch_compare(struct audit_watch *watch, unsigned long ino, dev_t dev)\\n{\\n\\treturn (watch->ino != AUDIT_INO_UNSET) &&\\n\\t\\t(watch->ino == ino) &&\\n\\t\\t(watch->dev == dev);\\n}\\n\\n/* Initialize a parent watch entry. */\\nstatic struct audit_parent *audit_init_parent(struct path *path)\\n{\\n\\tstruct inode *inode = d_backing_inode(path->dentry);\\n\\tstruct audit_parent *parent;\\n\\tint ret;\\n\\n\\tparent = kzalloc(sizeof(*parent), GFP_KERNEL);\\n\\tif (unlikely(!parent))\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\tINIT_LIST_HEAD(&parent->watches);\\n\\n\\tfsnotify_init_mark(&parent->mark, audit_watch_group);\\n\\tparent->mark.mask = AUDIT_FS_WATCH;\\n\\tret = fsnotify_add_inode_mark(&parent->mark, inode, 0);\\n\\tif (ret < 0) {\\n\\t\\taudit_free_parent(parent);\\n\\t\\treturn ERR_PTR(ret);\\n\\t}\\n\\n\\treturn parent;\\n}\\n\\n/* Initialize a watch entry. */\\nstatic struct audit_watch *audit_init_watch(char *path)\\n{\\n\\tstruct audit_watch *watch;\\n\\n\\twatch = kzalloc(sizeof(*watch), GFP_KERNEL);\\n\\tif (unlikely(!watch))\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\tINIT_LIST_HEAD(&watch->rules);\\n\\trefcount_set(&watch->count, 1);\\n\\twatch->path = path;\\n\\twatch->dev = AUDIT_DEV_UNSET;\\n\\twatch->ino = AUDIT_INO_UNSET;\\n\\n\\treturn watch;\\n}\\n\\n/* Translate a watch string to kernel representation. */\\nint audit_to_watch(struct audit_krule *krule, char *path, int len, u32 op)\\n{\\n\\tstruct audit_watch *watch;\\n\\n\\tif (!audit_watch_group)\\n\\t\\treturn -EOPNOTSUPP;\\n\\n\\tif (path[0] != \\'/\\' || path[len-1] == \\'/\\' ||\\n\\t    krule->listnr != AUDIT_FILTER_EXIT ||\\n\\t    op != Audit_equal ||\\n\\t    krule->inode_f || krule->watch || krule->tree)\\n\\t\\treturn -EINVAL;\\n\\n\\twatch = audit_init_watch(path);\\n\\tif (IS_ERR(watch))\\n\\t\\treturn PTR_ERR(watch);\\n\\n\\tkrule->watch = watch;\\n\\n\\treturn 0;\\n}\\n\\n/* Duplicate the given audit watch.  The new watch\\'s rules list is initialized\\n * to an empty list and wlist is undefined. */\\nstatic struct audit_watch *audit_dupe_watch(struct audit_watch *old)\\n{\\n\\tchar *path;\\n\\tstruct audit_watch *new;\\n\\n\\tpath = kstrdup(old->path, GFP_KERNEL);\\n\\tif (unlikely(!path))\\n\\t\\treturn ERR_PTR(-ENOMEM);\\n\\n\\tnew = audit_init_watch(path);\\n\\tif (IS_ERR(new)) {\\n\\t\\tkfree(path);\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tnew->dev = old->dev;\\n\\tnew->ino = old->ino;\\n\\taudit_get_parent(old->parent);\\n\\tnew->parent = old->parent;\\n\\nout:\\n\\treturn new;\\n}\\n\\nstatic void audit_watch_log_rule_change(struct audit_krule *r, struct audit_watch *w, char *op)\\n{\\n\\tstruct audit_buffer *ab;\\n\\n\\tif (!audit_enabled)\\n\\t\\treturn;\\n\\tab = audit_log_start(NULL, GFP_NOFS, AUDIT_CONFIG_CHANGE);\\n\\tif (!ab)\\n\\t\\treturn;\\n\\taudit_log_session_info(ab);\\n\\taudit_log_format(ab, \"op=%s path=\", op);\\n\\taudit_log_untrustedstring(ab, w->path);\\n\\taudit_log_key(ab, r->filterkey);\\n\\taudit_log_format(ab, \" list=%d res=1\", r->listnr);\\n\\taudit_log_end(ab);\\n}\\n\\n/* Update inode info in audit rules based on filesystem event. */\\nstatic void audit_update_watch(struct audit_parent *parent,\\n\\t\\t\\t       const char *dname, dev_t dev,\\n\\t\\t\\t       unsigned long ino, unsigned invalidating)\\n{\\n\\tstruct audit_watch *owatch, *nwatch, *nextw;\\n\\tstruct audit_krule *r, *nextr;\\n\\tstruct audit_entry *oentry, *nentry;\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\t/* Run all of the watches on this parent looking for the one that\\n\\t * matches the given dname */\\n\\tlist_for_each_entry_safe(owatch, nextw, &parent->watches, wlist) {\\n\\t\\tif (audit_compare_dname_path(dname, owatch->path,\\n\\t\\t\\t\\t\\t     AUDIT_NAME_FULL))\\n\\t\\t\\tcontinue;\\n\\n\\t\\t/* If the update involves invalidating rules, do the inode-based\\n\\t\\t * filtering now, so we don\\'t omit records. */\\n\\t\\tif (invalidating && !audit_dummy_context())\\n\\t\\t\\taudit_filter_inodes(current, audit_context());\\n\\n\\t\\t/* updating ino will likely change which audit_hash_list we\\n\\t\\t * are on so we need a new watch for the new list */\\n\\t\\tnwatch = audit_dupe_watch(owatch);\\n\\t\\tif (IS_ERR(nwatch)) {\\n\\t\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\t\\t\\taudit_panic(\"error updating watch, skipping\");\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\t\\tnwatch->dev = dev;\\n\\t\\tnwatch->ino = ino;\\n\\n\\t\\tlist_for_each_entry_safe(r, nextr, &owatch->rules, rlist) {\\n\\n\\t\\t\\toentry = container_of(r, struct audit_entry, rule);\\n\\t\\t\\tlist_del(&oentry->rule.rlist);\\n\\t\\t\\tlist_del_rcu(&oentry->list);\\n\\n\\t\\t\\tnentry = audit_dupe_rule(&oentry->rule);\\n\\t\\t\\tif (IS_ERR(nentry)) {\\n\\t\\t\\t\\tlist_del(&oentry->rule.list);\\n\\t\\t\\t\\taudit_panic(\"error updating watch, removing\");\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tint h = audit_hash_ino((u32)ino);\\n\\n\\t\\t\\t\\t/*\\n\\t\\t\\t\\t * nentry->rule.watch == oentry->rule.watch so\\n\\t\\t\\t\\t * we must drop that reference and set it to our\\n\\t\\t\\t\\t * new watch.\\n\\t\\t\\t\\t */\\n\\t\\t\\t\\taudit_put_watch(nentry->rule.watch);\\n\\t\\t\\t\\taudit_get_watch(nwatch);\\n\\t\\t\\t\\tnentry->rule.watch = nwatch;\\n\\t\\t\\t\\tlist_add(&nentry->rule.rlist, &nwatch->rules);\\n\\t\\t\\t\\tlist_add_rcu(&nentry->list, &audit_inode_hash[h]);\\n\\t\\t\\t\\tlist_replace(&oentry->rule.list,\\n\\t\\t\\t\\t\\t     &nentry->rule.list);\\n\\t\\t\\t}\\n\\t\\t\\tif (oentry->rule.exe)\\n\\t\\t\\t\\taudit_remove_mark(oentry->rule.exe);\\n\\n\\t\\t\\taudit_watch_log_rule_change(r, owatch, \"updated_rules\");\\n\\n\\t\\t\\tcall_rcu(&oentry->rcu, audit_free_rule_rcu);\\n\\t\\t}\\n\\n\\t\\taudit_remove_watch(owatch);\\n\\t\\tgoto add_watch_to_parent; /* event applies to a single watch */\\n\\t}\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\treturn;\\n\\nadd_watch_to_parent:\\n\\tlist_add(&nwatch->wlist, &parent->watches);\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\treturn;\\n}\\n\\n/* Remove all watches & rules associated with a parent that is going away. */\\nstatic void audit_remove_parent_watches(struct audit_parent *parent)\\n{\\n\\tstruct audit_watch *w, *nextw;\\n\\tstruct audit_krule *r, *nextr;\\n\\tstruct audit_entry *e;\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\tlist_for_each_entry_safe(w, nextw, &parent->watches, wlist) {\\n\\t\\tlist_for_each_entry_safe(r, nextr, &w->rules, rlist) {\\n\\t\\t\\te = container_of(r, struct audit_entry, rule);\\n\\t\\t\\taudit_watch_log_rule_change(r, w, \"remove_rule\");\\n\\t\\t\\tif (e->rule.exe)\\n\\t\\t\\t\\taudit_remove_mark(e->rule.exe);\\n\\t\\t\\tlist_del(&r->rlist);\\n\\t\\t\\tlist_del(&r->list);\\n\\t\\t\\tlist_del_rcu(&e->list);\\n\\t\\t\\tcall_rcu(&e->rcu, audit_free_rule_rcu);\\n\\t\\t}\\n\\t\\taudit_remove_watch(w);\\n\\t}\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\tfsnotify_destroy_mark(&parent->mark, audit_watch_group);\\n}\\n\\n/* Get path information necessary for adding watches. */\\nstatic int audit_get_nd(struct audit_watch *watch, struct path *parent)\\n{\\n\\tstruct dentry *d = kern_path_locked(watch->path, parent);\\n\\tif (IS_ERR(d))\\n\\t\\treturn PTR_ERR(d);\\n\\tinode_unlock(d_backing_inode(parent->dentry));\\n\\tif (d_is_positive(d)) {\\n\\t\\t/* update watch filter fields */\\n\\t\\twatch->dev = d->d_sb->s_dev;\\n\\t\\twatch->ino = d_backing_inode(d)->i_ino;\\n\\t}\\n\\tdput(d);\\n\\treturn 0;\\n}\\n\\n/* Associate the given rule with an existing parent.\\n * Caller must hold audit_filter_mutex. */\\nstatic void audit_add_to_parent(struct audit_krule *krule,\\n\\t\\t\\t\\tstruct audit_parent *parent)\\n{\\n\\tstruct audit_watch *w, *watch = krule->watch;\\n\\tint watch_found = 0;\\n\\n\\tBUG_ON(!mutex_is_locked(&audit_filter_mutex));\\n\\n\\tlist_for_each_entry(w, &parent->watches, wlist) {\\n\\t\\tif (strcmp(watch->path, w->path))\\n\\t\\t\\tcontinue;\\n\\n\\t\\twatch_found = 1;\\n\\n\\t\\t/* put krule\\'s ref to temporary watch */\\n\\t\\taudit_put_watch(watch);\\n\\n\\t\\taudit_get_watch(w);\\n\\t\\tkrule->watch = watch = w;\\n\\n\\t\\taudit_put_parent(parent);\\n\\t\\tbreak;\\n\\t}\\n\\n\\tif (!watch_found) {\\n\\t\\twatch->parent = parent;\\n\\n\\t\\taudit_get_watch(watch);\\n\\t\\tlist_add(&watch->wlist, &parent->watches);\\n\\t}\\n\\tlist_add(&krule->rlist, &watch->rules);\\n}\\n\\n/* Find a matching watch entry, or add this one.\\n * Caller must hold audit_filter_mutex. */\\nint audit_add_watch(struct audit_krule *krule, struct list_head **list)\\n{\\n\\tstruct audit_watch *watch = krule->watch;\\n\\tstruct audit_parent *parent;\\n\\tstruct path parent_path;\\n\\tint h, ret = 0;\\n\\n\\t/*\\n\\t * When we will be calling audit_add_to_parent, krule->watch might have\\n\\t * been updated and watch might have been freed.\\n\\t * So we need to keep a reference of watch.\\n\\t */\\n\\taudit_get_watch(watch);\\n\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\t/* Avoid calling path_lookup under audit_filter_mutex. */\\n\\tret = audit_get_nd(watch, &parent_path);\\n\\n\\t/* caller expects mutex locked */\\n\\tmutex_lock(&audit_filter_mutex);\\n\\n\\tif (ret) {\\n\\t\\taudit_put_watch(watch);\\n\\t\\treturn ret;\\n\\t}\\n\\n\\t/* either find an old parent or attach a new one */\\n\\tparent = audit_find_parent(d_backing_inode(parent_path.dentry));\\n\\tif (!parent) {\\n\\t\\tparent = audit_init_parent(&parent_path);\\n\\t\\tif (IS_ERR(parent)) {\\n\\t\\t\\tret = PTR_ERR(parent);\\n\\t\\t\\tgoto error;\\n\\t\\t}\\n\\t}\\n\\n\\taudit_add_to_parent(krule, parent);\\n\\n\\th = audit_hash_ino((u32)watch->ino);\\n\\t*list = &audit_inode_hash[h];\\nerror:\\n\\tpath_put(&parent_path);\\n\\taudit_put_watch(watch);\\n\\treturn ret;\\n}\\n\\nvoid audit_remove_watch_rule(struct audit_krule *krule)\\n{\\n\\tstruct audit_watch *watch = krule->watch;\\n\\tstruct audit_parent *parent = watch->parent;\\n\\n\\tlist_del(&krule->rlist);\\n\\n\\tif (list_empty(&watch->rules)) {\\n\\t\\t/*\\n\\t\\t * audit_remove_watch() drops our reference to \\'parent\\' which\\n\\t\\t * can get freed. Grab our own reference to be safe.\\n\\t\\t */\\n\\t\\taudit_get_parent(parent);\\n\\t\\taudit_remove_watch(watch);\\n\\t\\tif (list_empty(&parent->watches))\\n\\t\\t\\tfsnotify_destroy_mark(&parent->mark, audit_watch_group);\\n\\t\\taudit_put_parent(parent);\\n\\t}\\n}\\n\\n/* Update watch data in audit rules based on fsnotify events. */\\nstatic int audit_watch_handle_event(struct fsnotify_group *group,\\n\\t\\t\\t\\t    struct inode *to_tell,\\n\\t\\t\\t\\t    u32 mask, const void *data, int data_type,\\n\\t\\t\\t\\t    const unsigned char *dname, u32 cookie,\\n\\t\\t\\t\\t    struct fsnotify_iter_info *iter_info)\\n{\\n\\tstruct fsnotify_mark *inode_mark = fsnotify_iter_inode_mark(iter_info);\\n\\tconst struct inode *inode;\\n\\tstruct audit_parent *parent;\\n\\n\\tparent = container_of(inode_mark, struct audit_parent, mark);\\n\\n\\tBUG_ON(group != audit_watch_group);\\n\\n\\tswitch (data_type) {\\n\\tcase (FSNOTIFY_EVENT_PATH):\\n\\t\\tinode = d_backing_inode(((const struct path *)data)->dentry);\\n\\t\\tbreak;\\n\\tcase (FSNOTIFY_EVENT_INODE):\\n\\t\\tinode = (const struct inode *)data;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\tBUG();\\n\\t\\tinode = NULL;\\n\\t\\tbreak;\\n\\t}\\n\\n\\tif (mask & (FS_CREATE|FS_MOVED_TO) && inode)\\n\\t\\taudit_update_watch(parent, dname, inode->i_sb->s_dev, inode->i_ino, 0);\\n\\telse if (mask & (FS_DELETE|FS_MOVED_FROM))\\n\\t\\taudit_update_watch(parent, dname, AUDIT_DEV_UNSET, AUDIT_INO_UNSET, 1);\\n\\telse if (mask & (FS_DELETE_SELF|FS_UNMOUNT|FS_MOVE_SELF))\\n\\t\\taudit_remove_parent_watches(parent);\\n\\n\\treturn 0;\\n}\\n\\nstatic const struct fsnotify_ops audit_watch_fsnotify_ops = {\\n\\t.handle_event = \\taudit_watch_handle_event,\\n\\t.free_mark =\\t\\taudit_watch_free_mark,\\n};\\n\\nstatic int __init audit_watch_init(void)\\n{\\n\\taudit_watch_group = fsnotify_alloc_group(&audit_watch_fsnotify_ops);\\n\\tif (IS_ERR(audit_watch_group)) {\\n\\t\\taudit_watch_group = NULL;\\n\\t\\taudit_panic(\"cannot create audit fsnotify group\");\\n\\t}\\n\\treturn 0;\\n}\\ndevice_initcall(audit_watch_init);\\n\\nint audit_dupe_exe(struct audit_krule *new, struct audit_krule *old)\\n{\\n\\tstruct audit_fsnotify_mark *audit_mark;\\n\\tchar *pathname;\\n\\n\\tpathname = kstrdup(audit_mark_path(old->exe), GFP_KERNEL);\\n\\tif (!pathname)\\n\\t\\treturn -ENOMEM;\\n\\n\\taudit_mark = audit_alloc_mark(new, pathname, strlen(pathname));\\n\\tif (IS_ERR(audit_mark)) {\\n\\t\\tkfree(pathname);\\n\\t\\treturn PTR_ERR(audit_mark);\\n\\t}\\n\\tnew->exe = audit_mark;\\n\\n\\treturn 0;\\n}\\n\\nint audit_exe_compare(struct task_struct *tsk, struct audit_fsnotify_mark *mark)\\n{\\n\\tstruct file *exe_file;\\n\\tunsigned long ino;\\n\\tdev_t dev;\\n\\n\\texe_file = get_task_exe_file(tsk);\\n\\tif (!exe_file)\\n\\t\\treturn 0;\\n\\tino = file_inode(exe_file)->i_ino;\\n\\tdev = file_inode(exe_file)->i_sb->s_dev;\\n\\tfput(exe_file);\\n\\treturn audit_mark_compare(mark, ino, dev);\\n}\\n\\n/*\\n *\\tlinux/kernel/resource.c\\n *\\n * Copyright (C) 1999\\tLinus Torvalds\\n * Copyright (C) 1999\\tMartin Mares <mj@ucw.cz>\\n *\\n * Arbitrary resource management.\\n */\\n\\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\\n\\n#include <linux/export.h>\\n#include <linux/errno.h>\\n#include <linux/ioport.h>\\n#include <linux/init.h>\\n#include <linux/slab.h>\\n#include <linux/spinlock.h>\\n#include <linux/fs.h>\\n#include <linux/proc_fs.h>\\n#include <linux/sched.h>\\n#include <linux/seq_file.h>\\n#include <linux/device.h>\\n#include <linux/pfn.h>\\n#include <linux/mm.h>\\n#include <linux/resource_ext.h>\\n#include <asm/io.h>\\n\\n\\nstruct resource ioport_resource = {\\n\\t.name\\t= \"PCI IO\",\\n\\t.start\\t= 0,\\n\\t.end\\t= IO_SPACE_LIMIT,\\n\\t.flags\\t= IORESOURCE_IO,\\n};\\nEXPORT_SYMBOL(ioport_resource);\\n\\nstruct resource iomem_resource = {\\n\\t.name\\t= \"PCI mem\",\\n\\t.start\\t= 0,\\n\\t.end\\t= -1,\\n\\t.flags\\t= IORESOURCE_MEM,\\n};\\nEXPORT_SYMBOL(iomem_resource);\\n\\n/* constraints to be met while allocating resources */\\nstruct resource_constraint {\\n\\tresource_size_t min, max, align;\\n\\tresource_size_t (*alignf)(void *, const struct resource *,\\n\\t\\t\\tresource_size_t, resource_size_t);\\n\\tvoid *alignf_data;\\n};\\n\\nstatic DEFINE_RWLOCK(resource_lock);\\n\\n/*\\n * For memory hotplug, there is no way to free resource entries allocated\\n * by boot mem after the system is up. So for reusing the resource entry\\n * we need to remember the resource.\\n */\\nstatic struct resource *bootmem_resource_free;\\nstatic DEFINE_SPINLOCK(bootmem_resource_lock);\\n\\nstatic struct resource *next_resource(struct resource *p, bool sibling_only)\\n{\\n\\t/* Caller wants to traverse through siblings only */\\n\\tif (sibling_only)\\n\\t\\treturn p->sibling;\\n\\n\\tif (p->child)\\n\\t\\treturn p->child;\\n\\twhile (!p->sibling && p->parent)\\n\\t\\tp = p->parent;\\n\\treturn p->sibling;\\n}\\n\\nstatic void *r_next(struct seq_file *m, void *v, loff_t *pos)\\n{\\n\\tstruct resource *p = v;\\n\\t(*pos)++;\\n\\treturn (void *)next_resource(p, false);\\n}\\n\\n#ifdef CONFIG_PROC_FS\\n\\nenum { MAX_IORES_LEVEL = 5 };\\n\\nstatic void *r_start(struct seq_file *m, loff_t *pos)\\n\\t__acquires(resource_lock)\\n{\\n\\tstruct resource *p = PDE_DATA(file_inode(m->file));\\n\\tloff_t l = 0;\\n\\tread_lock(&resource_lock);\\n\\tfor (p = p->child; p && l < *pos; p = r_next(m, p, &l))\\n\\t\\t;\\n\\treturn p;\\n}\\n\\nstatic void r_stop(struct seq_file *m, void *v)\\n\\t__releases(resource_lock)\\n{\\n\\tread_unlock(&resource_lock);\\n}\\n\\nstatic int r_show(struct seq_file *m, void *v)\\n{\\n\\tstruct resource *root = PDE_DATA(file_inode(m->file));\\n\\tstruct resource *r = v, *p;\\n\\tunsigned long long start, end;\\n\\tint width = root->end < 0x10000 ? 4 : 8;\\n\\tint depth;\\n\\n\\tfor (depth = 0, p = r; depth < MAX_IORES_LEVEL; depth++, p = p->parent)\\n\\t\\tif (p->parent == root)\\n\\t\\t\\tbreak;\\n\\n\\tif (file_ns_capable(m->file, &init_user_ns, CAP_SYS_ADMIN)) {\\n\\t\\tstart = r->start;\\n\\t\\tend = r->end;\\n\\t} else {\\n\\t\\tstart = end = 0;\\n\\t}\\n\\n\\tseq_printf(m, \"%*s%0*llx-%0*llx : %s\\\\n\",\\n\\t\\t\\tdepth * 2, \"\",\\n\\t\\t\\twidth, start,\\n\\t\\t\\twidth, end,\\n\\t\\t\\tr->name ? r->name : \"<BAD>\");\\n\\treturn 0;\\n}\\n\\nstatic const struct seq_operations resource_op = {\\n\\t.start\\t= r_start,\\n\\t.next\\t= r_next,\\n\\t.stop\\t= r_stop,\\n\\t.show\\t= r_show,\\n};\\n\\nstatic int __init ioresources_init(void)\\n{\\n\\tproc_create_seq_data(\"ioports\", 0, NULL, &resource_op,\\n\\t\\t\\t&ioport_resource);\\n\\tproc_create_seq_data(\"iomem\", 0, NULL, &resource_op, &iomem_resource);\\n\\treturn 0;\\n}\\n__initcall(ioresources_init);\\n\\n#endif /* CONFIG_PROC_FS */\\n\\nstatic void free_resource(struct resource *res)\\n{\\n\\tif (!res)\\n\\t\\treturn;\\n\\n\\tif (!PageSlab(virt_to_head_page(res))) {\\n\\t\\tspin_lock(&bootmem_resource_lock);\\n\\t\\tres->sibling = bootmem_resource_free;\\n\\t\\tbootmem_resource_free = res;\\n\\t\\tspin_unlock(&bootmem_resource_lock);\\n\\t} else {\\n\\t\\tkfree(res);\\n\\t}\\n}\\n\\nstatic struct resource *alloc_resource(gfp_t flags)\\n{\\n\\tstruct resource *res = NULL;\\n\\n\\tspin_lock(&bootmem_resource_lock);\\n\\tif (bootmem_resource_free) {\\n\\t\\tres = bootmem_resource_free;\\n\\t\\tbootmem_resource_free = res->sibling;\\n\\t}\\n\\tspin_unlock(&bootmem_resource_lock);\\n\\n\\tif (res)\\n\\t\\tmemset(res, 0, sizeof(struct resource));\\n\\telse\\n\\t\\tres = kzalloc(sizeof(struct resource), flags);\\n\\n\\treturn res;\\n}\\n\\n/* Return the conflict entry if you can\\'t request it */\\nstatic struct resource * __request_resource(struct resource *root, struct resource *new)\\n{\\n\\tresource_size_t start = new->start;\\n\\tresource_size_t end = new->end;\\n\\tstruct resource *tmp, **p;\\n\\n\\tif (end < start)\\n\\t\\treturn root;\\n\\tif (start < root->start)\\n\\t\\treturn root;\\n\\tif (end > root->end)\\n\\t\\treturn root;\\n\\tp = &root->child;\\n\\tfor (;;) {\\n\\t\\ttmp = *p;\\n\\t\\tif (!tmp || tmp->start > end) {\\n\\t\\t\\tnew->sibling = tmp;\\n\\t\\t\\t*p = new;\\n\\t\\t\\tnew->parent = root;\\n\\t\\t\\treturn NULL;\\n\\t\\t}\\n\\t\\tp = &tmp->sibling;\\n\\t\\tif (tmp->end < start)\\n\\t\\t\\tcontinue;\\n\\t\\treturn tmp;\\n\\t}\\n}\\n\\nstatic int __release_resource(struct resource *old, bool release_child)\\n{\\n\\tstruct resource *tmp, **p, *chd;\\n\\n\\tp = &old->parent->child;\\n\\tfor (;;) {\\n\\t\\ttmp = *p;\\n\\t\\tif (!tmp)\\n\\t\\t\\tbreak;\\n\\t\\tif (tmp == old) {\\n\\t\\t\\tif (release_child || !(tmp->child)) {\\n\\t\\t\\t\\t*p = tmp->sibling;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tfor (chd = tmp->child;; chd = chd->sibling) {\\n\\t\\t\\t\\t\\tchd->parent = tmp->parent;\\n\\t\\t\\t\\t\\tif (!(chd->sibling))\\n\\t\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t*p = tmp->child;\\n\\t\\t\\t\\tchd->sibling = tmp->sibling;\\n\\t\\t\\t}\\n\\t\\t\\told->parent = NULL;\\n\\t\\t\\treturn 0;\\n\\t\\t}\\n\\t\\tp = &tmp->sibling;\\n\\t}\\n\\treturn -EINVAL;\\n}\\n\\nstatic void __release_child_resources(struct resource *r)\\n{\\n\\tstruct resource *tmp, *p;\\n\\tresource_size_t size;\\n\\n\\tp = r->child;\\n\\tr->child = NULL;\\n\\twhile (p) {\\n\\t\\ttmp = p;\\n\\t\\tp = p->sibling;\\n\\n\\t\\ttmp->parent = NULL;\\n\\t\\ttmp->sibling = NULL;\\n\\t\\t__release_child_resources(tmp);\\n\\n\\t\\tprintk(KERN_DEBUG \"release child resource %pR\\\\n\", tmp);\\n\\t\\t/* need to restore size, and keep flags */\\n\\t\\tsize = resource_size(tmp);\\n\\t\\ttmp->start = 0;\\n\\t\\ttmp->end = size - 1;\\n\\t}\\n}\\n\\nvoid release_child_resources(struct resource *r)\\n{\\n\\twrite_lock(&resource_lock);\\n\\t__release_child_resources(r);\\n\\twrite_unlock(&resource_lock);\\n}\\n\\n/**\\n * request_resource_conflict - request and reserve an I/O or memory resource\\n * @root: root resource descriptor\\n * @new: resource descriptor desired by caller\\n *\\n * Returns 0 for success, conflict resource on error.\\n */\\nstruct resource *request_resource_conflict(struct resource *root, struct resource *new)\\n{\\n\\tstruct resource *conflict;\\n\\n\\twrite_lock(&resource_lock);\\n\\tconflict = __request_resource(root, new);\\n\\twrite_unlock(&resource_lock);\\n\\treturn conflict;\\n}\\n\\n/**\\n * request_resource - request and reserve an I/O or memory resource\\n * @root: root resource descriptor\\n * @new: resource descriptor desired by caller\\n *\\n * Returns 0 for success, negative error code on error.\\n */\\nint request_resource(struct resource *root, struct resource *new)\\n{\\n\\tstruct resource *conflict;\\n\\n\\tconflict = request_resource_conflict(root, new);\\n\\treturn conflict ? -EBUSY : 0;\\n}\\n\\nEXPORT_SYMBOL(request_resource);\\n\\n/**\\n * release_resource - release a previously reserved resource\\n * @old: resource pointer\\n */\\nint release_resource(struct resource *old)\\n{\\n\\tint retval;\\n\\n\\twrite_lock(&resource_lock);\\n\\tretval = __release_resource(old, true);\\n\\twrite_unlock(&resource_lock);\\n\\treturn retval;\\n}\\n\\nEXPORT_SYMBOL(release_resource);\\n\\n/**\\n * Finds the lowest iomem resource that covers part of [@start..@end].  The\\n * caller must specify @start, @end, @flags, and @desc (which may be\\n * IORES_DESC_NONE).\\n *\\n * If a resource is found, returns 0 and @*res is overwritten with the part\\n * of the resource that\\'s within [@start..@end]; if none is found, returns\\n * -1 or -EINVAL for other invalid parameters.\\n *\\n * This function walks the whole tree and not just first level children\\n * unless @first_lvl is true.\\n *\\n * @start:\\tstart address of the resource searched for\\n * @end:\\tend address of same resource\\n * @flags:\\tflags which the resource must have\\n * @desc:\\tdescriptor the resource must have\\n * @first_lvl:\\twalk only the first level children, if set\\n * @res:\\treturn ptr, if resource found\\n */\\nstatic int find_next_iomem_res(resource_size_t start, resource_size_t end,\\n\\t\\t\\t       unsigned long flags, unsigned long desc,\\n\\t\\t\\t       bool first_lvl, struct resource *res)\\n{\\n\\tstruct resource *p;\\n\\n\\tif (!res)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (start >= end)\\n\\t\\treturn -EINVAL;\\n\\n\\tread_lock(&resource_lock);\\n\\n\\tfor (p = iomem_resource.child; p; p = next_resource(p, first_lvl)) {\\n\\t\\tif ((p->flags & flags) != flags)\\n\\t\\t\\tcontinue;\\n\\t\\tif ((desc != IORES_DESC_NONE) && (desc != p->desc))\\n\\t\\t\\tcontinue;\\n\\t\\tif (p->start > end) {\\n\\t\\t\\tp = NULL;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t\\tif ((p->end >= start) && (p->start <= end))\\n\\t\\t\\tbreak;\\n\\t}\\n\\n\\tread_unlock(&resource_lock);\\n\\tif (!p)\\n\\t\\treturn -1;\\n\\n\\t/* copy data */\\n\\tres->start = max(start, p->start);\\n\\tres->end = min(end, p->end);\\n\\tres->flags = p->flags;\\n\\tres->desc = p->desc;\\n\\treturn 0;\\n}\\n\\nstatic int __walk_iomem_res_desc(resource_size_t start, resource_size_t end,\\n\\t\\t\\t\\t unsigned long flags, unsigned long desc,\\n\\t\\t\\t\\t bool first_lvl, void *arg,\\n\\t\\t\\t\\t int (*func)(struct resource *, void *))\\n{\\n\\tstruct resource res;\\n\\tint ret = -1;\\n\\n\\twhile (start < end &&\\n\\t       !find_next_iomem_res(start, end, flags, desc, first_lvl, &res)) {\\n\\t\\tret = (*func)(&res, arg);\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\n\\t\\tstart = res.end + 1;\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\n/**\\n * Walks through iomem resources and calls func() with matching resource\\n * ranges. This walks through whole tree and not just first level children.\\n * All the memory ranges which overlap start,end and also match flags and\\n * desc are valid candidates.\\n *\\n * @desc: I/O resource descriptor. Use IORES_DESC_NONE to skip @desc check.\\n * @flags: I/O resource flags\\n * @start: start addr\\n * @end: end addr\\n * @arg: function argument for the callback @func\\n * @func: callback function that is called for each qualifying resource area\\n *\\n * NOTE: For a new descriptor search, define a new IORES_DESC in\\n * <linux/ioport.h> and set it in \\'desc\\' of a target resource entry.\\n */\\nint walk_iomem_res_desc(unsigned long desc, unsigned long flags, u64 start,\\n\\t\\tu64 end, void *arg, int (*func)(struct resource *, void *))\\n{\\n\\treturn __walk_iomem_res_desc(start, end, flags, desc, false, arg, func);\\n}\\nEXPORT_SYMBOL_GPL(walk_iomem_res_desc);\\n\\n/*\\n * This function calls the @func callback against all memory ranges of type\\n * System RAM which are marked as IORESOURCE_SYSTEM_RAM and IORESOUCE_BUSY.\\n * Now, this function is only for System RAM, it deals with full ranges and\\n * not PFNs. If resources are not PFN-aligned, dealing with PFNs can truncate\\n * ranges.\\n */\\nint walk_system_ram_res(u64 start, u64 end, void *arg,\\n\\t\\t\\tint (*func)(struct resource *, void *))\\n{\\n\\tunsigned long flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;\\n\\n\\treturn __walk_iomem_res_desc(start, end, flags, IORES_DESC_NONE, true,\\n\\t\\t\\t\\t     arg, func);\\n}\\n\\n/*\\n * This function calls the @func callback against all memory ranges, which\\n * are ranges marked as IORESOURCE_MEM and IORESOUCE_BUSY.\\n */\\nint walk_mem_res(u64 start, u64 end, void *arg,\\n\\t\\t int (*func)(struct resource *, void *))\\n{\\n\\tunsigned long flags = IORESOURCE_MEM | IORESOURCE_BUSY;\\n\\n\\treturn __walk_iomem_res_desc(start, end, flags, IORES_DESC_NONE, true,\\n\\t\\t\\t\\t     arg, func);\\n}\\n\\n#if !defined(CONFIG_ARCH_HAS_WALK_MEMORY)\\n\\n/*\\n * This function calls the @func callback against all memory ranges of type\\n * System RAM which are marked as IORESOURCE_SYSTEM_RAM and IORESOUCE_BUSY.\\n * It is to be used only for System RAM.\\n */\\nint walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,\\n\\t\\t\\t  void *arg, int (*func)(unsigned long, unsigned long, void *))\\n{\\n\\tresource_size_t start, end;\\n\\tunsigned long flags;\\n\\tstruct resource res;\\n\\tunsigned long pfn, end_pfn;\\n\\tint ret = -1;\\n\\n\\tstart = (u64) start_pfn << PAGE_SHIFT;\\n\\tend = ((u64)(start_pfn + nr_pages) << PAGE_SHIFT) - 1;\\n\\tflags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;\\n\\twhile (start < end &&\\n\\t       !find_next_iomem_res(start, end, flags, IORES_DESC_NONE,\\n\\t\\t\\t\\t    true, &res)) {\\n\\t\\tpfn = (res.start + PAGE_SIZE - 1) >> PAGE_SHIFT;\\n\\t\\tend_pfn = (res.end + 1) >> PAGE_SHIFT;\\n\\t\\tif (end_pfn > pfn)\\n\\t\\t\\tret = (*func)(pfn, end_pfn - pfn, arg);\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\t\\tstart = res.end + 1;\\n\\t}\\n\\treturn ret;\\n}\\n\\n#endif\\n\\nstatic int __is_ram(unsigned long pfn, unsigned long nr_pages, void *arg)\\n{\\n\\treturn 1;\\n}\\n\\n/*\\n * This generic page_is_ram() returns true if specified address is\\n * registered as System RAM in iomem_resource list.\\n */\\nint __weak page_is_ram(unsigned long pfn)\\n{\\n\\treturn walk_system_ram_range(pfn, 1, NULL, __is_ram) == 1;\\n}\\nEXPORT_SYMBOL_GPL(page_is_ram);\\n\\n/**\\n * region_intersects() - determine intersection of region with known resources\\n * @start: region start address\\n * @size: size of region\\n * @flags: flags of resource (in iomem_resource)\\n * @desc: descriptor of resource (in iomem_resource) or IORES_DESC_NONE\\n *\\n * Check if the specified region partially overlaps or fully eclipses a\\n * resource identified by @flags and @desc (optional with IORES_DESC_NONE).\\n * Return REGION_DISJOINT if the region does not overlap @flags/@desc,\\n * return REGION_MIXED if the region overlaps @flags/@desc and another\\n * resource, and return REGION_INTERSECTS if the region overlaps @flags/@desc\\n * and no other defined resource. Note that REGION_INTERSECTS is also\\n * returned in the case when the specified region overlaps RAM and undefined\\n * memory holes.\\n *\\n * region_intersect() is used by memory remapping functions to ensure\\n * the user is not remapping RAM and is a vast speed up over walking\\n * through the resource table page by page.\\n */\\nint region_intersects(resource_size_t start, size_t size, unsigned long flags,\\n\\t\\t      unsigned long desc)\\n{\\n\\tresource_size_t end = start + size - 1;\\n\\tint type = 0; int other = 0;\\n\\tstruct resource *p;\\n\\n\\tread_lock(&resource_lock);\\n\\tfor (p = iomem_resource.child; p ; p = p->sibling) {\\n\\t\\tbool is_type = (((p->flags & flags) == flags) &&\\n\\t\\t\\t\\t((desc == IORES_DESC_NONE) ||\\n\\t\\t\\t\\t (desc == p->desc)));\\n\\n\\t\\tif (start >= p->start && start <= p->end)\\n\\t\\t\\tis_type ? type++ : other++;\\n\\t\\tif (end >= p->start && end <= p->end)\\n\\t\\t\\tis_type ? type++ : other++;\\n\\t\\tif (p->start >= start && p->end <= end)\\n\\t\\t\\tis_type ? type++ : other++;\\n\\t}\\n\\tread_unlock(&resource_lock);\\n\\n\\tif (other == 0)\\n\\t\\treturn type ? REGION_INTERSECTS : REGION_DISJOINT;\\n\\n\\tif (type)\\n\\t\\treturn REGION_MIXED;\\n\\n\\treturn REGION_DISJOINT;\\n}\\nEXPORT_SYMBOL_GPL(region_intersects);\\n\\nvoid __weak arch_remove_reservations(struct resource *avail)\\n{\\n}\\n\\nstatic resource_size_t simple_align_resource(void *data,\\n\\t\\t\\t\\t\\t     const struct resource *avail,\\n\\t\\t\\t\\t\\t     resource_size_t size,\\n\\t\\t\\t\\t\\t     resource_size_t align)\\n{\\n\\treturn avail->start;\\n}\\n\\nstatic void resource_clip(struct resource *res, resource_size_t min,\\n\\t\\t\\t  resource_size_t max)\\n{\\n\\tif (res->start < min)\\n\\t\\tres->start = min;\\n\\tif (res->end > max)\\n\\t\\tres->end = max;\\n}\\n\\n/*\\n * Find empty slot in the resource tree with the given range and\\n * alignment constraints\\n */\\nstatic int __find_resource(struct resource *root, struct resource *old,\\n\\t\\t\\t struct resource *new,\\n\\t\\t\\t resource_size_t  size,\\n\\t\\t\\t struct resource_constraint *constraint)\\n{\\n\\tstruct resource *this = root->child;\\n\\tstruct resource tmp = *new, avail, alloc;\\n\\n\\ttmp.start = root->start;\\n\\t/*\\n\\t * Skip past an allocated resource that starts at 0, since the assignment\\n\\t * of this->start - 1 to tmp->end below would cause an underflow.\\n\\t */\\n\\tif (this && this->start == root->start) {\\n\\t\\ttmp.start = (this == old) ? old->start : this->end + 1;\\n\\t\\tthis = this->sibling;\\n\\t}\\n\\tfor(;;) {\\n\\t\\tif (this)\\n\\t\\t\\ttmp.end = (this == old) ?  this->end : this->start - 1;\\n\\t\\telse\\n\\t\\t\\ttmp.end = root->end;\\n\\n\\t\\tif (tmp.end < tmp.start)\\n\\t\\t\\tgoto next;\\n\\n\\t\\tresource_clip(&tmp, constraint->min, constraint->max);\\n\\t\\tarch_remove_reservations(&tmp);\\n\\n\\t\\t/* Check for overflow after ALIGN() */\\n\\t\\tavail.start = ALIGN(tmp.start, constraint->align);\\n\\t\\tavail.end = tmp.end;\\n\\t\\tavail.flags = new->flags & ~IORESOURCE_UNSET;\\n\\t\\tif (avail.start >= tmp.start) {\\n\\t\\t\\talloc.flags = avail.flags;\\n\\t\\t\\talloc.start = constraint->alignf(constraint->alignf_data, &avail,\\n\\t\\t\\t\\t\\tsize, constraint->align);\\n\\t\\t\\talloc.end = alloc.start + size - 1;\\n\\t\\t\\tif (alloc.start <= alloc.end &&\\n\\t\\t\\t    resource_contains(&avail, &alloc)) {\\n\\t\\t\\t\\tnew->start = alloc.start;\\n\\t\\t\\t\\tnew->end = alloc.end;\\n\\t\\t\\t\\treturn 0;\\n\\t\\t\\t}\\n\\t\\t}\\n\\nnext:\\t\\tif (!this || this->end == root->end)\\n\\t\\t\\tbreak;\\n\\n\\t\\tif (this != old)\\n\\t\\t\\ttmp.start = this->end + 1;\\n\\t\\tthis = this->sibling;\\n\\t}\\n\\treturn -EBUSY;\\n}\\n\\n/*\\n * Find empty slot in the resource tree given range and alignment.\\n */\\nstatic int find_resource(struct resource *root, struct resource *new,\\n\\t\\t\\tresource_size_t size,\\n\\t\\t\\tstruct resource_constraint  *constraint)\\n{\\n\\treturn  __find_resource(root, NULL, new, size, constraint);\\n}\\n\\n/**\\n * reallocate_resource - allocate a slot in the resource tree given range & alignment.\\n *\\tThe resource will be relocated if the new size cannot be reallocated in the\\n *\\tcurrent location.\\n *\\n * @root: root resource descriptor\\n * @old:  resource descriptor desired by caller\\n * @newsize: new size of the resource descriptor\\n * @constraint: the size and alignment constraints to be met.\\n */\\nstatic int reallocate_resource(struct resource *root, struct resource *old,\\n\\t\\t\\t       resource_size_t newsize,\\n\\t\\t\\t       struct resource_constraint *constraint)\\n{\\n\\tint err=0;\\n\\tstruct resource new = *old;\\n\\tstruct resource *conflict;\\n\\n\\twrite_lock(&resource_lock);\\n\\n\\tif ((err = __find_resource(root, old, &new, newsize, constraint)))\\n\\t\\tgoto out;\\n\\n\\tif (resource_contains(&new, old)) {\\n\\t\\told->start = new.start;\\n\\t\\told->end = new.end;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tif (old->child) {\\n\\t\\terr = -EBUSY;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\tif (resource_contains(old, &new)) {\\n\\t\\told->start = new.start;\\n\\t\\told->end = new.end;\\n\\t} else {\\n\\t\\t__release_resource(old, true);\\n\\t\\t*old = new;\\n\\t\\tconflict = __request_resource(root, old);\\n\\t\\tBUG_ON(conflict);\\n\\t}\\nout:\\n\\twrite_unlock(&resource_lock);\\n\\treturn err;\\n}\\n\\n\\n/**\\n * allocate_resource - allocate empty slot in the resource tree given range & alignment.\\n * \\tThe resource will be reallocated with a new size if it was already allocated\\n * @root: root resource descriptor\\n * @new: resource descriptor desired by caller\\n * @size: requested resource region size\\n * @min: minimum boundary to allocate\\n * @max: maximum boundary to allocate\\n * @align: alignment requested, in bytes\\n * @alignf: alignment function, optional, called if not NULL\\n * @alignf_data: arbitrary data to pass to the @alignf function\\n */\\nint allocate_resource(struct resource *root, struct resource *new,\\n\\t\\t      resource_size_t size, resource_size_t min,\\n\\t\\t      resource_size_t max, resource_size_t align,\\n\\t\\t      resource_size_t (*alignf)(void *,\\n\\t\\t\\t\\t\\t\\tconst struct resource *,\\n\\t\\t\\t\\t\\t\\tresource_size_t,\\n\\t\\t\\t\\t\\t\\tresource_size_t),\\n\\t\\t      void *alignf_data)\\n{\\n\\tint err;\\n\\tstruct resource_constraint constraint;\\n\\n\\tif (!alignf)\\n\\t\\talignf = simple_align_resource;\\n\\n\\tconstraint.min = min;\\n\\tconstraint.max = max;\\n\\tconstraint.align = align;\\n\\tconstraint.alignf = alignf;\\n\\tconstraint.alignf_data = alignf_data;\\n\\n\\tif ( new->parent ) {\\n\\t\\t/* resource is already allocated, try reallocating with\\n\\t\\t   the new constraints */\\n\\t\\treturn reallocate_resource(root, new, size, &constraint);\\n\\t}\\n\\n\\twrite_lock(&resource_lock);\\n\\terr = find_resource(root, new, size, &constraint);\\n\\tif (err >= 0 && __request_resource(root, new))\\n\\t\\terr = -EBUSY;\\n\\twrite_unlock(&resource_lock);\\n\\treturn err;\\n}\\n\\nEXPORT_SYMBOL(allocate_resource);\\n\\n/**\\n * lookup_resource - find an existing resource by a resource start address\\n * @root: root resource descriptor\\n * @start: resource start address\\n *\\n * Returns a pointer to the resource if found, NULL otherwise\\n */\\nstruct resource *lookup_resource(struct resource *root, resource_size_t start)\\n{\\n\\tstruct resource *res;\\n\\n\\tread_lock(&resource_lock);\\n\\tfor (res = root->child; res; res = res->sibling) {\\n\\t\\tif (res->start == start)\\n\\t\\t\\tbreak;\\n\\t}\\n\\tread_unlock(&resource_lock);\\n\\n\\treturn res;\\n}\\n\\n/*\\n * Insert a resource into the resource tree. If successful, return NULL,\\n * otherwise return the conflicting resource (compare to __request_resource())\\n */\\nstatic struct resource * __insert_resource(struct resource *parent, struct resource *new)\\n{\\n\\tstruct resource *first, *next;\\n\\n\\tfor (;; parent = first) {\\n\\t\\tfirst = __request_resource(parent, new);\\n\\t\\tif (!first)\\n\\t\\t\\treturn first;\\n\\n\\t\\tif (first == parent)\\n\\t\\t\\treturn first;\\n\\t\\tif (WARN_ON(first == new))\\t/* duplicated insertion */\\n\\t\\t\\treturn first;\\n\\n\\t\\tif ((first->start > new->start) || (first->end < new->end))\\n\\t\\t\\tbreak;\\n\\t\\tif ((first->start == new->start) && (first->end == new->end))\\n\\t\\t\\tbreak;\\n\\t}\\n\\n\\tfor (next = first; ; next = next->sibling) {\\n\\t\\t/* Partial overlap? Bad, and unfixable */\\n\\t\\tif (next->start < new->start || next->end > new->end)\\n\\t\\t\\treturn next;\\n\\t\\tif (!next->sibling)\\n\\t\\t\\tbreak;\\n\\t\\tif (next->sibling->start > new->end)\\n\\t\\t\\tbreak;\\n\\t}\\n\\n\\tnew->parent = parent;\\n\\tnew->sibling = next->sibling;\\n\\tnew->child = first;\\n\\n\\tnext->sibling = NULL;\\n\\tfor (next = first; next; next = next->sibling)\\n\\t\\tnext->parent = new;\\n\\n\\tif (parent->child == first) {\\n\\t\\tparent->child = new;\\n\\t} else {\\n\\t\\tnext = parent->child;\\n\\t\\twhile (next->sibling != first)\\n\\t\\t\\tnext = next->sibling;\\n\\t\\tnext->sibling = new;\\n\\t}\\n\\treturn NULL;\\n}\\n\\n/**\\n * insert_resource_conflict - Inserts resource in the resource tree\\n * @parent: parent of the new resource\\n * @new: new resource to insert\\n *\\n * Returns 0 on success, conflict resource if the resource can\\'t be inserted.\\n *\\n * This function is equivalent to request_resource_conflict when no conflict\\n * happens. If a conflict happens, and the conflicting resources\\n * entirely fit within the range of the new resource, then the new\\n * resource is inserted and the conflicting resources become children of\\n * the new resource.\\n *\\n * This function is intended for producers of resources, such as FW modules\\n * and bus drivers.\\n */\\nstruct resource *insert_resource_conflict(struct resource *parent, struct resource *new)\\n{\\n\\tstruct resource *conflict;\\n\\n\\twrite_lock(&resource_lock);\\n\\tconflict = __insert_resource(parent, new);\\n\\twrite_unlock(&resource_lock);\\n\\treturn conflict;\\n}\\n\\n/**\\n * insert_resource - Inserts a resource in the resource tree\\n * @parent: parent of the new resource\\n * @new: new resource to insert\\n *\\n * Returns 0 on success, -EBUSY if the resource can\\'t be inserted.\\n *\\n * This function is intended for producers of resources, such as FW modules\\n * and bus drivers.\\n */\\nint insert_resource(struct resource *parent, struct resource *new)\\n{\\n\\tstruct resource *conflict;\\n\\n\\tconflict = insert_resource_conflict(parent, new);\\n\\treturn conflict ? -EBUSY : 0;\\n}\\nEXPORT_SYMBOL_GPL(insert_resource);\\n\\n/**\\n * insert_resource_expand_to_fit - Insert a resource into the resource tree\\n * @root: root resource descriptor\\n * @new: new resource to insert\\n *\\n * Insert a resource into the resource tree, possibly expanding it in order\\n * to make it encompass any conflicting resources.\\n */\\nvoid insert_resource_expand_to_fit(struct resource *root, struct resource *new)\\n{\\n\\tif (new->parent)\\n\\t\\treturn;\\n\\n\\twrite_lock(&resource_lock);\\n\\tfor (;;) {\\n\\t\\tstruct resource *conflict;\\n\\n\\t\\tconflict = __insert_resource(root, new);\\n\\t\\tif (!conflict)\\n\\t\\t\\tbreak;\\n\\t\\tif (conflict == root)\\n\\t\\t\\tbreak;\\n\\n\\t\\t/* Ok, expand resource to cover the conflict, then try again .. */\\n\\t\\tif (conflict->start < new->start)\\n\\t\\t\\tnew->start = conflict->start;\\n\\t\\tif (conflict->end > new->end)\\n\\t\\t\\tnew->end = conflict->end;\\n\\n\\t\\tprintk(\"Expanded resource %s due to conflict with %s\\\\n\", new->name, conflict->name);\\n\\t}\\n\\twrite_unlock(&resource_lock);\\n}\\n\\n/**\\n * remove_resource - Remove a resource in the resource tree\\n * @old: resource to remove\\n *\\n * Returns 0 on success, -EINVAL if the resource is not valid.\\n *\\n * This function removes a resource previously inserted by insert_resource()\\n * or insert_resource_conflict(), and moves the children (if any) up to\\n * where they were before.  insert_resource() and insert_resource_conflict()\\n * insert a new resource, and move any conflicting resources down to the\\n * children of the new resource.\\n *\\n * insert_resource(), insert_resource_conflict() and remove_resource() are\\n * intended for producers of resources, such as FW modules and bus drivers.\\n */\\nint remove_resource(struct resource *old)\\n{\\n\\tint retval;\\n\\n\\twrite_lock(&resource_lock);\\n\\tretval = __release_resource(old, false);\\n\\twrite_unlock(&resource_lock);\\n\\treturn retval;\\n}\\nEXPORT_SYMBOL_GPL(remove_resource);\\n\\nstatic int __adjust_resource(struct resource *res, resource_size_t start,\\n\\t\\t\\t\\tresource_size_t size)\\n{\\n\\tstruct resource *tmp, *parent = res->parent;\\n\\tresource_size_t end = start + size - 1;\\n\\tint result = -EBUSY;\\n\\n\\tif (!parent)\\n\\t\\tgoto skip;\\n\\n\\tif ((start < parent->start) || (end > parent->end))\\n\\t\\tgoto out;\\n\\n\\tif (res->sibling && (res->sibling->start <= end))\\n\\t\\tgoto out;\\n\\n\\ttmp = parent->child;\\n\\tif (tmp != res) {\\n\\t\\twhile (tmp->sibling != res)\\n\\t\\t\\ttmp = tmp->sibling;\\n\\t\\tif (start <= tmp->end)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\nskip:\\n\\tfor (tmp = res->child; tmp; tmp = tmp->sibling)\\n\\t\\tif ((tmp->start < start) || (tmp->end > end))\\n\\t\\t\\tgoto out;\\n\\n\\tres->start = start;\\n\\tres->end = end;\\n\\tresult = 0;\\n\\n out:\\n\\treturn result;\\n}\\n\\n/**\\n * adjust_resource - modify a resource\\'s start and size\\n * @res: resource to modify\\n * @start: new start value\\n * @size: new size\\n *\\n * Given an existing resource, change its start and size to match the\\n * arguments.  Returns 0 on success, -EBUSY if it can\\'t fit.\\n * Existing children of the resource are assumed to be immutable.\\n */\\nint adjust_resource(struct resource *res, resource_size_t start,\\n\\t\\t    resource_size_t size)\\n{\\n\\tint result;\\n\\n\\twrite_lock(&resource_lock);\\n\\tresult = __adjust_resource(res, start, size);\\n\\twrite_unlock(&resource_lock);\\n\\treturn result;\\n}\\nEXPORT_SYMBOL(adjust_resource);\\n\\nstatic void __init\\n__reserve_region_with_split(struct resource *root, resource_size_t start,\\n\\t\\t\\t    resource_size_t end, const char *name)\\n{\\n\\tstruct resource *parent = root;\\n\\tstruct resource *conflict;\\n\\tstruct resource *res = alloc_resource(GFP_ATOMIC);\\n\\tstruct resource *next_res = NULL;\\n\\tint type = resource_type(root);\\n\\n\\tif (!res)\\n\\t\\treturn;\\n\\n\\tres->name = name;\\n\\tres->start = start;\\n\\tres->end = end;\\n\\tres->flags = type | IORESOURCE_BUSY;\\n\\tres->desc = IORES_DESC_NONE;\\n\\n\\twhile (1) {\\n\\n\\t\\tconflict = __request_resource(parent, res);\\n\\t\\tif (!conflict) {\\n\\t\\t\\tif (!next_res)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tres = next_res;\\n\\t\\t\\tnext_res = NULL;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\t/* conflict covered whole area */\\n\\t\\tif (conflict->start <= res->start &&\\n\\t\\t\\t\\tconflict->end >= res->end) {\\n\\t\\t\\tfree_resource(res);\\n\\t\\t\\tWARN_ON(next_res);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\t/* failed, split and try again */\\n\\t\\tif (conflict->start > res->start) {\\n\\t\\t\\tend = res->end;\\n\\t\\t\\tres->end = conflict->start - 1;\\n\\t\\t\\tif (conflict->end < end) {\\n\\t\\t\\t\\tnext_res = alloc_resource(GFP_ATOMIC);\\n\\t\\t\\t\\tif (!next_res) {\\n\\t\\t\\t\\t\\tfree_resource(res);\\n\\t\\t\\t\\t\\tbreak;\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\tnext_res->name = name;\\n\\t\\t\\t\\tnext_res->start = conflict->end + 1;\\n\\t\\t\\t\\tnext_res->end = end;\\n\\t\\t\\t\\tnext_res->flags = type | IORESOURCE_BUSY;\\n\\t\\t\\t\\tnext_res->desc = IORES_DESC_NONE;\\n\\t\\t\\t}\\n\\t\\t} else {\\n\\t\\t\\tres->start = conflict->end + 1;\\n\\t\\t}\\n\\t}\\n\\n}\\n\\nvoid __init\\nreserve_region_with_split(struct resource *root, resource_size_t start,\\n\\t\\t\\t  resource_size_t end, const char *name)\\n{\\n\\tint abort = 0;\\n\\n\\twrite_lock(&resource_lock);\\n\\tif (root->start > start || root->end < end) {\\n\\t\\tpr_err(\"requested range [0x%llx-0x%llx] not in root %pr\\\\n\",\\n\\t\\t       (unsigned long long)start, (unsigned long long)end,\\n\\t\\t       root);\\n\\t\\tif (start > root->end || end < root->start)\\n\\t\\t\\tabort = 1;\\n\\t\\telse {\\n\\t\\t\\tif (end > root->end)\\n\\t\\t\\t\\tend = root->end;\\n\\t\\t\\tif (start < root->start)\\n\\t\\t\\t\\tstart = root->start;\\n\\t\\t\\tpr_err(\"fixing request to [0x%llx-0x%llx]\\\\n\",\\n\\t\\t\\t       (unsigned long long)start,\\n\\t\\t\\t       (unsigned long long)end);\\n\\t\\t}\\n\\t\\tdump_stack();\\n\\t}\\n\\tif (!abort)\\n\\t\\t__reserve_region_with_split(root, start, end, name);\\n\\twrite_unlock(&resource_lock);\\n}\\n\\n/**\\n * resource_alignment - calculate resource\\'s alignment\\n * @res: resource pointer\\n *\\n * Returns alignment on success, 0 (invalid alignment) on failure.\\n */\\nresource_size_t resource_alignment(struct resource *res)\\n{\\n\\tswitch (res->flags & (IORESOURCE_SIZEALIGN | IORESOURCE_STARTALIGN)) {\\n\\tcase IORESOURCE_SIZEALIGN:\\n\\t\\treturn resource_size(res);\\n\\tcase IORESOURCE_STARTALIGN:\\n\\t\\treturn res->start;\\n\\tdefault:\\n\\t\\treturn 0;\\n\\t}\\n}\\n\\n/*\\n * This is compatibility stuff for IO resources.\\n *\\n * Note how this, unlike the above, knows about\\n * the IO flag meanings (busy etc).\\n *\\n * request_region creates a new busy region.\\n *\\n * release_region releases a matching busy region.\\n */\\n\\nstatic DECLARE_WAIT_QUEUE_HEAD(muxed_resource_wait);\\n\\n/**\\n * __request_region - create a new busy resource region\\n * @parent: parent resource descriptor\\n * @start: resource start address\\n * @n: resource region size\\n * @name: reserving caller\\'s ID string\\n * @flags: IO resource flags\\n */\\nstruct resource * __request_region(struct resource *parent,\\n\\t\\t\\t\\t   resource_size_t start, resource_size_t n,\\n\\t\\t\\t\\t   const char *name, int flags)\\n{\\n\\tDECLARE_WAITQUEUE(wait, current);\\n\\tstruct resource *res = alloc_resource(GFP_KERNEL);\\n\\n\\tif (!res)\\n\\t\\treturn NULL;\\n\\n\\tres->name = name;\\n\\tres->start = start;\\n\\tres->end = start + n - 1;\\n\\n\\twrite_lock(&resource_lock);\\n\\n\\tfor (;;) {\\n\\t\\tstruct resource *conflict;\\n\\n\\t\\tres->flags = resource_type(parent) | resource_ext_type(parent);\\n\\t\\tres->flags |= IORESOURCE_BUSY | flags;\\n\\t\\tres->desc = parent->desc;\\n\\n\\t\\tconflict = __request_resource(parent, res);\\n\\t\\tif (!conflict)\\n\\t\\t\\tbreak;\\n\\t\\tif (conflict != parent) {\\n\\t\\t\\tif (!(conflict->flags & IORESOURCE_BUSY)) {\\n\\t\\t\\t\\tparent = conflict;\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tif (conflict->flags & flags & IORESOURCE_MUXED) {\\n\\t\\t\\tadd_wait_queue(&muxed_resource_wait, &wait);\\n\\t\\t\\twrite_unlock(&resource_lock);\\n\\t\\t\\tset_current_state(TASK_UNINTERRUPTIBLE);\\n\\t\\t\\tschedule();\\n\\t\\t\\tremove_wait_queue(&muxed_resource_wait, &wait);\\n\\t\\t\\twrite_lock(&resource_lock);\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\t/* Uhhuh, that didn\\'t work out.. */\\n\\t\\tfree_resource(res);\\n\\t\\tres = NULL;\\n\\t\\tbreak;\\n\\t}\\n\\twrite_unlock(&resource_lock);\\n\\treturn res;\\n}\\nEXPORT_SYMBOL(__request_region);\\n\\n/**\\n * __release_region - release a previously reserved resource region\\n * @parent: parent resource descriptor\\n * @start: resource start address\\n * @n: resource region size\\n *\\n * The described resource region must match a currently busy region.\\n */\\nvoid __release_region(struct resource *parent, resource_size_t start,\\n\\t\\t      resource_size_t n)\\n{\\n\\tstruct resource **p;\\n\\tresource_size_t end;\\n\\n\\tp = &parent->child;\\n\\tend = start + n - 1;\\n\\n\\twrite_lock(&resource_lock);\\n\\n\\tfor (;;) {\\n\\t\\tstruct resource *res = *p;\\n\\n\\t\\tif (!res)\\n\\t\\t\\tbreak;\\n\\t\\tif (res->start <= start && res->end >= end) {\\n\\t\\t\\tif (!(res->flags & IORESOURCE_BUSY)) {\\n\\t\\t\\t\\tp = &res->child;\\n\\t\\t\\t\\tcontinue;\\n\\t\\t\\t}\\n\\t\\t\\tif (res->start != start || res->end != end)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t*p = res->sibling;\\n\\t\\t\\twrite_unlock(&resource_lock);\\n\\t\\t\\tif (res->flags & IORESOURCE_MUXED)\\n\\t\\t\\t\\twake_up(&muxed_resource_wait);\\n\\t\\t\\tfree_resource(res);\\n\\t\\t\\treturn;\\n\\t\\t}\\n\\t\\tp = &res->sibling;\\n\\t}\\n\\n\\twrite_unlock(&resource_lock);\\n\\n\\tprintk(KERN_WARNING \"Trying to free nonexistent resource \"\\n\\t\\t\"<%016llx-%016llx>\\\\n\", (unsigned long long)start,\\n\\t\\t(unsigned long long)end);\\n}\\nEXPORT_SYMBOL(__release_region);\\n\\n#ifdef CONFIG_MEMORY_HOTREMOVE\\n/**\\n * release_mem_region_adjustable - release a previously reserved memory region\\n * @parent: parent resource descriptor\\n * @start: resource start address\\n * @size: resource region size\\n *\\n * This interface is intended for memory hot-delete.  The requested region\\n * is released from a currently busy memory resource.  The requested region\\n * must either match exactly or fit into a single busy resource entry.  In\\n * the latter case, the remaining resource is adjusted accordingly.\\n * Existing children of the busy memory resource must be immutable in the\\n * request.\\n *\\n * Note:\\n * - Additional release conditions, such as overlapping region, can be\\n *   supported after they are confirmed as valid cases.\\n * - When a busy memory resource gets split into two entries, the code\\n *   assumes that all children remain in the lower address entry for\\n *   simplicity.  Enhance this logic when necessary.\\n */\\nint release_mem_region_adjustable(struct resource *parent,\\n\\t\\t\\t\\t  resource_size_t start, resource_size_t size)\\n{\\n\\tstruct resource **p;\\n\\tstruct resource *res;\\n\\tstruct resource *new_res;\\n\\tresource_size_t end;\\n\\tint ret = -EINVAL;\\n\\n\\tend = start + size - 1;\\n\\tif ((start < parent->start) || (end > parent->end))\\n\\t\\treturn ret;\\n\\n\\t/* The alloc_resource() result gets checked later */\\n\\tnew_res = alloc_resource(GFP_KERNEL);\\n\\n\\tp = &parent->child;\\n\\twrite_lock(&resource_lock);\\n\\n\\twhile ((res = *p)) {\\n\\t\\tif (res->start >= end)\\n\\t\\t\\tbreak;\\n\\n\\t\\t/* look for the next resource if it does not fit into */\\n\\t\\tif (res->start > start || res->end < end) {\\n\\t\\t\\tp = &res->sibling;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\t/*\\n\\t\\t * All memory regions added from memory-hotplug path have the\\n\\t\\t * flag IORESOURCE_SYSTEM_RAM. If the resource does not have\\n\\t\\t * this flag, we know that we are dealing with a resource coming\\n\\t\\t * from HMM/devm. HMM/devm use another mechanism to add/release\\n\\t\\t * a resource. This goes via devm_request_mem_region and\\n\\t\\t * devm_release_mem_region.\\n\\t\\t * HMM/devm take care to release their resources when they want,\\n\\t\\t * so if we are dealing with them, let us just back off here.\\n\\t\\t */\\n\\t\\tif (!(res->flags & IORESOURCE_SYSRAM)) {\\n\\t\\t\\tret = 0;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tif (!(res->flags & IORESOURCE_MEM))\\n\\t\\t\\tbreak;\\n\\n\\t\\tif (!(res->flags & IORESOURCE_BUSY)) {\\n\\t\\t\\tp = &res->child;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\t/* found the target resource; let\\'s adjust accordingly */\\n\\t\\tif (res->start == start && res->end == end) {\\n\\t\\t\\t/* free the whole entry */\\n\\t\\t\\t*p = res->sibling;\\n\\t\\t\\tfree_resource(res);\\n\\t\\t\\tret = 0;\\n\\t\\t} else if (res->start == start && res->end != end) {\\n\\t\\t\\t/* adjust the start */\\n\\t\\t\\tret = __adjust_resource(res, end + 1,\\n\\t\\t\\t\\t\\t\\tres->end - end);\\n\\t\\t} else if (res->start != start && res->end == end) {\\n\\t\\t\\t/* adjust the end */\\n\\t\\t\\tret = __adjust_resource(res, res->start,\\n\\t\\t\\t\\t\\t\\tstart - res->start);\\n\\t\\t} else {\\n\\t\\t\\t/* split into two entries */\\n\\t\\t\\tif (!new_res) {\\n\\t\\t\\t\\tret = -ENOMEM;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t\\tnew_res->name = res->name;\\n\\t\\t\\tnew_res->start = end + 1;\\n\\t\\t\\tnew_res->end = res->end;\\n\\t\\t\\tnew_res->flags = res->flags;\\n\\t\\t\\tnew_res->desc = res->desc;\\n\\t\\t\\tnew_res->parent = res->parent;\\n\\t\\t\\tnew_res->sibling = res->sibling;\\n\\t\\t\\tnew_res->child = NULL;\\n\\n\\t\\t\\tret = __adjust_resource(res, res->start,\\n\\t\\t\\t\\t\\t\\tstart - res->start);\\n\\t\\t\\tif (ret)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tres->sibling = new_res;\\n\\t\\t\\tnew_res = NULL;\\n\\t\\t}\\n\\n\\t\\tbreak;\\n\\t}\\n\\n\\twrite_unlock(&resource_lock);\\n\\tfree_resource(new_res);\\n\\treturn ret;\\n}\\n#endif\\t/* CONFIG_MEMORY_HOTREMOVE */\\n\\n/*\\n * Managed region resource\\n */\\nstatic void devm_resource_release(struct device *dev, void *ptr)\\n{\\n\\tstruct resource **r = ptr;\\n\\n\\trelease_resource(*r);\\n}\\n\\n/**\\n * devm_request_resource() - request and reserve an I/O or memory resource\\n * @dev: device for which to request the resource\\n * @root: root of the resource tree from which to request the resource\\n * @new: descriptor of the resource to request\\n *\\n * This is a device-managed version of request_resource(). There is usually\\n * no need to release resources requested by this function explicitly since\\n * that will be taken care of when the device is unbound from its driver.\\n * If for some reason the resource needs to be released explicitly, because\\n * of ordering issues for example, drivers must call devm_release_resource()\\n * rather than the regular release_resource().\\n *\\n * When a conflict is detected between any existing resources and the newly\\n * requested resource, an error message will be printed.\\n *\\n * Returns 0 on success or a negative error code on failure.\\n */\\nint devm_request_resource(struct device *dev, struct resource *root,\\n\\t\\t\\t  struct resource *new)\\n{\\n\\tstruct resource *conflict, **ptr;\\n\\n\\tptr = devres_alloc(devm_resource_release, sizeof(*ptr), GFP_KERNEL);\\n\\tif (!ptr)\\n\\t\\treturn -ENOMEM;\\n\\n\\t*ptr = new;\\n\\n\\tconflict = request_resource_conflict(root, new);\\n\\tif (conflict) {\\n\\t\\tdev_err(dev, \"resource collision: %pR conflicts with %s %pR\\\\n\",\\n\\t\\t\\tnew, conflict->name, conflict);\\n\\t\\tdevres_free(ptr);\\n\\t\\treturn -EBUSY;\\n\\t}\\n\\n\\tdevres_add(dev, ptr);\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL(devm_request_resource);\\n\\nstatic int devm_resource_match(struct device *dev, void *res, void *data)\\n{\\n\\tstruct resource **ptr = res;\\n\\n\\treturn *ptr == data;\\n}\\n\\n/**\\n * devm_release_resource() - release a previously requested resource\\n * @dev: device for which to release the resource\\n * @new: descriptor of the resource to release\\n *\\n * Releases a resource previously requested using devm_request_resource().\\n */\\nvoid devm_release_resource(struct device *dev, struct resource *new)\\n{\\n\\tWARN_ON(devres_release(dev, devm_resource_release, devm_resource_match,\\n\\t\\t\\t       new));\\n}\\nEXPORT_SYMBOL(devm_release_resource);\\n\\nstruct region_devres {\\n\\tstruct resource *parent;\\n\\tresource_size_t start;\\n\\tresource_size_t n;\\n};\\n\\nstatic void devm_region_release(struct device *dev, void *res)\\n{\\n\\tstruct region_devres *this = res;\\n\\n\\t__release_region(this->parent, this->start, this->n);\\n}\\n\\nstatic int devm_region_match(struct device *dev, void *res, void *match_data)\\n{\\n\\tstruct region_devres *this = res, *match = match_data;\\n\\n\\treturn this->parent == match->parent &&\\n\\t\\tthis->start == match->start && this->n == match->n;\\n}\\n\\nstruct resource *\\n__devm_request_region(struct device *dev, struct resource *parent,\\n\\t\\t      resource_size_t start, resource_size_t n, const char *name)\\n{\\n\\tstruct region_devres *dr = NULL;\\n\\tstruct resource *res;\\n\\n\\tdr = devres_alloc(devm_region_release, sizeof(struct region_devres),\\n\\t\\t\\t  GFP_KERNEL);\\n\\tif (!dr)\\n\\t\\treturn NULL;\\n\\n\\tdr->parent = parent;\\n\\tdr->start = start;\\n\\tdr->n = n;\\n\\n\\tres = __request_region(parent, start, n, name, 0);\\n\\tif (res)\\n\\t\\tdevres_add(dev, dr);\\n\\telse\\n\\t\\tdevres_free(dr);\\n\\n\\treturn res;\\n}\\nEXPORT_SYMBOL(__devm_request_region);\\n\\nvoid __devm_release_region(struct device *dev, struct resource *parent,\\n\\t\\t\\t   resource_size_t start, resource_size_t n)\\n{\\n\\tstruct region_devres match_data = { parent, start, n };\\n\\n\\t__release_region(parent, start, n);\\n\\tWARN_ON(devres_destroy(dev, devm_region_release, devm_region_match,\\n\\t\\t\\t       &match_data));\\n}\\nEXPORT_SYMBOL(__devm_release_region);\\n\\n/*\\n * Reserve I/O ports or memory based on \"reserve=\" kernel parameter.\\n */\\n#define MAXRESERVE 4\\nstatic int __init reserve_setup(char *str)\\n{\\n\\tstatic int reserved;\\n\\tstatic struct resource reserve[MAXRESERVE];\\n\\n\\tfor (;;) {\\n\\t\\tunsigned int io_start, io_num;\\n\\t\\tint x = reserved;\\n\\t\\tstruct resource *parent;\\n\\n\\t\\tif (get_option(&str, &io_start) != 2)\\n\\t\\t\\tbreak;\\n\\t\\tif (get_option(&str, &io_num) == 0)\\n\\t\\t\\tbreak;\\n\\t\\tif (x < MAXRESERVE) {\\n\\t\\t\\tstruct resource *res = reserve + x;\\n\\n\\t\\t\\t/*\\n\\t\\t\\t * If the region starts below 0x10000, we assume it\\'s\\n\\t\\t\\t * I/O port space; otherwise assume it\\'s memory.\\n\\t\\t\\t */\\n\\t\\t\\tif (io_start < 0x10000) {\\n\\t\\t\\t\\tres->flags = IORESOURCE_IO;\\n\\t\\t\\t\\tparent = &ioport_resource;\\n\\t\\t\\t} else {\\n\\t\\t\\t\\tres->flags = IORESOURCE_MEM;\\n\\t\\t\\t\\tparent = &iomem_resource;\\n\\t\\t\\t}\\n\\t\\t\\tres->name = \"reserved\";\\n\\t\\t\\tres->start = io_start;\\n\\t\\t\\tres->end = io_start + io_num - 1;\\n\\t\\t\\tres->flags |= IORESOURCE_BUSY;\\n\\t\\t\\tres->desc = IORES_DESC_NONE;\\n\\t\\t\\tres->child = NULL;\\n\\t\\t\\tif (request_resource(parent, res) == 0)\\n\\t\\t\\t\\treserved = x+1;\\n\\t\\t}\\n\\t}\\n\\treturn 1;\\n}\\n__setup(\"reserve=\", reserve_setup);\\n\\n/*\\n * Check if the requested addr and size spans more than any slot in the\\n * iomem resource tree.\\n */\\nint iomem_map_sanity_check(resource_size_t addr, unsigned long size)\\n{\\n\\tstruct resource *p = &iomem_resource;\\n\\tint err = 0;\\n\\tloff_t l;\\n\\n\\tread_lock(&resource_lock);\\n\\tfor (p = p->child; p ; p = r_next(NULL, p, &l)) {\\n\\t\\t/*\\n\\t\\t * We can probably skip the resources without\\n\\t\\t * IORESOURCE_IO attribute?\\n\\t\\t */\\n\\t\\tif (p->start >= addr + size)\\n\\t\\t\\tcontinue;\\n\\t\\tif (p->end < addr)\\n\\t\\t\\tcontinue;\\n\\t\\tif (PFN_DOWN(p->start) <= PFN_DOWN(addr) &&\\n\\t\\t    PFN_DOWN(p->end) >= PFN_DOWN(addr + size - 1))\\n\\t\\t\\tcontinue;\\n\\t\\t/*\\n\\t\\t * if a resource is \"BUSY\", it\\'s not a hardware resource\\n\\t\\t * but a driver mapping of such a resource; we don\\'t want\\n\\t\\t * to warn for those; some drivers legitimately map only\\n\\t\\t * partial hardware resources. (example: vesafb)\\n\\t\\t */\\n\\t\\tif (p->flags & IORESOURCE_BUSY)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tprintk(KERN_WARNING \"resource sanity check: requesting [mem %#010llx-%#010llx], which spans more than %s %pR\\\\n\",\\n\\t\\t       (unsigned long long)addr,\\n\\t\\t       (unsigned long long)(addr + size - 1),\\n\\t\\t       p->name, p);\\n\\t\\terr = -1;\\n\\t\\tbreak;\\n\\t}\\n\\tread_unlock(&resource_lock);\\n\\n\\treturn err;\\n}\\n\\n#ifdef CONFIG_STRICT_DEVMEM\\nstatic int strict_iomem_checks = 1;\\n#else\\nstatic int strict_iomem_checks;\\n#endif\\n\\n/*\\n * check if an address is reserved in the iomem resource tree\\n * returns true if reserved, false if not reserved.\\n */\\nbool iomem_is_exclusive(u64 addr)\\n{\\n\\tstruct resource *p = &iomem_resource;\\n\\tbool err = false;\\n\\tloff_t l;\\n\\tint size = PAGE_SIZE;\\n\\n\\tif (!strict_iomem_checks)\\n\\t\\treturn false;\\n\\n\\taddr = addr & PAGE_MASK;\\n\\n\\tread_lock(&resource_lock);\\n\\tfor (p = p->child; p ; p = r_next(NULL, p, &l)) {\\n\\t\\t/*\\n\\t\\t * We can probably skip the resources without\\n\\t\\t * IORESOURCE_IO attribute?\\n\\t\\t */\\n\\t\\tif (p->start >= addr + size)\\n\\t\\t\\tbreak;\\n\\t\\tif (p->end < addr)\\n\\t\\t\\tcontinue;\\n\\t\\t/*\\n\\t\\t * A resource is exclusive if IORESOURCE_EXCLUSIVE is set\\n\\t\\t * or CONFIG_IO_STRICT_DEVMEM is enabled and the\\n\\t\\t * resource is busy.\\n\\t\\t */\\n\\t\\tif ((p->flags & IORESOURCE_BUSY) == 0)\\n\\t\\t\\tcontinue;\\n\\t\\tif (IS_ENABLED(CONFIG_IO_STRICT_DEVMEM)\\n\\t\\t\\t\\t|| p->flags & IORESOURCE_EXCLUSIVE) {\\n\\t\\t\\terr = true;\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\tread_unlock(&resource_lock);\\n\\n\\treturn err;\\n}\\n\\nstruct resource_entry *resource_list_create_entry(struct resource *res,\\n\\t\\t\\t\\t\\t\\t  size_t extra_size)\\n{\\n\\tstruct resource_entry *entry;\\n\\n\\tentry = kzalloc(sizeof(*entry) + extra_size, GFP_KERNEL);\\n\\tif (entry) {\\n\\t\\tINIT_LIST_HEAD(&entry->node);\\n\\t\\tentry->res = res ? res : &entry->__res;\\n\\t}\\n\\n\\treturn entry;\\n}\\nEXPORT_SYMBOL(resource_list_create_entry);\\n\\nvoid resource_list_free(struct list_head *head)\\n{\\n\\tstruct resource_entry *entry, *tmp;\\n\\n\\tlist_for_each_entry_safe(entry, tmp, head, node)\\n\\t\\tresource_list_destroy_entry(entry);\\n}\\nEXPORT_SYMBOL(resource_list_free);\\n\\nstatic int __init strict_iomem(char *str)\\n{\\n\\tif (strstr(str, \"relaxed\"))\\n\\t\\tstrict_iomem_checks = 0;\\n\\tif (strstr(str, \"strict\"))\\n\\t\\tstrict_iomem_checks = 1;\\n\\treturn 1;\\n}\\n\\n__setup(\"iomem=\", strict_iomem);\\n\\n/*\\n * kernel/stop_machine.c\\n *\\n * Copyright (C) 2008, 2005\\tIBM Corporation.\\n * Copyright (C) 2008, 2005\\tRusty Russell rusty@rustcorp.com.au\\n * Copyright (C) 2010\\t\\tSUSE Linux Products GmbH\\n * Copyright (C) 2010\\t\\tTejun Heo <tj@kernel.org>\\n *\\n * This file is released under the GPLv2 and any later version.\\n */\\n#include <linux/completion.h>\\n#include <linux/cpu.h>\\n#include <linux/init.h>\\n#include <linux/kthread.h>\\n#include <linux/export.h>\\n#include <linux/percpu.h>\\n#include <linux/sched.h>\\n#include <linux/stop_machine.h>\\n#include <linux/interrupt.h>\\n#include <linux/kallsyms.h>\\n#include <linux/smpboot.h>\\n#include <linux/atomic.h>\\n#include <linux/nmi.h>\\n#include <linux/sched/wake_q.h>\\n\\n/*\\n * Structure to determine completion condition and record errors.  May\\n * be shared by works on different cpus.\\n */\\nstruct cpu_stop_done {\\n\\tatomic_t\\t\\tnr_todo;\\t/* nr left to execute */\\n\\tint\\t\\t\\tret;\\t\\t/* collected return value */\\n\\tstruct completion\\tcompletion;\\t/* fired if nr_todo reaches 0 */\\n};\\n\\n/* the actual stopper, one per every possible cpu, enabled on online cpus */\\nstruct cpu_stopper {\\n\\tstruct task_struct\\t*thread;\\n\\n\\traw_spinlock_t\\t\\tlock;\\n\\tbool\\t\\t\\tenabled;\\t/* is this stopper enabled? */\\n\\tstruct list_head\\tworks;\\t\\t/* list of pending works */\\n\\n\\tstruct cpu_stop_work\\tstop_work;\\t/* for stop_cpus */\\n};\\n\\nstatic DEFINE_PER_CPU(struct cpu_stopper, cpu_stopper);\\nstatic bool stop_machine_initialized = false;\\n\\n/* static data for stop_cpus */\\nstatic DEFINE_MUTEX(stop_cpus_mutex);\\nstatic bool stop_cpus_in_progress;\\n\\nstatic void cpu_stop_init_done(struct cpu_stop_done *done, unsigned int nr_todo)\\n{\\n\\tmemset(done, 0, sizeof(*done));\\n\\tatomic_set(&done->nr_todo, nr_todo);\\n\\tinit_completion(&done->completion);\\n}\\n\\n/* signal completion unless @done is NULL */\\nstatic void cpu_stop_signal_done(struct cpu_stop_done *done)\\n{\\n\\tif (atomic_dec_and_test(&done->nr_todo))\\n\\t\\tcomplete(&done->completion);\\n}\\n\\nstatic void __cpu_stop_queue_work(struct cpu_stopper *stopper,\\n\\t\\t\\t\\t\\tstruct cpu_stop_work *work,\\n\\t\\t\\t\\t\\tstruct wake_q_head *wakeq)\\n{\\n\\tlist_add_tail(&work->list, &stopper->works);\\n\\twake_q_add(wakeq, stopper->thread);\\n}\\n\\n/* queue @work to @stopper.  if offline, @work is completed immediately */\\nstatic bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work)\\n{\\n\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\tDEFINE_WAKE_Q(wakeq);\\n\\tunsigned long flags;\\n\\tbool enabled;\\n\\n\\tpreempt_disable();\\n\\traw_spin_lock_irqsave(&stopper->lock, flags);\\n\\tenabled = stopper->enabled;\\n\\tif (enabled)\\n\\t\\t__cpu_stop_queue_work(stopper, work, &wakeq);\\n\\telse if (work->done)\\n\\t\\tcpu_stop_signal_done(work->done);\\n\\traw_spin_unlock_irqrestore(&stopper->lock, flags);\\n\\n\\twake_up_q(&wakeq);\\n\\tpreempt_enable();\\n\\n\\treturn enabled;\\n}\\n\\n/**\\n * stop_one_cpu - stop a cpu\\n * @cpu: cpu to stop\\n * @fn: function to execute\\n * @arg: argument to @fn\\n *\\n * Execute @fn(@arg) on @cpu.  @fn is run in a process context with\\n * the highest priority preempting any task on the cpu and\\n * monopolizing it.  This function returns after the execution is\\n * complete.\\n *\\n * This function doesn\\'t guarantee @cpu stays online till @fn\\n * completes.  If @cpu goes down in the middle, execution may happen\\n * partially or fully on different cpus.  @fn should either be ready\\n * for that or the caller should ensure that @cpu stays online until\\n * this function completes.\\n *\\n * CONTEXT:\\n * Might sleep.\\n *\\n * RETURNS:\\n * -ENOENT if @fn(@arg) was not executed because @cpu was offline;\\n * otherwise, the return value of @fn.\\n */\\nint stop_one_cpu(unsigned int cpu, cpu_stop_fn_t fn, void *arg)\\n{\\n\\tstruct cpu_stop_done done;\\n\\tstruct cpu_stop_work work = { .fn = fn, .arg = arg, .done = &done };\\n\\n\\tcpu_stop_init_done(&done, 1);\\n\\tif (!cpu_stop_queue_work(cpu, &work))\\n\\t\\treturn -ENOENT;\\n\\t/*\\n\\t * In case @cpu == smp_proccessor_id() we can avoid a sleep+wakeup\\n\\t * cycle by doing a preemption:\\n\\t */\\n\\tcond_resched();\\n\\twait_for_completion(&done.completion);\\n\\treturn done.ret;\\n}\\n\\n/* This controls the threads on each CPU. */\\nenum multi_stop_state {\\n\\t/* Dummy starting state for thread. */\\n\\tMULTI_STOP_NONE,\\n\\t/* Awaiting everyone to be scheduled. */\\n\\tMULTI_STOP_PREPARE,\\n\\t/* Disable interrupts. */\\n\\tMULTI_STOP_DISABLE_IRQ,\\n\\t/* Run the function */\\n\\tMULTI_STOP_RUN,\\n\\t/* Exit */\\n\\tMULTI_STOP_EXIT,\\n};\\n\\nstruct multi_stop_data {\\n\\tcpu_stop_fn_t\\t\\tfn;\\n\\tvoid\\t\\t\\t*data;\\n\\t/* Like num_online_cpus(), but hotplug cpu uses us, so we need this. */\\n\\tunsigned int\\t\\tnum_threads;\\n\\tconst struct cpumask\\t*active_cpus;\\n\\n\\tenum multi_stop_state\\tstate;\\n\\tatomic_t\\t\\tthread_ack;\\n};\\n\\nstatic void set_state(struct multi_stop_data *msdata,\\n\\t\\t      enum multi_stop_state newstate)\\n{\\n\\t/* Reset ack counter. */\\n\\tatomic_set(&msdata->thread_ack, msdata->num_threads);\\n\\tsmp_wmb();\\n\\tmsdata->state = newstate;\\n}\\n\\n/* Last one to ack a state moves to the next state. */\\nstatic void ack_state(struct multi_stop_data *msdata)\\n{\\n\\tif (atomic_dec_and_test(&msdata->thread_ack))\\n\\t\\tset_state(msdata, msdata->state + 1);\\n}\\n\\n/* This is the cpu_stop function which stops the CPU. */\\nstatic int multi_cpu_stop(void *data)\\n{\\n\\tstruct multi_stop_data *msdata = data;\\n\\tenum multi_stop_state curstate = MULTI_STOP_NONE;\\n\\tint cpu = smp_processor_id(), err = 0;\\n\\tunsigned long flags;\\n\\tbool is_active;\\n\\n\\t/*\\n\\t * When called from stop_machine_from_inactive_cpu(), irq might\\n\\t * already be disabled.  Save the state and restore it on exit.\\n\\t */\\n\\tlocal_save_flags(flags);\\n\\n\\tif (!msdata->active_cpus)\\n\\t\\tis_active = cpu == cpumask_first(cpu_online_mask);\\n\\telse\\n\\t\\tis_active = cpumask_test_cpu(cpu, msdata->active_cpus);\\n\\n\\t/* Simple state machine */\\n\\tdo {\\n\\t\\t/* Chill out and ensure we re-read multi_stop_state. */\\n\\t\\tcpu_relax_yield();\\n\\t\\tif (msdata->state != curstate) {\\n\\t\\t\\tcurstate = msdata->state;\\n\\t\\t\\tswitch (curstate) {\\n\\t\\t\\tcase MULTI_STOP_DISABLE_IRQ:\\n\\t\\t\\t\\tlocal_irq_disable();\\n\\t\\t\\t\\thard_irq_disable();\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tcase MULTI_STOP_RUN:\\n\\t\\t\\t\\tif (is_active)\\n\\t\\t\\t\\t\\terr = msdata->fn(msdata->data);\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tdefault:\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t\\tack_state(msdata);\\n\\t\\t} else if (curstate > MULTI_STOP_PREPARE) {\\n\\t\\t\\t/*\\n\\t\\t\\t * At this stage all other CPUs we depend on must spin\\n\\t\\t\\t * in the same loop. Any reason for hard-lockup should\\n\\t\\t\\t * be detected and reported on their side.\\n\\t\\t\\t */\\n\\t\\t\\ttouch_nmi_watchdog();\\n\\t\\t}\\n\\t} while (curstate != MULTI_STOP_EXIT);\\n\\n\\tlocal_irq_restore(flags);\\n\\treturn err;\\n}\\n\\nstatic int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,\\n\\t\\t\\t\\t    int cpu2, struct cpu_stop_work *work2)\\n{\\n\\tstruct cpu_stopper *stopper1 = per_cpu_ptr(&cpu_stopper, cpu1);\\n\\tstruct cpu_stopper *stopper2 = per_cpu_ptr(&cpu_stopper, cpu2);\\n\\tDEFINE_WAKE_Q(wakeq);\\n\\tint err;\\n\\nretry:\\n\\t/*\\n\\t * The waking up of stopper threads has to happen in the same\\n\\t * scheduling context as the queueing.  Otherwise, there is a\\n\\t * possibility of one of the above stoppers being woken up by another\\n\\t * CPU, and preempting us. This will cause us to not wake up the other\\n\\t * stopper forever.\\n\\t */\\n\\tpreempt_disable();\\n\\traw_spin_lock_irq(&stopper1->lock);\\n\\traw_spin_lock_nested(&stopper2->lock, SINGLE_DEPTH_NESTING);\\n\\n\\tif (!stopper1->enabled || !stopper2->enabled) {\\n\\t\\terr = -ENOENT;\\n\\t\\tgoto unlock;\\n\\t}\\n\\n\\t/*\\n\\t * Ensure that if we race with __stop_cpus() the stoppers won\\'t get\\n\\t * queued up in reverse order leading to system deadlock.\\n\\t *\\n\\t * We can\\'t miss stop_cpus_in_progress if queue_stop_cpus_work() has\\n\\t * queued a work on cpu1 but not on cpu2, we hold both locks.\\n\\t *\\n\\t * It can be falsely true but it is safe to spin until it is cleared,\\n\\t * queue_stop_cpus_work() does everything under preempt_disable().\\n\\t */\\n\\tif (unlikely(stop_cpus_in_progress)) {\\n\\t\\terr = -EDEADLK;\\n\\t\\tgoto unlock;\\n\\t}\\n\\n\\terr = 0;\\n\\t__cpu_stop_queue_work(stopper1, work1, &wakeq);\\n\\t__cpu_stop_queue_work(stopper2, work2, &wakeq);\\n\\nunlock:\\n\\traw_spin_unlock(&stopper2->lock);\\n\\traw_spin_unlock_irq(&stopper1->lock);\\n\\n\\tif (unlikely(err == -EDEADLK)) {\\n\\t\\tpreempt_enable();\\n\\n\\t\\twhile (stop_cpus_in_progress)\\n\\t\\t\\tcpu_relax();\\n\\n\\t\\tgoto retry;\\n\\t}\\n\\n\\twake_up_q(&wakeq);\\n\\tpreempt_enable();\\n\\n\\treturn err;\\n}\\n/**\\n * stop_two_cpus - stops two cpus\\n * @cpu1: the cpu to stop\\n * @cpu2: the other cpu to stop\\n * @fn: function to execute\\n * @arg: argument to @fn\\n *\\n * Stops both the current and specified CPU and runs @fn on one of them.\\n *\\n * returns when both are completed.\\n */\\nint stop_two_cpus(unsigned int cpu1, unsigned int cpu2, cpu_stop_fn_t fn, void *arg)\\n{\\n\\tstruct cpu_stop_done done;\\n\\tstruct cpu_stop_work work1, work2;\\n\\tstruct multi_stop_data msdata;\\n\\n\\tmsdata = (struct multi_stop_data){\\n\\t\\t.fn = fn,\\n\\t\\t.data = arg,\\n\\t\\t.num_threads = 2,\\n\\t\\t.active_cpus = cpumask_of(cpu1),\\n\\t};\\n\\n\\twork1 = work2 = (struct cpu_stop_work){\\n\\t\\t.fn = multi_cpu_stop,\\n\\t\\t.arg = &msdata,\\n\\t\\t.done = &done\\n\\t};\\n\\n\\tcpu_stop_init_done(&done, 2);\\n\\tset_state(&msdata, MULTI_STOP_PREPARE);\\n\\n\\tif (cpu1 > cpu2)\\n\\t\\tswap(cpu1, cpu2);\\n\\tif (cpu_stop_queue_two_works(cpu1, &work1, cpu2, &work2))\\n\\t\\treturn -ENOENT;\\n\\n\\twait_for_completion(&done.completion);\\n\\treturn done.ret;\\n}\\n\\n/**\\n * stop_one_cpu_nowait - stop a cpu but don\\'t wait for completion\\n * @cpu: cpu to stop\\n * @fn: function to execute\\n * @arg: argument to @fn\\n * @work_buf: pointer to cpu_stop_work structure\\n *\\n * Similar to stop_one_cpu() but doesn\\'t wait for completion.  The\\n * caller is responsible for ensuring @work_buf is currently unused\\n * and will remain untouched until stopper starts executing @fn.\\n *\\n * CONTEXT:\\n * Don\\'t care.\\n *\\n * RETURNS:\\n * true if cpu_stop_work was queued successfully and @fn will be called,\\n * false otherwise.\\n */\\nbool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,\\n\\t\\t\\tstruct cpu_stop_work *work_buf)\\n{\\n\\t*work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, };\\n\\treturn cpu_stop_queue_work(cpu, work_buf);\\n}\\n\\nstatic bool queue_stop_cpus_work(const struct cpumask *cpumask,\\n\\t\\t\\t\\t cpu_stop_fn_t fn, void *arg,\\n\\t\\t\\t\\t struct cpu_stop_done *done)\\n{\\n\\tstruct cpu_stop_work *work;\\n\\tunsigned int cpu;\\n\\tbool queued = false;\\n\\n\\t/*\\n\\t * Disable preemption while queueing to avoid getting\\n\\t * preempted by a stopper which might wait for other stoppers\\n\\t * to enter @fn which can lead to deadlock.\\n\\t */\\n\\tpreempt_disable();\\n\\tstop_cpus_in_progress = true;\\n\\tfor_each_cpu(cpu, cpumask) {\\n\\t\\twork = &per_cpu(cpu_stopper.stop_work, cpu);\\n\\t\\twork->fn = fn;\\n\\t\\twork->arg = arg;\\n\\t\\twork->done = done;\\n\\t\\tif (cpu_stop_queue_work(cpu, work))\\n\\t\\t\\tqueued = true;\\n\\t}\\n\\tstop_cpus_in_progress = false;\\n\\tpreempt_enable();\\n\\n\\treturn queued;\\n}\\n\\nstatic int __stop_cpus(const struct cpumask *cpumask,\\n\\t\\t       cpu_stop_fn_t fn, void *arg)\\n{\\n\\tstruct cpu_stop_done done;\\n\\n\\tcpu_stop_init_done(&done, cpumask_weight(cpumask));\\n\\tif (!queue_stop_cpus_work(cpumask, fn, arg, &done))\\n\\t\\treturn -ENOENT;\\n\\twait_for_completion(&done.completion);\\n\\treturn done.ret;\\n}\\n\\n/**\\n * stop_cpus - stop multiple cpus\\n * @cpumask: cpus to stop\\n * @fn: function to execute\\n * @arg: argument to @fn\\n *\\n * Execute @fn(@arg) on online cpus in @cpumask.  On each target cpu,\\n * @fn is run in a process context with the highest priority\\n * preempting any task on the cpu and monopolizing it.  This function\\n * returns after all executions are complete.\\n *\\n * This function doesn\\'t guarantee the cpus in @cpumask stay online\\n * till @fn completes.  If some cpus go down in the middle, execution\\n * on the cpu may happen partially or fully on different cpus.  @fn\\n * should either be ready for that or the caller should ensure that\\n * the cpus stay online until this function completes.\\n *\\n * All stop_cpus() calls are serialized making it safe for @fn to wait\\n * for all cpus to start executing it.\\n *\\n * CONTEXT:\\n * Might sleep.\\n *\\n * RETURNS:\\n * -ENOENT if @fn(@arg) was not executed at all because all cpus in\\n * @cpumask were offline; otherwise, 0 if all executions of @fn\\n * returned 0, any non zero return value if any returned non zero.\\n */\\nint stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)\\n{\\n\\tint ret;\\n\\n\\t/* static works are used, process one request at a time */\\n\\tmutex_lock(&stop_cpus_mutex);\\n\\tret = __stop_cpus(cpumask, fn, arg);\\n\\tmutex_unlock(&stop_cpus_mutex);\\n\\treturn ret;\\n}\\n\\n/**\\n * try_stop_cpus - try to stop multiple cpus\\n * @cpumask: cpus to stop\\n * @fn: function to execute\\n * @arg: argument to @fn\\n *\\n * Identical to stop_cpus() except that it fails with -EAGAIN if\\n * someone else is already using the facility.\\n *\\n * CONTEXT:\\n * Might sleep.\\n *\\n * RETURNS:\\n * -EAGAIN if someone else is already stopping cpus, -ENOENT if\\n * @fn(@arg) was not executed at all because all cpus in @cpumask were\\n * offline; otherwise, 0 if all executions of @fn returned 0, any non\\n * zero return value if any returned non zero.\\n */\\nint try_stop_cpus(const struct cpumask *cpumask, cpu_stop_fn_t fn, void *arg)\\n{\\n\\tint ret;\\n\\n\\t/* static works are used, process one request at a time */\\n\\tif (!mutex_trylock(&stop_cpus_mutex))\\n\\t\\treturn -EAGAIN;\\n\\tret = __stop_cpus(cpumask, fn, arg);\\n\\tmutex_unlock(&stop_cpus_mutex);\\n\\treturn ret;\\n}\\n\\nstatic int cpu_stop_should_run(unsigned int cpu)\\n{\\n\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\tunsigned long flags;\\n\\tint run;\\n\\n\\traw_spin_lock_irqsave(&stopper->lock, flags);\\n\\trun = !list_empty(&stopper->works);\\n\\traw_spin_unlock_irqrestore(&stopper->lock, flags);\\n\\treturn run;\\n}\\n\\nstatic void cpu_stopper_thread(unsigned int cpu)\\n{\\n\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\tstruct cpu_stop_work *work;\\n\\nrepeat:\\n\\twork = NULL;\\n\\traw_spin_lock_irq(&stopper->lock);\\n\\tif (!list_empty(&stopper->works)) {\\n\\t\\twork = list_first_entry(&stopper->works,\\n\\t\\t\\t\\t\\tstruct cpu_stop_work, list);\\n\\t\\tlist_del_init(&work->list);\\n\\t}\\n\\traw_spin_unlock_irq(&stopper->lock);\\n\\n\\tif (work) {\\n\\t\\tcpu_stop_fn_t fn = work->fn;\\n\\t\\tvoid *arg = work->arg;\\n\\t\\tstruct cpu_stop_done *done = work->done;\\n\\t\\tint ret;\\n\\n\\t\\t/* cpu stop callbacks must not sleep, make in_atomic() == T */\\n\\t\\tpreempt_count_inc();\\n\\t\\tret = fn(arg);\\n\\t\\tif (done) {\\n\\t\\t\\tif (ret)\\n\\t\\t\\t\\tdone->ret = ret;\\n\\t\\t\\tcpu_stop_signal_done(done);\\n\\t\\t}\\n\\t\\tpreempt_count_dec();\\n\\t\\tWARN_ONCE(preempt_count(),\\n\\t\\t\\t  \"cpu_stop: %pf(%p) leaked preempt count\\\\n\", fn, arg);\\n\\t\\tgoto repeat;\\n\\t}\\n}\\n\\nvoid stop_machine_park(int cpu)\\n{\\n\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\t/*\\n\\t * Lockless. cpu_stopper_thread() will take stopper->lock and flush\\n\\t * the pending works before it parks, until then it is fine to queue\\n\\t * the new works.\\n\\t */\\n\\tstopper->enabled = false;\\n\\tkthread_park(stopper->thread);\\n}\\n\\nextern void sched_set_stop_task(int cpu, struct task_struct *stop);\\n\\nstatic void cpu_stop_create(unsigned int cpu)\\n{\\n\\tsched_set_stop_task(cpu, per_cpu(cpu_stopper.thread, cpu));\\n}\\n\\nstatic void cpu_stop_park(unsigned int cpu)\\n{\\n\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\n\\tWARN_ON(!list_empty(&stopper->works));\\n}\\n\\nvoid stop_machine_unpark(int cpu)\\n{\\n\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\n\\tstopper->enabled = true;\\n\\tkthread_unpark(stopper->thread);\\n}\\n\\nstatic struct smp_hotplug_thread cpu_stop_threads = {\\n\\t.store\\t\\t\\t= &cpu_stopper.thread,\\n\\t.thread_should_run\\t= cpu_stop_should_run,\\n\\t.thread_fn\\t\\t= cpu_stopper_thread,\\n\\t.thread_comm\\t\\t= \"migration/%u\",\\n\\t.create\\t\\t\\t= cpu_stop_create,\\n\\t.park\\t\\t\\t= cpu_stop_park,\\n\\t.selfparking\\t\\t= true,\\n};\\n\\nstatic int __init cpu_stop_init(void)\\n{\\n\\tunsigned int cpu;\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tstruct cpu_stopper *stopper = &per_cpu(cpu_stopper, cpu);\\n\\n\\t\\traw_spin_lock_init(&stopper->lock);\\n\\t\\tINIT_LIST_HEAD(&stopper->works);\\n\\t}\\n\\n\\tBUG_ON(smpboot_register_percpu_thread(&cpu_stop_threads));\\n\\tstop_machine_unpark(raw_smp_processor_id());\\n\\tstop_machine_initialized = true;\\n\\treturn 0;\\n}\\nearly_initcall(cpu_stop_init);\\n\\nint stop_machine_cpuslocked(cpu_stop_fn_t fn, void *data,\\n\\t\\t\\t    const struct cpumask *cpus)\\n{\\n\\tstruct multi_stop_data msdata = {\\n\\t\\t.fn = fn,\\n\\t\\t.data = data,\\n\\t\\t.num_threads = num_online_cpus(),\\n\\t\\t.active_cpus = cpus,\\n\\t};\\n\\n\\tlockdep_assert_cpus_held();\\n\\n\\tif (!stop_machine_initialized) {\\n\\t\\t/*\\n\\t\\t * Handle the case where stop_machine() is called\\n\\t\\t * early in boot before stop_machine() has been\\n\\t\\t * initialized.\\n\\t\\t */\\n\\t\\tunsigned long flags;\\n\\t\\tint ret;\\n\\n\\t\\tWARN_ON_ONCE(msdata.num_threads != 1);\\n\\n\\t\\tlocal_irq_save(flags);\\n\\t\\thard_irq_disable();\\n\\t\\tret = (*fn)(data);\\n\\t\\tlocal_irq_restore(flags);\\n\\n\\t\\treturn ret;\\n\\t}\\n\\n\\t/* Set the initial state and stop all online cpus. */\\n\\tset_state(&msdata, MULTI_STOP_PREPARE);\\n\\treturn stop_cpus(cpu_online_mask, multi_cpu_stop, &msdata);\\n}\\n\\nint stop_machine(cpu_stop_fn_t fn, void *data, const struct cpumask *cpus)\\n{\\n\\tint ret;\\n\\n\\t/* No CPUs can come up or down during this. */\\n\\tcpus_read_lock();\\n\\tret = stop_machine_cpuslocked(fn, data, cpus);\\n\\tcpus_read_unlock();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(stop_machine);\\n\\n/**\\n * stop_machine_from_inactive_cpu - stop_machine() from inactive CPU\\n * @fn: the function to run\\n * @data: the data ptr for the @fn()\\n * @cpus: the cpus to run the @fn() on (NULL = any online cpu)\\n *\\n * This is identical to stop_machine() but can be called from a CPU which\\n * is not active.  The local CPU is in the process of hotplug (so no other\\n * CPU hotplug can start) and not marked active and doesn\\'t have enough\\n * context to sleep.\\n *\\n * This function provides stop_machine() functionality for such state by\\n * using busy-wait for synchronization and executing @fn directly for local\\n * CPU.\\n *\\n * CONTEXT:\\n * Local CPU is inactive.  Temporarily stops all active CPUs.\\n *\\n * RETURNS:\\n * 0 if all executions of @fn returned 0, any non zero return value if any\\n * returned non zero.\\n */\\nint stop_machine_from_inactive_cpu(cpu_stop_fn_t fn, void *data,\\n\\t\\t\\t\\t  const struct cpumask *cpus)\\n{\\n\\tstruct multi_stop_data msdata = { .fn = fn, .data = data,\\n\\t\\t\\t\\t\\t    .active_cpus = cpus };\\n\\tstruct cpu_stop_done done;\\n\\tint ret;\\n\\n\\t/* Local CPU must be inactive and CPU hotplug in progress. */\\n\\tBUG_ON(cpu_active(raw_smp_processor_id()));\\n\\tmsdata.num_threads = num_active_cpus() + 1;\\t/* +1 for local */\\n\\n\\t/* No proper task established and can\\'t sleep - busy wait for lock. */\\n\\twhile (!mutex_trylock(&stop_cpus_mutex))\\n\\t\\tcpu_relax();\\n\\n\\t/* Schedule work on other CPUs and execute directly for local CPU */\\n\\tset_state(&msdata, MULTI_STOP_PREPARE);\\n\\tcpu_stop_init_done(&done, num_active_cpus());\\n\\tqueue_stop_cpus_work(cpu_active_mask, multi_cpu_stop, &msdata,\\n\\t\\t\\t     &done);\\n\\tret = multi_cpu_stop(&msdata);\\n\\n\\t/* Busy wait for completion. */\\n\\twhile (!completion_done(&done.completion))\\n\\t\\tcpu_relax();\\n\\n\\tmutex_unlock(&stop_cpus_mutex);\\n\\treturn ret ?: done.ret;\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n#include \"audit.h\"\\n#include <linux/fsnotify_backend.h>\\n#include <linux/namei.h>\\n#include <linux/mount.h>\\n#include <linux/kthread.h>\\n#include <linux/refcount.h>\\n#include <linux/slab.h>\\n\\nstruct audit_tree;\\nstruct audit_chunk;\\n\\nstruct audit_tree {\\n\\trefcount_t count;\\n\\tint goner;\\n\\tstruct audit_chunk *root;\\n\\tstruct list_head chunks;\\n\\tstruct list_head rules;\\n\\tstruct list_head list;\\n\\tstruct list_head same_root;\\n\\tstruct rcu_head head;\\n\\tchar pathname[];\\n};\\n\\nstruct audit_chunk {\\n\\tstruct list_head hash;\\n\\tunsigned long key;\\n\\tstruct fsnotify_mark *mark;\\n\\tstruct list_head trees;\\t\\t/* with root here */\\n\\tint count;\\n\\tatomic_long_t refs;\\n\\tstruct rcu_head head;\\n\\tstruct node {\\n\\t\\tstruct list_head list;\\n\\t\\tstruct audit_tree *owner;\\n\\t\\tunsigned index;\\t\\t/* index; upper bit indicates \\'will prune\\' */\\n\\t} owners[];\\n};\\n\\nstruct audit_tree_mark {\\n\\tstruct fsnotify_mark mark;\\n\\tstruct audit_chunk *chunk;\\n};\\n\\nstatic LIST_HEAD(tree_list);\\nstatic LIST_HEAD(prune_list);\\nstatic struct task_struct *prune_thread;\\n\\n/*\\n * One struct chunk is attached to each inode of interest through\\n * audit_tree_mark (fsnotify mark). We replace struct chunk on tagging /\\n * untagging, the mark is stable as long as there is chunk attached. The\\n * association between mark and chunk is protected by hash_lock and\\n * audit_tree_group->mark_mutex. Thus as long as we hold\\n * audit_tree_group->mark_mutex and check that the mark is alive by\\n * FSNOTIFY_MARK_FLAG_ATTACHED flag check, we are sure the mark points to\\n * the current chunk.\\n *\\n * Rules have pointer to struct audit_tree.\\n * Rules have struct list_head rlist forming a list of rules over\\n * the same tree.\\n * References to struct chunk are collected at audit_inode{,_child}()\\n * time and used in AUDIT_TREE rule matching.\\n * These references are dropped at the same time we are calling\\n * audit_free_names(), etc.\\n *\\n * Cyclic lists galore:\\n * tree.chunks anchors chunk.owners[].list\\t\\t\\thash_lock\\n * tree.rules anchors rule.rlist\\t\\t\\t\\taudit_filter_mutex\\n * chunk.trees anchors tree.same_root\\t\\t\\t\\thash_lock\\n * chunk.hash is a hash with middle bits of watch.inode as\\n * a hash function.\\t\\t\\t\\t\\t\\tRCU, hash_lock\\n *\\n * tree is refcounted; one reference for \"some rules on rules_list refer to\\n * it\", one for each chunk with pointer to it.\\n *\\n * chunk is refcounted by embedded .refs. Mark associated with the chunk holds\\n * one chunk reference. This reference is dropped either when a mark is going\\n * to be freed (corresponding inode goes away) or when chunk attached to the\\n * mark gets replaced. This reference must be dropped using\\n * audit_mark_put_chunk() to make sure the reference is dropped only after RCU\\n * grace period as it protects RCU readers of the hash table.\\n *\\n * node.index allows to get from node.list to containing chunk.\\n * MSB of that sucker is stolen to mark taggings that we might have to\\n * revert - several operations have very unpleasant cleanup logics and\\n * that makes a difference.  Some.\\n */\\n\\nstatic struct fsnotify_group *audit_tree_group;\\nstatic struct kmem_cache *audit_tree_mark_cachep __read_mostly;\\n\\nstatic struct audit_tree *alloc_tree(const char *s)\\n{\\n\\tstruct audit_tree *tree;\\n\\n\\ttree = kmalloc(sizeof(struct audit_tree) + strlen(s) + 1, GFP_KERNEL);\\n\\tif (tree) {\\n\\t\\trefcount_set(&tree->count, 1);\\n\\t\\ttree->goner = 0;\\n\\t\\tINIT_LIST_HEAD(&tree->chunks);\\n\\t\\tINIT_LIST_HEAD(&tree->rules);\\n\\t\\tINIT_LIST_HEAD(&tree->list);\\n\\t\\tINIT_LIST_HEAD(&tree->same_root);\\n\\t\\ttree->root = NULL;\\n\\t\\tstrcpy(tree->pathname, s);\\n\\t}\\n\\treturn tree;\\n}\\n\\nstatic inline void get_tree(struct audit_tree *tree)\\n{\\n\\trefcount_inc(&tree->count);\\n}\\n\\nstatic inline void put_tree(struct audit_tree *tree)\\n{\\n\\tif (refcount_dec_and_test(&tree->count))\\n\\t\\tkfree_rcu(tree, head);\\n}\\n\\n/* to avoid bringing the entire thing in audit.h */\\nconst char *audit_tree_path(struct audit_tree *tree)\\n{\\n\\treturn tree->pathname;\\n}\\n\\nstatic void free_chunk(struct audit_chunk *chunk)\\n{\\n\\tint i;\\n\\n\\tfor (i = 0; i < chunk->count; i++) {\\n\\t\\tif (chunk->owners[i].owner)\\n\\t\\t\\tput_tree(chunk->owners[i].owner);\\n\\t}\\n\\tkfree(chunk);\\n}\\n\\nvoid audit_put_chunk(struct audit_chunk *chunk)\\n{\\n\\tif (atomic_long_dec_and_test(&chunk->refs))\\n\\t\\tfree_chunk(chunk);\\n}\\n\\nstatic void __put_chunk(struct rcu_head *rcu)\\n{\\n\\tstruct audit_chunk *chunk = container_of(rcu, struct audit_chunk, head);\\n\\taudit_put_chunk(chunk);\\n}\\n\\n/*\\n * Drop reference to the chunk that was held by the mark. This is the reference\\n * that gets dropped after we\\'ve removed the chunk from the hash table and we\\n * use it to make sure chunk cannot be freed before RCU grace period expires.\\n */\\nstatic void audit_mark_put_chunk(struct audit_chunk *chunk)\\n{\\n\\tcall_rcu(&chunk->head, __put_chunk);\\n}\\n\\nstatic inline struct audit_tree_mark *audit_mark(struct fsnotify_mark *mark)\\n{\\n\\treturn container_of(mark, struct audit_tree_mark, mark);\\n}\\n\\nstatic struct audit_chunk *mark_chunk(struct fsnotify_mark *mark)\\n{\\n\\treturn audit_mark(mark)->chunk;\\n}\\n\\nstatic void audit_tree_destroy_watch(struct fsnotify_mark *mark)\\n{\\n\\tkmem_cache_free(audit_tree_mark_cachep, audit_mark(mark));\\n}\\n\\nstatic struct fsnotify_mark *alloc_mark(void)\\n{\\n\\tstruct audit_tree_mark *amark;\\n\\n\\tamark = kmem_cache_zalloc(audit_tree_mark_cachep, GFP_KERNEL);\\n\\tif (!amark)\\n\\t\\treturn NULL;\\n\\tfsnotify_init_mark(&amark->mark, audit_tree_group);\\n\\tamark->mark.mask = FS_IN_IGNORED;\\n\\treturn &amark->mark;\\n}\\n\\nstatic struct audit_chunk *alloc_chunk(int count)\\n{\\n\\tstruct audit_chunk *chunk;\\n\\tsize_t size;\\n\\tint i;\\n\\n\\tsize = offsetof(struct audit_chunk, owners) + count * sizeof(struct node);\\n\\tchunk = kzalloc(size, GFP_KERNEL);\\n\\tif (!chunk)\\n\\t\\treturn NULL;\\n\\n\\tINIT_LIST_HEAD(&chunk->hash);\\n\\tINIT_LIST_HEAD(&chunk->trees);\\n\\tchunk->count = count;\\n\\tatomic_long_set(&chunk->refs, 1);\\n\\tfor (i = 0; i < count; i++) {\\n\\t\\tINIT_LIST_HEAD(&chunk->owners[i].list);\\n\\t\\tchunk->owners[i].index = i;\\n\\t}\\n\\treturn chunk;\\n}\\n\\nenum {HASH_SIZE = 128};\\nstatic struct list_head chunk_hash_heads[HASH_SIZE];\\nstatic __cacheline_aligned_in_smp DEFINE_SPINLOCK(hash_lock);\\n\\n/* Function to return search key in our hash from inode. */\\nstatic unsigned long inode_to_key(const struct inode *inode)\\n{\\n\\t/* Use address pointed to by connector->obj as the key */\\n\\treturn (unsigned long)&inode->i_fsnotify_marks;\\n}\\n\\nstatic inline struct list_head *chunk_hash(unsigned long key)\\n{\\n\\tunsigned long n = key / L1_CACHE_BYTES;\\n\\treturn chunk_hash_heads + n % HASH_SIZE;\\n}\\n\\n/* hash_lock & mark->group->mark_mutex is held by caller */\\nstatic void insert_hash(struct audit_chunk *chunk)\\n{\\n\\tstruct list_head *list;\\n\\n\\t/*\\n\\t * Make sure chunk is fully initialized before making it visible in the\\n\\t * hash. Pairs with a data dependency barrier in READ_ONCE() in\\n\\t * audit_tree_lookup().\\n\\t */\\n\\tsmp_wmb();\\n\\tWARN_ON_ONCE(!chunk->key);\\n\\tlist = chunk_hash(chunk->key);\\n\\tlist_add_rcu(&chunk->hash, list);\\n}\\n\\n/* called under rcu_read_lock */\\nstruct audit_chunk *audit_tree_lookup(const struct inode *inode)\\n{\\n\\tunsigned long key = inode_to_key(inode);\\n\\tstruct list_head *list = chunk_hash(key);\\n\\tstruct audit_chunk *p;\\n\\n\\tlist_for_each_entry_rcu(p, list, hash) {\\n\\t\\t/*\\n\\t\\t * We use a data dependency barrier in READ_ONCE() to make sure\\n\\t\\t * the chunk we see is fully initialized.\\n\\t\\t */\\n\\t\\tif (READ_ONCE(p->key) == key) {\\n\\t\\t\\tatomic_long_inc(&p->refs);\\n\\t\\t\\treturn p;\\n\\t\\t}\\n\\t}\\n\\treturn NULL;\\n}\\n\\nbool audit_tree_match(struct audit_chunk *chunk, struct audit_tree *tree)\\n{\\n\\tint n;\\n\\tfor (n = 0; n < chunk->count; n++)\\n\\t\\tif (chunk->owners[n].owner == tree)\\n\\t\\t\\treturn true;\\n\\treturn false;\\n}\\n\\n/* tagging and untagging inodes with trees */\\n\\nstatic struct audit_chunk *find_chunk(struct node *p)\\n{\\n\\tint index = p->index & ~(1U<<31);\\n\\tp -= index;\\n\\treturn container_of(p, struct audit_chunk, owners[0]);\\n}\\n\\nstatic void replace_mark_chunk(struct fsnotify_mark *mark,\\n\\t\\t\\t       struct audit_chunk *chunk)\\n{\\n\\tstruct audit_chunk *old;\\n\\n\\tassert_spin_locked(&hash_lock);\\n\\told = mark_chunk(mark);\\n\\taudit_mark(mark)->chunk = chunk;\\n\\tif (chunk)\\n\\t\\tchunk->mark = mark;\\n\\tif (old)\\n\\t\\told->mark = NULL;\\n}\\n\\nstatic void replace_chunk(struct audit_chunk *new, struct audit_chunk *old)\\n{\\n\\tstruct audit_tree *owner;\\n\\tint i, j;\\n\\n\\tnew->key = old->key;\\n\\tlist_splice_init(&old->trees, &new->trees);\\n\\tlist_for_each_entry(owner, &new->trees, same_root)\\n\\t\\towner->root = new;\\n\\tfor (i = j = 0; j < old->count; i++, j++) {\\n\\t\\tif (!old->owners[j].owner) {\\n\\t\\t\\ti--;\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\towner = old->owners[j].owner;\\n\\t\\tnew->owners[i].owner = owner;\\n\\t\\tnew->owners[i].index = old->owners[j].index - j + i;\\n\\t\\tif (!owner) /* result of earlier fallback */\\n\\t\\t\\tcontinue;\\n\\t\\tget_tree(owner);\\n\\t\\tlist_replace_init(&old->owners[j].list, &new->owners[i].list);\\n\\t}\\n\\treplace_mark_chunk(old->mark, new);\\n\\t/*\\n\\t * Make sure chunk is fully initialized before making it visible in the\\n\\t * hash. Pairs with a data dependency barrier in READ_ONCE() in\\n\\t * audit_tree_lookup().\\n\\t */\\n\\tsmp_wmb();\\n\\tlist_replace_rcu(&old->hash, &new->hash);\\n}\\n\\nstatic void remove_chunk_node(struct audit_chunk *chunk, struct node *p)\\n{\\n\\tstruct audit_tree *owner = p->owner;\\n\\n\\tif (owner->root == chunk) {\\n\\t\\tlist_del_init(&owner->same_root);\\n\\t\\towner->root = NULL;\\n\\t}\\n\\tlist_del_init(&p->list);\\n\\tp->owner = NULL;\\n\\tput_tree(owner);\\n}\\n\\nstatic int chunk_count_trees(struct audit_chunk *chunk)\\n{\\n\\tint i;\\n\\tint ret = 0;\\n\\n\\tfor (i = 0; i < chunk->count; i++)\\n\\t\\tif (chunk->owners[i].owner)\\n\\t\\t\\tret++;\\n\\treturn ret;\\n}\\n\\nstatic void untag_chunk(struct audit_chunk *chunk, struct fsnotify_mark *mark)\\n{\\n\\tstruct audit_chunk *new;\\n\\tint size;\\n\\n\\tmutex_lock(&audit_tree_group->mark_mutex);\\n\\t/*\\n\\t * mark_mutex stabilizes chunk attached to the mark so we can check\\n\\t * whether it didn\\'t change while we\\'ve dropped hash_lock.\\n\\t */\\n\\tif (!(mark->flags & FSNOTIFY_MARK_FLAG_ATTACHED) ||\\n\\t    mark_chunk(mark) != chunk)\\n\\t\\tgoto out_mutex;\\n\\n\\tsize = chunk_count_trees(chunk);\\n\\tif (!size) {\\n\\t\\tspin_lock(&hash_lock);\\n\\t\\tlist_del_init(&chunk->trees);\\n\\t\\tlist_del_rcu(&chunk->hash);\\n\\t\\treplace_mark_chunk(mark, NULL);\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\tfsnotify_detach_mark(mark);\\n\\t\\tmutex_unlock(&audit_tree_group->mark_mutex);\\n\\t\\taudit_mark_put_chunk(chunk);\\n\\t\\tfsnotify_free_mark(mark);\\n\\t\\treturn;\\n\\t}\\n\\n\\tnew = alloc_chunk(size);\\n\\tif (!new)\\n\\t\\tgoto out_mutex;\\n\\n\\tspin_lock(&hash_lock);\\n\\t/*\\n\\t * This has to go last when updating chunk as once replace_chunk() is\\n\\t * called, new RCU readers can see the new chunk.\\n\\t */\\n\\treplace_chunk(new, chunk);\\n\\tspin_unlock(&hash_lock);\\n\\tmutex_unlock(&audit_tree_group->mark_mutex);\\n\\taudit_mark_put_chunk(chunk);\\n\\treturn;\\n\\nout_mutex:\\n\\tmutex_unlock(&audit_tree_group->mark_mutex);\\n}\\n\\n/* Call with group->mark_mutex held, releases it */\\nstatic int create_chunk(struct inode *inode, struct audit_tree *tree)\\n{\\n\\tstruct fsnotify_mark *mark;\\n\\tstruct audit_chunk *chunk = alloc_chunk(1);\\n\\n\\tif (!chunk) {\\n\\t\\tmutex_unlock(&audit_tree_group->mark_mutex);\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\tmark = alloc_mark();\\n\\tif (!mark) {\\n\\t\\tmutex_unlock(&audit_tree_group->mark_mutex);\\n\\t\\tkfree(chunk);\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\tif (fsnotify_add_inode_mark_locked(mark, inode, 0)) {\\n\\t\\tmutex_unlock(&audit_tree_group->mark_mutex);\\n\\t\\tfsnotify_put_mark(mark);\\n\\t\\tkfree(chunk);\\n\\t\\treturn -ENOSPC;\\n\\t}\\n\\n\\tspin_lock(&hash_lock);\\n\\tif (tree->goner) {\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\tfsnotify_detach_mark(mark);\\n\\t\\tmutex_unlock(&audit_tree_group->mark_mutex);\\n\\t\\tfsnotify_free_mark(mark);\\n\\t\\tfsnotify_put_mark(mark);\\n\\t\\tkfree(chunk);\\n\\t\\treturn 0;\\n\\t}\\n\\treplace_mark_chunk(mark, chunk);\\n\\tchunk->owners[0].index = (1U << 31);\\n\\tchunk->owners[0].owner = tree;\\n\\tget_tree(tree);\\n\\tlist_add(&chunk->owners[0].list, &tree->chunks);\\n\\tif (!tree->root) {\\n\\t\\ttree->root = chunk;\\n\\t\\tlist_add(&tree->same_root, &chunk->trees);\\n\\t}\\n\\tchunk->key = inode_to_key(inode);\\n\\t/*\\n\\t * Inserting into the hash table has to go last as once we do that RCU\\n\\t * readers can see the chunk.\\n\\t */\\n\\tinsert_hash(chunk);\\n\\tspin_unlock(&hash_lock);\\n\\tmutex_unlock(&audit_tree_group->mark_mutex);\\n\\t/*\\n\\t * Drop our initial reference. When mark we point to is getting freed,\\n\\t * we get notification through ->freeing_mark callback and cleanup\\n\\t * chunk pointing to this mark.\\n\\t */\\n\\tfsnotify_put_mark(mark);\\n\\treturn 0;\\n}\\n\\n/* the first tagged inode becomes root of tree */\\nstatic int tag_chunk(struct inode *inode, struct audit_tree *tree)\\n{\\n\\tstruct fsnotify_mark *mark;\\n\\tstruct audit_chunk *chunk, *old;\\n\\tstruct node *p;\\n\\tint n;\\n\\n\\tmutex_lock(&audit_tree_group->mark_mutex);\\n\\tmark = fsnotify_find_mark(&inode->i_fsnotify_marks, audit_tree_group);\\n\\tif (!mark)\\n\\t\\treturn create_chunk(inode, tree);\\n\\n\\t/*\\n\\t * Found mark is guaranteed to be attached and mark_mutex protects mark\\n\\t * from getting detached and thus it makes sure there is chunk attached\\n\\t * to the mark.\\n\\t */\\n\\t/* are we already there? */\\n\\tspin_lock(&hash_lock);\\n\\told = mark_chunk(mark);\\n\\tfor (n = 0; n < old->count; n++) {\\n\\t\\tif (old->owners[n].owner == tree) {\\n\\t\\t\\tspin_unlock(&hash_lock);\\n\\t\\t\\tmutex_unlock(&audit_tree_group->mark_mutex);\\n\\t\\t\\tfsnotify_put_mark(mark);\\n\\t\\t\\treturn 0;\\n\\t\\t}\\n\\t}\\n\\tspin_unlock(&hash_lock);\\n\\n\\tchunk = alloc_chunk(old->count + 1);\\n\\tif (!chunk) {\\n\\t\\tmutex_unlock(&audit_tree_group->mark_mutex);\\n\\t\\tfsnotify_put_mark(mark);\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\n\\tspin_lock(&hash_lock);\\n\\tif (tree->goner) {\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\tmutex_unlock(&audit_tree_group->mark_mutex);\\n\\t\\tfsnotify_put_mark(mark);\\n\\t\\tkfree(chunk);\\n\\t\\treturn 0;\\n\\t}\\n\\tp = &chunk->owners[chunk->count - 1];\\n\\tp->index = (chunk->count - 1) | (1U<<31);\\n\\tp->owner = tree;\\n\\tget_tree(tree);\\n\\tlist_add(&p->list, &tree->chunks);\\n\\tif (!tree->root) {\\n\\t\\ttree->root = chunk;\\n\\t\\tlist_add(&tree->same_root, &chunk->trees);\\n\\t}\\n\\t/*\\n\\t * This has to go last when updating chunk as once replace_chunk() is\\n\\t * called, new RCU readers can see the new chunk.\\n\\t */\\n\\treplace_chunk(chunk, old);\\n\\tspin_unlock(&hash_lock);\\n\\tmutex_unlock(&audit_tree_group->mark_mutex);\\n\\tfsnotify_put_mark(mark); /* pair to fsnotify_find_mark */\\n\\taudit_mark_put_chunk(old);\\n\\n\\treturn 0;\\n}\\n\\nstatic void audit_tree_log_remove_rule(struct audit_krule *rule)\\n{\\n\\tstruct audit_buffer *ab;\\n\\n\\tif (!audit_enabled)\\n\\t\\treturn;\\n\\tab = audit_log_start(NULL, GFP_KERNEL, AUDIT_CONFIG_CHANGE);\\n\\tif (unlikely(!ab))\\n\\t\\treturn;\\n\\taudit_log_format(ab, \"op=remove_rule dir=\");\\n\\taudit_log_untrustedstring(ab, rule->tree->pathname);\\n\\taudit_log_key(ab, rule->filterkey);\\n\\taudit_log_format(ab, \" list=%d res=1\", rule->listnr);\\n\\taudit_log_end(ab);\\n}\\n\\nstatic void kill_rules(struct audit_tree *tree)\\n{\\n\\tstruct audit_krule *rule, *next;\\n\\tstruct audit_entry *entry;\\n\\n\\tlist_for_each_entry_safe(rule, next, &tree->rules, rlist) {\\n\\t\\tentry = container_of(rule, struct audit_entry, rule);\\n\\n\\t\\tlist_del_init(&rule->rlist);\\n\\t\\tif (rule->tree) {\\n\\t\\t\\t/* not a half-baked one */\\n\\t\\t\\taudit_tree_log_remove_rule(rule);\\n\\t\\t\\tif (entry->rule.exe)\\n\\t\\t\\t\\taudit_remove_mark(entry->rule.exe);\\n\\t\\t\\trule->tree = NULL;\\n\\t\\t\\tlist_del_rcu(&entry->list);\\n\\t\\t\\tlist_del(&entry->rule.list);\\n\\t\\t\\tcall_rcu(&entry->rcu, audit_free_rule_rcu);\\n\\t\\t}\\n\\t}\\n}\\n\\n/*\\n * Remove tree from chunks. If \\'tagged\\' is set, remove tree only from tagged\\n * chunks. The function expects tagged chunks are all at the beginning of the\\n * chunks list.\\n */\\nstatic void prune_tree_chunks(struct audit_tree *victim, bool tagged)\\n{\\n\\tspin_lock(&hash_lock);\\n\\twhile (!list_empty(&victim->chunks)) {\\n\\t\\tstruct node *p;\\n\\t\\tstruct audit_chunk *chunk;\\n\\t\\tstruct fsnotify_mark *mark;\\n\\n\\t\\tp = list_first_entry(&victim->chunks, struct node, list);\\n\\t\\t/* have we run out of marked? */\\n\\t\\tif (tagged && !(p->index & (1U<<31)))\\n\\t\\t\\tbreak;\\n\\t\\tchunk = find_chunk(p);\\n\\t\\tmark = chunk->mark;\\n\\t\\tremove_chunk_node(chunk, p);\\n\\t\\t/* Racing with audit_tree_freeing_mark()? */\\n\\t\\tif (!mark)\\n\\t\\t\\tcontinue;\\n\\t\\tfsnotify_get_mark(mark);\\n\\t\\tspin_unlock(&hash_lock);\\n\\n\\t\\tuntag_chunk(chunk, mark);\\n\\t\\tfsnotify_put_mark(mark);\\n\\n\\t\\tspin_lock(&hash_lock);\\n\\t}\\n\\tspin_unlock(&hash_lock);\\n\\tput_tree(victim);\\n}\\n\\n/*\\n * finish killing struct audit_tree\\n */\\nstatic void prune_one(struct audit_tree *victim)\\n{\\n\\tprune_tree_chunks(victim, false);\\n}\\n\\n/* trim the uncommitted chunks from tree */\\n\\nstatic void trim_marked(struct audit_tree *tree)\\n{\\n\\tstruct list_head *p, *q;\\n\\tspin_lock(&hash_lock);\\n\\tif (tree->goner) {\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\treturn;\\n\\t}\\n\\t/* reorder */\\n\\tfor (p = tree->chunks.next; p != &tree->chunks; p = q) {\\n\\t\\tstruct node *node = list_entry(p, struct node, list);\\n\\t\\tq = p->next;\\n\\t\\tif (node->index & (1U<<31)) {\\n\\t\\t\\tlist_del_init(p);\\n\\t\\t\\tlist_add(p, &tree->chunks);\\n\\t\\t}\\n\\t}\\n\\tspin_unlock(&hash_lock);\\n\\n\\tprune_tree_chunks(tree, true);\\n\\n\\tspin_lock(&hash_lock);\\n\\tif (!tree->root && !tree->goner) {\\n\\t\\ttree->goner = 1;\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t\\tkill_rules(tree);\\n\\t\\tlist_del_init(&tree->list);\\n\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\t\\tprune_one(tree);\\n\\t} else {\\n\\t\\tspin_unlock(&hash_lock);\\n\\t}\\n}\\n\\nstatic void audit_schedule_prune(void);\\n\\n/* called with audit_filter_mutex */\\nint audit_remove_tree_rule(struct audit_krule *rule)\\n{\\n\\tstruct audit_tree *tree;\\n\\ttree = rule->tree;\\n\\tif (tree) {\\n\\t\\tspin_lock(&hash_lock);\\n\\t\\tlist_del_init(&rule->rlist);\\n\\t\\tif (list_empty(&tree->rules) && !tree->goner) {\\n\\t\\t\\ttree->root = NULL;\\n\\t\\t\\tlist_del_init(&tree->same_root);\\n\\t\\t\\ttree->goner = 1;\\n\\t\\t\\tlist_move(&tree->list, &prune_list);\\n\\t\\t\\trule->tree = NULL;\\n\\t\\t\\tspin_unlock(&hash_lock);\\n\\t\\t\\taudit_schedule_prune();\\n\\t\\t\\treturn 1;\\n\\t\\t}\\n\\t\\trule->tree = NULL;\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\treturn 1;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int compare_root(struct vfsmount *mnt, void *arg)\\n{\\n\\treturn inode_to_key(d_backing_inode(mnt->mnt_root)) ==\\n\\t       (unsigned long)arg;\\n}\\n\\nvoid audit_trim_trees(void)\\n{\\n\\tstruct list_head cursor;\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\tlist_add(&cursor, &tree_list);\\n\\twhile (cursor.next != &tree_list) {\\n\\t\\tstruct audit_tree *tree;\\n\\t\\tstruct path path;\\n\\t\\tstruct vfsmount *root_mnt;\\n\\t\\tstruct node *node;\\n\\t\\tint err;\\n\\n\\t\\ttree = container_of(cursor.next, struct audit_tree, list);\\n\\t\\tget_tree(tree);\\n\\t\\tlist_del(&cursor);\\n\\t\\tlist_add(&cursor, &tree->list);\\n\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\t\\terr = kern_path(tree->pathname, 0, &path);\\n\\t\\tif (err)\\n\\t\\t\\tgoto skip_it;\\n\\n\\t\\troot_mnt = collect_mounts(&path);\\n\\t\\tpath_put(&path);\\n\\t\\tif (IS_ERR(root_mnt))\\n\\t\\t\\tgoto skip_it;\\n\\n\\t\\tspin_lock(&hash_lock);\\n\\t\\tlist_for_each_entry(node, &tree->chunks, list) {\\n\\t\\t\\tstruct audit_chunk *chunk = find_chunk(node);\\n\\t\\t\\t/* this could be NULL if the watch is dying else where... */\\n\\t\\t\\tnode->index |= 1U<<31;\\n\\t\\t\\tif (iterate_mounts(compare_root,\\n\\t\\t\\t\\t\\t   (void *)(chunk->key),\\n\\t\\t\\t\\t\\t   root_mnt))\\n\\t\\t\\t\\tnode->index &= ~(1U<<31);\\n\\t\\t}\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\ttrim_marked(tree);\\n\\t\\tdrop_collected_mounts(root_mnt);\\nskip_it:\\n\\t\\tput_tree(tree);\\n\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t}\\n\\tlist_del(&cursor);\\n\\tmutex_unlock(&audit_filter_mutex);\\n}\\n\\nint audit_make_tree(struct audit_krule *rule, char *pathname, u32 op)\\n{\\n\\n\\tif (pathname[0] != \\'/\\' ||\\n\\t    rule->listnr != AUDIT_FILTER_EXIT ||\\n\\t    op != Audit_equal ||\\n\\t    rule->inode_f || rule->watch || rule->tree)\\n\\t\\treturn -EINVAL;\\n\\trule->tree = alloc_tree(pathname);\\n\\tif (!rule->tree)\\n\\t\\treturn -ENOMEM;\\n\\treturn 0;\\n}\\n\\nvoid audit_put_tree(struct audit_tree *tree)\\n{\\n\\tput_tree(tree);\\n}\\n\\nstatic int tag_mount(struct vfsmount *mnt, void *arg)\\n{\\n\\treturn tag_chunk(d_backing_inode(mnt->mnt_root), arg);\\n}\\n\\n/*\\n * That gets run when evict_chunk() ends up needing to kill audit_tree.\\n * Runs from a separate thread.\\n */\\nstatic int prune_tree_thread(void *unused)\\n{\\n\\tfor (;;) {\\n\\t\\tif (list_empty(&prune_list)) {\\n\\t\\t\\tset_current_state(TASK_INTERRUPTIBLE);\\n\\t\\t\\tschedule();\\n\\t\\t}\\n\\n\\t\\taudit_ctl_lock();\\n\\t\\tmutex_lock(&audit_filter_mutex);\\n\\n\\t\\twhile (!list_empty(&prune_list)) {\\n\\t\\t\\tstruct audit_tree *victim;\\n\\n\\t\\t\\tvictim = list_entry(prune_list.next,\\n\\t\\t\\t\\t\\tstruct audit_tree, list);\\n\\t\\t\\tlist_del_init(&victim->list);\\n\\n\\t\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\t\\t\\tprune_one(victim);\\n\\n\\t\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t\\t}\\n\\n\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\t\\taudit_ctl_unlock();\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int audit_launch_prune(void)\\n{\\n\\tif (prune_thread)\\n\\t\\treturn 0;\\n\\tprune_thread = kthread_run(prune_tree_thread, NULL,\\n\\t\\t\\t\\t\"audit_prune_tree\");\\n\\tif (IS_ERR(prune_thread)) {\\n\\t\\tpr_err(\"cannot start thread audit_prune_tree\");\\n\\t\\tprune_thread = NULL;\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\treturn 0;\\n}\\n\\n/* called with audit_filter_mutex */\\nint audit_add_tree_rule(struct audit_krule *rule)\\n{\\n\\tstruct audit_tree *seed = rule->tree, *tree;\\n\\tstruct path path;\\n\\tstruct vfsmount *mnt;\\n\\tint err;\\n\\n\\trule->tree = NULL;\\n\\tlist_for_each_entry(tree, &tree_list, list) {\\n\\t\\tif (!strcmp(seed->pathname, tree->pathname)) {\\n\\t\\t\\tput_tree(seed);\\n\\t\\t\\trule->tree = tree;\\n\\t\\t\\tlist_add(&rule->rlist, &tree->rules);\\n\\t\\t\\treturn 0;\\n\\t\\t}\\n\\t}\\n\\ttree = seed;\\n\\tlist_add(&tree->list, &tree_list);\\n\\tlist_add(&rule->rlist, &tree->rules);\\n\\t/* do not set rule->tree yet */\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\tif (unlikely(!prune_thread)) {\\n\\t\\terr = audit_launch_prune();\\n\\t\\tif (err)\\n\\t\\t\\tgoto Err;\\n\\t}\\n\\n\\terr = kern_path(tree->pathname, 0, &path);\\n\\tif (err)\\n\\t\\tgoto Err;\\n\\tmnt = collect_mounts(&path);\\n\\tpath_put(&path);\\n\\tif (IS_ERR(mnt)) {\\n\\t\\terr = PTR_ERR(mnt);\\n\\t\\tgoto Err;\\n\\t}\\n\\n\\tget_tree(tree);\\n\\terr = iterate_mounts(tag_mount, tree, mnt);\\n\\tdrop_collected_mounts(mnt);\\n\\n\\tif (!err) {\\n\\t\\tstruct node *node;\\n\\t\\tspin_lock(&hash_lock);\\n\\t\\tlist_for_each_entry(node, &tree->chunks, list)\\n\\t\\t\\tnode->index &= ~(1U<<31);\\n\\t\\tspin_unlock(&hash_lock);\\n\\t} else {\\n\\t\\ttrim_marked(tree);\\n\\t\\tgoto Err;\\n\\t}\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\tif (list_empty(&rule->rlist)) {\\n\\t\\tput_tree(tree);\\n\\t\\treturn -ENOENT;\\n\\t}\\n\\trule->tree = tree;\\n\\tput_tree(tree);\\n\\n\\treturn 0;\\nErr:\\n\\tmutex_lock(&audit_filter_mutex);\\n\\tlist_del_init(&tree->list);\\n\\tlist_del_init(&tree->rules);\\n\\tput_tree(tree);\\n\\treturn err;\\n}\\n\\nint audit_tag_tree(char *old, char *new)\\n{\\n\\tstruct list_head cursor, barrier;\\n\\tint failed = 0;\\n\\tstruct path path1, path2;\\n\\tstruct vfsmount *tagged;\\n\\tint err;\\n\\n\\terr = kern_path(new, 0, &path2);\\n\\tif (err)\\n\\t\\treturn err;\\n\\ttagged = collect_mounts(&path2);\\n\\tpath_put(&path2);\\n\\tif (IS_ERR(tagged))\\n\\t\\treturn PTR_ERR(tagged);\\n\\n\\terr = kern_path(old, 0, &path1);\\n\\tif (err) {\\n\\t\\tdrop_collected_mounts(tagged);\\n\\t\\treturn err;\\n\\t}\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\tlist_add(&barrier, &tree_list);\\n\\tlist_add(&cursor, &barrier);\\n\\n\\twhile (cursor.next != &tree_list) {\\n\\t\\tstruct audit_tree *tree;\\n\\t\\tint good_one = 0;\\n\\n\\t\\ttree = container_of(cursor.next, struct audit_tree, list);\\n\\t\\tget_tree(tree);\\n\\t\\tlist_del(&cursor);\\n\\t\\tlist_add(&cursor, &tree->list);\\n\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\t\\terr = kern_path(tree->pathname, 0, &path2);\\n\\t\\tif (!err) {\\n\\t\\t\\tgood_one = path_is_under(&path1, &path2);\\n\\t\\t\\tpath_put(&path2);\\n\\t\\t}\\n\\n\\t\\tif (!good_one) {\\n\\t\\t\\tput_tree(tree);\\n\\t\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\n\\t\\tfailed = iterate_mounts(tag_mount, tree, tagged);\\n\\t\\tif (failed) {\\n\\t\\t\\tput_tree(tree);\\n\\t\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\n\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t\\tspin_lock(&hash_lock);\\n\\t\\tif (!tree->goner) {\\n\\t\\t\\tlist_del(&tree->list);\\n\\t\\t\\tlist_add(&tree->list, &tree_list);\\n\\t\\t}\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\tput_tree(tree);\\n\\t}\\n\\n\\twhile (barrier.prev != &tree_list) {\\n\\t\\tstruct audit_tree *tree;\\n\\n\\t\\ttree = container_of(barrier.prev, struct audit_tree, list);\\n\\t\\tget_tree(tree);\\n\\t\\tlist_del(&tree->list);\\n\\t\\tlist_add(&tree->list, &barrier);\\n\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\t\\tif (!failed) {\\n\\t\\t\\tstruct node *node;\\n\\t\\t\\tspin_lock(&hash_lock);\\n\\t\\t\\tlist_for_each_entry(node, &tree->chunks, list)\\n\\t\\t\\t\\tnode->index &= ~(1U<<31);\\n\\t\\t\\tspin_unlock(&hash_lock);\\n\\t\\t} else {\\n\\t\\t\\ttrim_marked(tree);\\n\\t\\t}\\n\\n\\t\\tput_tree(tree);\\n\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t}\\n\\tlist_del(&barrier);\\n\\tlist_del(&cursor);\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\tpath_put(&path1);\\n\\tdrop_collected_mounts(tagged);\\n\\treturn failed;\\n}\\n\\n\\nstatic void audit_schedule_prune(void)\\n{\\n\\twake_up_process(prune_thread);\\n}\\n\\n/*\\n * ... and that one is done if evict_chunk() decides to delay until the end\\n * of syscall.  Runs synchronously.\\n */\\nvoid audit_kill_trees(struct list_head *list)\\n{\\n\\taudit_ctl_lock();\\n\\tmutex_lock(&audit_filter_mutex);\\n\\n\\twhile (!list_empty(list)) {\\n\\t\\tstruct audit_tree *victim;\\n\\n\\t\\tvictim = list_entry(list->next, struct audit_tree, list);\\n\\t\\tkill_rules(victim);\\n\\t\\tlist_del_init(&victim->list);\\n\\n\\t\\tmutex_unlock(&audit_filter_mutex);\\n\\n\\t\\tprune_one(victim);\\n\\n\\t\\tmutex_lock(&audit_filter_mutex);\\n\\t}\\n\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\taudit_ctl_unlock();\\n}\\n\\n/*\\n *  Here comes the stuff asynchronous to auditctl operations\\n */\\n\\nstatic void evict_chunk(struct audit_chunk *chunk)\\n{\\n\\tstruct audit_tree *owner;\\n\\tstruct list_head *postponed = audit_killed_trees();\\n\\tint need_prune = 0;\\n\\tint n;\\n\\n\\tmutex_lock(&audit_filter_mutex);\\n\\tspin_lock(&hash_lock);\\n\\twhile (!list_empty(&chunk->trees)) {\\n\\t\\towner = list_entry(chunk->trees.next,\\n\\t\\t\\t\\t   struct audit_tree, same_root);\\n\\t\\towner->goner = 1;\\n\\t\\towner->root = NULL;\\n\\t\\tlist_del_init(&owner->same_root);\\n\\t\\tspin_unlock(&hash_lock);\\n\\t\\tif (!postponed) {\\n\\t\\t\\tkill_rules(owner);\\n\\t\\t\\tlist_move(&owner->list, &prune_list);\\n\\t\\t\\tneed_prune = 1;\\n\\t\\t} else {\\n\\t\\t\\tlist_move(&owner->list, postponed);\\n\\t\\t}\\n\\t\\tspin_lock(&hash_lock);\\n\\t}\\n\\tlist_del_rcu(&chunk->hash);\\n\\tfor (n = 0; n < chunk->count; n++)\\n\\t\\tlist_del_init(&chunk->owners[n].list);\\n\\tspin_unlock(&hash_lock);\\n\\tmutex_unlock(&audit_filter_mutex);\\n\\tif (need_prune)\\n\\t\\taudit_schedule_prune();\\n}\\n\\nstatic int audit_tree_handle_event(struct fsnotify_group *group,\\n\\t\\t\\t\\t   struct inode *to_tell,\\n\\t\\t\\t\\t   u32 mask, const void *data, int data_type,\\n\\t\\t\\t\\t   const unsigned char *file_name, u32 cookie,\\n\\t\\t\\t\\t   struct fsnotify_iter_info *iter_info)\\n{\\n\\treturn 0;\\n}\\n\\nstatic void audit_tree_freeing_mark(struct fsnotify_mark *mark,\\n\\t\\t\\t\\t    struct fsnotify_group *group)\\n{\\n\\tstruct audit_chunk *chunk;\\n\\n\\tmutex_lock(&mark->group->mark_mutex);\\n\\tspin_lock(&hash_lock);\\n\\tchunk = mark_chunk(mark);\\n\\treplace_mark_chunk(mark, NULL);\\n\\tspin_unlock(&hash_lock);\\n\\tmutex_unlock(&mark->group->mark_mutex);\\n\\tif (chunk) {\\n\\t\\tevict_chunk(chunk);\\n\\t\\taudit_mark_put_chunk(chunk);\\n\\t}\\n\\n\\t/*\\n\\t * We are guaranteed to have at least one reference to the mark from\\n\\t * either the inode or the caller of fsnotify_destroy_mark().\\n\\t */\\n\\tBUG_ON(refcount_read(&mark->refcnt) < 1);\\n}\\n\\nstatic const struct fsnotify_ops audit_tree_ops = {\\n\\t.handle_event = audit_tree_handle_event,\\n\\t.freeing_mark = audit_tree_freeing_mark,\\n\\t.free_mark = audit_tree_destroy_watch,\\n};\\n\\nstatic int __init audit_tree_init(void)\\n{\\n\\tint i;\\n\\n\\taudit_tree_mark_cachep = KMEM_CACHE(audit_tree_mark, SLAB_PANIC);\\n\\n\\taudit_tree_group = fsnotify_alloc_group(&audit_tree_ops);\\n\\tif (IS_ERR(audit_tree_group))\\n\\t\\taudit_panic(\"cannot initialize fsnotify group for rectree watches\");\\n\\n\\tfor (i = 0; i < HASH_SIZE; i++)\\n\\t\\tINIT_LIST_HEAD(&chunk_hash_heads[i]);\\n\\n\\treturn 0;\\n}\\n__initcall(audit_tree_init);\\n\\n/* CPU control.\\n * (C) 2001, 2002, 2003, 2004 Rusty Russell\\n *\\n * This code is licenced under the GPL.\\n */\\n#include <linux/proc_fs.h>\\n#include <linux/smp.h>\\n#include <linux/init.h>\\n#include <linux/notifier.h>\\n#include <linux/sched/signal.h>\\n#include <linux/sched/hotplug.h>\\n#include <linux/sched/task.h>\\n#include <linux/sched/smt.h>\\n#include <linux/unistd.h>\\n#include <linux/cpu.h>\\n#include <linux/oom.h>\\n#include <linux/rcupdate.h>\\n#include <linux/export.h>\\n#include <linux/bug.h>\\n#include <linux/kthread.h>\\n#include <linux/stop_machine.h>\\n#include <linux/mutex.h>\\n#include <linux/gfp.h>\\n#include <linux/suspend.h>\\n#include <linux/lockdep.h>\\n#include <linux/tick.h>\\n#include <linux/irq.h>\\n#include <linux/nmi.h>\\n#include <linux/smpboot.h>\\n#include <linux/relay.h>\\n#include <linux/slab.h>\\n#include <linux/percpu-rwsem.h>\\n\\n#include <trace/events/power.h>\\n#define CREATE_TRACE_POINTS\\n#include <trace/events/cpuhp.h>\\n\\n#include \"smpboot.h\"\\n\\n/**\\n * cpuhp_cpu_state - Per cpu hotplug state storage\\n * @state:\\tThe current cpu state\\n * @target:\\tThe target state\\n * @thread:\\tPointer to the hotplug thread\\n * @should_run:\\tThread should execute\\n * @rollback:\\tPerform a rollback\\n * @single:\\tSingle callback invocation\\n * @bringup:\\tSingle callback bringup or teardown selector\\n * @cb_state:\\tThe state for a single callback (install/uninstall)\\n * @result:\\tResult of the operation\\n * @done_up:\\tSignal completion to the issuer of the task for cpu-up\\n * @done_down:\\tSignal completion to the issuer of the task for cpu-down\\n */\\nstruct cpuhp_cpu_state {\\n\\tenum cpuhp_state\\tstate;\\n\\tenum cpuhp_state\\ttarget;\\n\\tenum cpuhp_state\\tfail;\\n#ifdef CONFIG_SMP\\n\\tstruct task_struct\\t*thread;\\n\\tbool\\t\\t\\tshould_run;\\n\\tbool\\t\\t\\trollback;\\n\\tbool\\t\\t\\tsingle;\\n\\tbool\\t\\t\\tbringup;\\n\\tbool\\t\\t\\tbooted_once;\\n\\tstruct hlist_node\\t*node;\\n\\tstruct hlist_node\\t*last;\\n\\tenum cpuhp_state\\tcb_state;\\n\\tint\\t\\t\\tresult;\\n\\tstruct completion\\tdone_up;\\n\\tstruct completion\\tdone_down;\\n#endif\\n};\\n\\nstatic DEFINE_PER_CPU(struct cpuhp_cpu_state, cpuhp_state) = {\\n\\t.fail = CPUHP_INVALID,\\n};\\n\\n#if defined(CONFIG_LOCKDEP) && defined(CONFIG_SMP)\\nstatic struct lockdep_map cpuhp_state_up_map =\\n\\tSTATIC_LOCKDEP_MAP_INIT(\"cpuhp_state-up\", &cpuhp_state_up_map);\\nstatic struct lockdep_map cpuhp_state_down_map =\\n\\tSTATIC_LOCKDEP_MAP_INIT(\"cpuhp_state-down\", &cpuhp_state_down_map);\\n\\n\\nstatic inline void cpuhp_lock_acquire(bool bringup)\\n{\\n\\tlock_map_acquire(bringup ? &cpuhp_state_up_map : &cpuhp_state_down_map);\\n}\\n\\nstatic inline void cpuhp_lock_release(bool bringup)\\n{\\n\\tlock_map_release(bringup ? &cpuhp_state_up_map : &cpuhp_state_down_map);\\n}\\n#else\\n\\nstatic inline void cpuhp_lock_acquire(bool bringup) { }\\nstatic inline void cpuhp_lock_release(bool bringup) { }\\n\\n#endif\\n\\n/**\\n * cpuhp_step - Hotplug state machine step\\n * @name:\\tName of the step\\n * @startup:\\tStartup function of the step\\n * @teardown:\\tTeardown function of the step\\n * @cant_stop:\\tBringup/teardown can\\'t be stopped at this step\\n */\\nstruct cpuhp_step {\\n\\tconst char\\t\\t*name;\\n\\tunion {\\n\\t\\tint\\t\\t(*single)(unsigned int cpu);\\n\\t\\tint\\t\\t(*multi)(unsigned int cpu,\\n\\t\\t\\t\\t\\t struct hlist_node *node);\\n\\t} startup;\\n\\tunion {\\n\\t\\tint\\t\\t(*single)(unsigned int cpu);\\n\\t\\tint\\t\\t(*multi)(unsigned int cpu,\\n\\t\\t\\t\\t\\t struct hlist_node *node);\\n\\t} teardown;\\n\\tstruct hlist_head\\tlist;\\n\\tbool\\t\\t\\tcant_stop;\\n\\tbool\\t\\t\\tmulti_instance;\\n};\\n\\nstatic DEFINE_MUTEX(cpuhp_state_mutex);\\nstatic struct cpuhp_step cpuhp_hp_states[];\\n\\nstatic struct cpuhp_step *cpuhp_get_step(enum cpuhp_state state)\\n{\\n\\treturn cpuhp_hp_states + state;\\n}\\n\\n/**\\n * cpuhp_invoke_callback _ Invoke the callbacks for a given state\\n * @cpu:\\tThe cpu for which the callback should be invoked\\n * @state:\\tThe state to do callbacks for\\n * @bringup:\\tTrue if the bringup callback should be invoked\\n * @node:\\tFor multi-instance, do a single entry callback for install/remove\\n * @lastp:\\tFor multi-instance rollback, remember how far we got\\n *\\n * Called from cpu hotplug and from the state register machinery.\\n */\\nstatic int cpuhp_invoke_callback(unsigned int cpu, enum cpuhp_state state,\\n\\t\\t\\t\\t bool bringup, struct hlist_node *node,\\n\\t\\t\\t\\t struct hlist_node **lastp)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tstruct cpuhp_step *step = cpuhp_get_step(state);\\n\\tint (*cbm)(unsigned int cpu, struct hlist_node *node);\\n\\tint (*cb)(unsigned int cpu);\\n\\tint ret, cnt;\\n\\n\\tif (st->fail == state) {\\n\\t\\tst->fail = CPUHP_INVALID;\\n\\n\\t\\tif (!(bringup ? step->startup.single : step->teardown.single))\\n\\t\\t\\treturn 0;\\n\\n\\t\\treturn -EAGAIN;\\n\\t}\\n\\n\\tif (!step->multi_instance) {\\n\\t\\tWARN_ON_ONCE(lastp && *lastp);\\n\\t\\tcb = bringup ? step->startup.single : step->teardown.single;\\n\\t\\tif (!cb)\\n\\t\\t\\treturn 0;\\n\\t\\ttrace_cpuhp_enter(cpu, st->target, state, cb);\\n\\t\\tret = cb(cpu);\\n\\t\\ttrace_cpuhp_exit(cpu, st->state, state, ret);\\n\\t\\treturn ret;\\n\\t}\\n\\tcbm = bringup ? step->startup.multi : step->teardown.multi;\\n\\tif (!cbm)\\n\\t\\treturn 0;\\n\\n\\t/* Single invocation for instance add/remove */\\n\\tif (node) {\\n\\t\\tWARN_ON_ONCE(lastp && *lastp);\\n\\t\\ttrace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);\\n\\t\\tret = cbm(cpu, node);\\n\\t\\ttrace_cpuhp_exit(cpu, st->state, state, ret);\\n\\t\\treturn ret;\\n\\t}\\n\\n\\t/* State transition. Invoke on all instances */\\n\\tcnt = 0;\\n\\thlist_for_each(node, &step->list) {\\n\\t\\tif (lastp && node == *lastp)\\n\\t\\t\\tbreak;\\n\\n\\t\\ttrace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);\\n\\t\\tret = cbm(cpu, node);\\n\\t\\ttrace_cpuhp_exit(cpu, st->state, state, ret);\\n\\t\\tif (ret) {\\n\\t\\t\\tif (!lastp)\\n\\t\\t\\t\\tgoto err;\\n\\n\\t\\t\\t*lastp = node;\\n\\t\\t\\treturn ret;\\n\\t\\t}\\n\\t\\tcnt++;\\n\\t}\\n\\tif (lastp)\\n\\t\\t*lastp = NULL;\\n\\treturn 0;\\nerr:\\n\\t/* Rollback the instances if one failed */\\n\\tcbm = !bringup ? step->startup.multi : step->teardown.multi;\\n\\tif (!cbm)\\n\\t\\treturn ret;\\n\\n\\thlist_for_each(node, &step->list) {\\n\\t\\tif (!cnt--)\\n\\t\\t\\tbreak;\\n\\n\\t\\ttrace_cpuhp_multi_enter(cpu, st->target, state, cbm, node);\\n\\t\\tret = cbm(cpu, node);\\n\\t\\ttrace_cpuhp_exit(cpu, st->state, state, ret);\\n\\t\\t/*\\n\\t\\t * Rollback must not fail,\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(ret);\\n\\t}\\n\\treturn ret;\\n}\\n\\n#ifdef CONFIG_SMP\\nstatic bool cpuhp_is_ap_state(enum cpuhp_state state)\\n{\\n\\t/*\\n\\t * The extra check for CPUHP_TEARDOWN_CPU is only for documentation\\n\\t * purposes as that state is handled explicitly in cpu_down.\\n\\t */\\n\\treturn state > CPUHP_BRINGUP_CPU && state != CPUHP_TEARDOWN_CPU;\\n}\\n\\nstatic inline void wait_for_ap_thread(struct cpuhp_cpu_state *st, bool bringup)\\n{\\n\\tstruct completion *done = bringup ? &st->done_up : &st->done_down;\\n\\twait_for_completion(done);\\n}\\n\\nstatic inline void complete_ap_thread(struct cpuhp_cpu_state *st, bool bringup)\\n{\\n\\tstruct completion *done = bringup ? &st->done_up : &st->done_down;\\n\\tcomplete(done);\\n}\\n\\n/*\\n * The former STARTING/DYING states, ran with IRQs disabled and must not fail.\\n */\\nstatic bool cpuhp_is_atomic_state(enum cpuhp_state state)\\n{\\n\\treturn CPUHP_AP_IDLE_DEAD <= state && state < CPUHP_AP_ONLINE;\\n}\\n\\n/* Serializes the updates to cpu_online_mask, cpu_present_mask */\\nstatic DEFINE_MUTEX(cpu_add_remove_lock);\\nbool cpuhp_tasks_frozen;\\nEXPORT_SYMBOL_GPL(cpuhp_tasks_frozen);\\n\\n/*\\n * The following two APIs (cpu_maps_update_begin/done) must be used when\\n * attempting to serialize the updates to cpu_online_mask & cpu_present_mask.\\n */\\nvoid cpu_maps_update_begin(void)\\n{\\n\\tmutex_lock(&cpu_add_remove_lock);\\n}\\n\\nvoid cpu_maps_update_done(void)\\n{\\n\\tmutex_unlock(&cpu_add_remove_lock);\\n}\\n\\n/*\\n * If set, cpu_up and cpu_down will return -EBUSY and do nothing.\\n * Should always be manipulated under cpu_add_remove_lock\\n */\\nstatic int cpu_hotplug_disabled;\\n\\n#ifdef CONFIG_HOTPLUG_CPU\\n\\nDEFINE_STATIC_PERCPU_RWSEM(cpu_hotplug_lock);\\n\\nvoid cpus_read_lock(void)\\n{\\n\\tpercpu_down_read(&cpu_hotplug_lock);\\n}\\nEXPORT_SYMBOL_GPL(cpus_read_lock);\\n\\nint cpus_read_trylock(void)\\n{\\n\\treturn percpu_down_read_trylock(&cpu_hotplug_lock);\\n}\\nEXPORT_SYMBOL_GPL(cpus_read_trylock);\\n\\nvoid cpus_read_unlock(void)\\n{\\n\\tpercpu_up_read(&cpu_hotplug_lock);\\n}\\nEXPORT_SYMBOL_GPL(cpus_read_unlock);\\n\\nvoid cpus_write_lock(void)\\n{\\n\\tpercpu_down_write(&cpu_hotplug_lock);\\n}\\n\\nvoid cpus_write_unlock(void)\\n{\\n\\tpercpu_up_write(&cpu_hotplug_lock);\\n}\\n\\nvoid lockdep_assert_cpus_held(void)\\n{\\n\\tpercpu_rwsem_assert_held(&cpu_hotplug_lock);\\n}\\n\\nstatic void lockdep_acquire_cpus_lock(void)\\n{\\n\\trwsem_acquire(&cpu_hotplug_lock.rw_sem.dep_map, 0, 0, _THIS_IP_);\\n}\\n\\nstatic void lockdep_release_cpus_lock(void)\\n{\\n\\trwsem_release(&cpu_hotplug_lock.rw_sem.dep_map, 1, _THIS_IP_);\\n}\\n\\n/*\\n * Wait for currently running CPU hotplug operations to complete (if any) and\\n * disable future CPU hotplug (from sysfs). The \\'cpu_add_remove_lock\\' protects\\n * the \\'cpu_hotplug_disabled\\' flag. The same lock is also acquired by the\\n * hotplug path before performing hotplug operations. So acquiring that lock\\n * guarantees mutual exclusion from any currently running hotplug operations.\\n */\\nvoid cpu_hotplug_disable(void)\\n{\\n\\tcpu_maps_update_begin();\\n\\tcpu_hotplug_disabled++;\\n\\tcpu_maps_update_done();\\n}\\nEXPORT_SYMBOL_GPL(cpu_hotplug_disable);\\n\\nstatic void __cpu_hotplug_enable(void)\\n{\\n\\tif (WARN_ONCE(!cpu_hotplug_disabled, \"Unbalanced cpu hotplug enable\\\\n\"))\\n\\t\\treturn;\\n\\tcpu_hotplug_disabled--;\\n}\\n\\nvoid cpu_hotplug_enable(void)\\n{\\n\\tcpu_maps_update_begin();\\n\\t__cpu_hotplug_enable();\\n\\tcpu_maps_update_done();\\n}\\nEXPORT_SYMBOL_GPL(cpu_hotplug_enable);\\n\\n#else\\n\\nstatic void lockdep_acquire_cpus_lock(void)\\n{\\n}\\n\\nstatic void lockdep_release_cpus_lock(void)\\n{\\n}\\n\\n#endif\\t/* CONFIG_HOTPLUG_CPU */\\n\\n/*\\n * Architectures that need SMT-specific errata handling during SMT hotplug\\n * should override this.\\n */\\nvoid __weak arch_smt_update(void) { }\\n\\n#ifdef CONFIG_HOTPLUG_SMT\\nenum cpuhp_smt_control cpu_smt_control __read_mostly = CPU_SMT_ENABLED;\\n\\nvoid __init cpu_smt_disable(bool force)\\n{\\n\\tif (cpu_smt_control == CPU_SMT_FORCE_DISABLED ||\\n\\t\\tcpu_smt_control == CPU_SMT_NOT_SUPPORTED)\\n\\t\\treturn;\\n\\n\\tif (force) {\\n\\t\\tpr_info(\"SMT: Force disabled\\\\n\");\\n\\t\\tcpu_smt_control = CPU_SMT_FORCE_DISABLED;\\n\\t} else {\\n\\t\\tpr_info(\"SMT: disabled\\\\n\");\\n\\t\\tcpu_smt_control = CPU_SMT_DISABLED;\\n\\t}\\n}\\n\\n/*\\n * The decision whether SMT is supported can only be done after the full\\n * CPU identification. Called from architecture code.\\n */\\nvoid __init cpu_smt_check_topology(void)\\n{\\n\\tif (!topology_smt_supported())\\n\\t\\tcpu_smt_control = CPU_SMT_NOT_SUPPORTED;\\n}\\n\\nstatic int __init smt_cmdline_disable(char *str)\\n{\\n\\tcpu_smt_disable(str && !strcmp(str, \"force\"));\\n\\treturn 0;\\n}\\nearly_param(\"nosmt\", smt_cmdline_disable);\\n\\nstatic inline bool cpu_smt_allowed(unsigned int cpu)\\n{\\n\\tif (cpu_smt_control == CPU_SMT_ENABLED)\\n\\t\\treturn true;\\n\\n\\tif (topology_is_primary_thread(cpu))\\n\\t\\treturn true;\\n\\n\\t/*\\n\\t * On x86 it\\'s required to boot all logical CPUs at least once so\\n\\t * that the init code can get a chance to set CR4.MCE on each\\n\\t * CPU. Otherwise, a broadacasted MCE observing CR4.MCE=0b on any\\n\\t * core will shutdown the machine.\\n\\t */\\n\\treturn !per_cpu(cpuhp_state, cpu).booted_once;\\n}\\n#else\\nstatic inline bool cpu_smt_allowed(unsigned int cpu) { return true; }\\n#endif\\n\\nstatic inline enum cpuhp_state\\ncpuhp_set_state(struct cpuhp_cpu_state *st, enum cpuhp_state target)\\n{\\n\\tenum cpuhp_state prev_state = st->state;\\n\\n\\tst->rollback = false;\\n\\tst->last = NULL;\\n\\n\\tst->target = target;\\n\\tst->single = false;\\n\\tst->bringup = st->state < target;\\n\\n\\treturn prev_state;\\n}\\n\\nstatic inline void\\ncpuhp_reset_state(struct cpuhp_cpu_state *st, enum cpuhp_state prev_state)\\n{\\n\\tst->rollback = true;\\n\\n\\t/*\\n\\t * If we have st->last we need to undo partial multi_instance of this\\n\\t * state first. Otherwise start undo at the previous state.\\n\\t */\\n\\tif (!st->last) {\\n\\t\\tif (st->bringup)\\n\\t\\t\\tst->state--;\\n\\t\\telse\\n\\t\\t\\tst->state++;\\n\\t}\\n\\n\\tst->target = prev_state;\\n\\tst->bringup = !st->bringup;\\n}\\n\\n/* Regular hotplug invocation of the AP hotplug thread */\\nstatic void __cpuhp_kick_ap(struct cpuhp_cpu_state *st)\\n{\\n\\tif (!st->single && st->state == st->target)\\n\\t\\treturn;\\n\\n\\tst->result = 0;\\n\\t/*\\n\\t * Make sure the above stores are visible before should_run becomes\\n\\t * true. Paired with the mb() above in cpuhp_thread_fun()\\n\\t */\\n\\tsmp_mb();\\n\\tst->should_run = true;\\n\\twake_up_process(st->thread);\\n\\twait_for_ap_thread(st, st->bringup);\\n}\\n\\nstatic int cpuhp_kick_ap(struct cpuhp_cpu_state *st, enum cpuhp_state target)\\n{\\n\\tenum cpuhp_state prev_state;\\n\\tint ret;\\n\\n\\tprev_state = cpuhp_set_state(st, target);\\n\\t__cpuhp_kick_ap(st);\\n\\tif ((ret = st->result)) {\\n\\t\\tcpuhp_reset_state(st, prev_state);\\n\\t\\t__cpuhp_kick_ap(st);\\n\\t}\\n\\n\\treturn ret;\\n}\\n\\nstatic int bringup_wait_for_ap(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\n\\t/* Wait for the CPU to reach CPUHP_AP_ONLINE_IDLE */\\n\\twait_for_ap_thread(st, true);\\n\\tif (WARN_ON_ONCE((!cpu_online(cpu))))\\n\\t\\treturn -ECANCELED;\\n\\n\\t/* Unpark the stopper thread and the hotplug thread of the target cpu */\\n\\tstop_machine_unpark(cpu);\\n\\tkthread_unpark(st->thread);\\n\\n\\t/*\\n\\t * SMT soft disabling on X86 requires to bring the CPU out of the\\n\\t * BIOS \\'wait for SIPI\\' state in order to set the CR4.MCE bit.  The\\n\\t * CPU marked itself as booted_once in cpu_notify_starting() so the\\n\\t * cpu_smt_allowed() check will now return false if this is not the\\n\\t * primary sibling.\\n\\t */\\n\\tif (!cpu_smt_allowed(cpu))\\n\\t\\treturn -ECANCELED;\\n\\n\\tif (st->target <= CPUHP_AP_ONLINE_IDLE)\\n\\t\\treturn 0;\\n\\n\\treturn cpuhp_kick_ap(st, st->target);\\n}\\n\\nstatic int bringup_cpu(unsigned int cpu)\\n{\\n\\tstruct task_struct *idle = idle_thread_get(cpu);\\n\\tint ret;\\n\\n\\t/*\\n\\t * Some architectures have to walk the irq descriptors to\\n\\t * setup the vector space for the cpu which comes online.\\n\\t * Prevent irq alloc/free across the bringup.\\n\\t */\\n\\tirq_lock_sparse();\\n\\n\\t/* Arch-specific enabling code. */\\n\\tret = __cpu_up(cpu, idle);\\n\\tirq_unlock_sparse();\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\treturn bringup_wait_for_ap(cpu);\\n}\\n\\n/*\\n * Hotplug state machine related functions\\n */\\n\\nstatic void undo_cpu_up(unsigned int cpu, struct cpuhp_cpu_state *st)\\n{\\n\\tfor (st->state--; st->state > st->target; st->state--)\\n\\t\\tcpuhp_invoke_callback(cpu, st->state, false, NULL, NULL);\\n}\\n\\nstatic int cpuhp_up_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,\\n\\t\\t\\t      enum cpuhp_state target)\\n{\\n\\tenum cpuhp_state prev_state = st->state;\\n\\tint ret = 0;\\n\\n\\twhile (st->state < target) {\\n\\t\\tst->state++;\\n\\t\\tret = cpuhp_invoke_callback(cpu, st->state, true, NULL, NULL);\\n\\t\\tif (ret) {\\n\\t\\t\\tst->target = prev_state;\\n\\t\\t\\tundo_cpu_up(cpu, st);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\treturn ret;\\n}\\n\\n/*\\n * The cpu hotplug threads manage the bringup and teardown of the cpus\\n */\\nstatic void cpuhp_create(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\n\\tinit_completion(&st->done_up);\\n\\tinit_completion(&st->done_down);\\n}\\n\\nstatic int cpuhp_should_run(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\\n\\n\\treturn st->should_run;\\n}\\n\\n/*\\n * Execute teardown/startup callbacks on the plugged cpu. Also used to invoke\\n * callbacks when a state gets [un]installed at runtime.\\n *\\n * Each invocation of this function by the smpboot thread does a single AP\\n * state callback.\\n *\\n * It has 3 modes of operation:\\n *  - single: runs st->cb_state\\n *  - up:     runs ++st->state, while st->state < st->target\\n *  - down:   runs st->state--, while st->state > st->target\\n *\\n * When complete or on error, should_run is cleared and the completion is fired.\\n */\\nstatic void cpuhp_thread_fun(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\\n\\tbool bringup = st->bringup;\\n\\tenum cpuhp_state state;\\n\\n\\tif (WARN_ON_ONCE(!st->should_run))\\n\\t\\treturn;\\n\\n\\t/*\\n\\t * ACQUIRE for the cpuhp_should_run() load of ->should_run. Ensures\\n\\t * that if we see ->should_run we also see the rest of the state.\\n\\t */\\n\\tsmp_mb();\\n\\n\\t/*\\n\\t * The BP holds the hotplug lock, but we\\'re now running on the AP,\\n\\t * ensure that anybody asserting the lock is held, will actually find\\n\\t * it so.\\n\\t */\\n\\tlockdep_acquire_cpus_lock();\\n\\tcpuhp_lock_acquire(bringup);\\n\\n\\tif (st->single) {\\n\\t\\tstate = st->cb_state;\\n\\t\\tst->should_run = false;\\n\\t} else {\\n\\t\\tif (bringup) {\\n\\t\\t\\tst->state++;\\n\\t\\t\\tstate = st->state;\\n\\t\\t\\tst->should_run = (st->state < st->target);\\n\\t\\t\\tWARN_ON_ONCE(st->state > st->target);\\n\\t\\t} else {\\n\\t\\t\\tstate = st->state;\\n\\t\\t\\tst->state--;\\n\\t\\t\\tst->should_run = (st->state > st->target);\\n\\t\\t\\tWARN_ON_ONCE(st->state < st->target);\\n\\t\\t}\\n\\t}\\n\\n\\tWARN_ON_ONCE(!cpuhp_is_ap_state(state));\\n\\n\\tif (cpuhp_is_atomic_state(state)) {\\n\\t\\tlocal_irq_disable();\\n\\t\\tst->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);\\n\\t\\tlocal_irq_enable();\\n\\n\\t\\t/*\\n\\t\\t * STARTING/DYING must not fail!\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(st->result);\\n\\t} else {\\n\\t\\tst->result = cpuhp_invoke_callback(cpu, state, bringup, st->node, &st->last);\\n\\t}\\n\\n\\tif (st->result) {\\n\\t\\t/*\\n\\t\\t * If we fail on a rollback, we\\'re up a creek without no\\n\\t\\t * paddle, no way forward, no way back. We loose, thanks for\\n\\t\\t * playing.\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(st->rollback);\\n\\t\\tst->should_run = false;\\n\\t}\\n\\n\\tcpuhp_lock_release(bringup);\\n\\tlockdep_release_cpus_lock();\\n\\n\\tif (!st->should_run)\\n\\t\\tcomplete_ap_thread(st, bringup);\\n}\\n\\n/* Invoke a single callback on a remote cpu */\\nstatic int\\ncpuhp_invoke_ap_callback(int cpu, enum cpuhp_state state, bool bringup,\\n\\t\\t\\t struct hlist_node *node)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tint ret;\\n\\n\\tif (!cpu_online(cpu))\\n\\t\\treturn 0;\\n\\n\\tcpuhp_lock_acquire(false);\\n\\tcpuhp_lock_release(false);\\n\\n\\tcpuhp_lock_acquire(true);\\n\\tcpuhp_lock_release(true);\\n\\n\\t/*\\n\\t * If we are up and running, use the hotplug thread. For early calls\\n\\t * we invoke the thread function directly.\\n\\t */\\n\\tif (!st->thread)\\n\\t\\treturn cpuhp_invoke_callback(cpu, state, bringup, node, NULL);\\n\\n\\tst->rollback = false;\\n\\tst->last = NULL;\\n\\n\\tst->node = node;\\n\\tst->bringup = bringup;\\n\\tst->cb_state = state;\\n\\tst->single = true;\\n\\n\\t__cpuhp_kick_ap(st);\\n\\n\\t/*\\n\\t * If we failed and did a partial, do a rollback.\\n\\t */\\n\\tif ((ret = st->result) && st->last) {\\n\\t\\tst->rollback = true;\\n\\t\\tst->bringup = !bringup;\\n\\n\\t\\t__cpuhp_kick_ap(st);\\n\\t}\\n\\n\\t/*\\n\\t * Clean up the leftovers so the next hotplug operation wont use stale\\n\\t * data.\\n\\t */\\n\\tst->node = st->last = NULL;\\n\\treturn ret;\\n}\\n\\nstatic int cpuhp_kick_ap_work(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tenum cpuhp_state prev_state = st->state;\\n\\tint ret;\\n\\n\\tcpuhp_lock_acquire(false);\\n\\tcpuhp_lock_release(false);\\n\\n\\tcpuhp_lock_acquire(true);\\n\\tcpuhp_lock_release(true);\\n\\n\\ttrace_cpuhp_enter(cpu, st->target, prev_state, cpuhp_kick_ap_work);\\n\\tret = cpuhp_kick_ap(st, st->target);\\n\\ttrace_cpuhp_exit(cpu, st->state, prev_state, ret);\\n\\n\\treturn ret;\\n}\\n\\nstatic struct smp_hotplug_thread cpuhp_threads = {\\n\\t.store\\t\\t\\t= &cpuhp_state.thread,\\n\\t.create\\t\\t\\t= &cpuhp_create,\\n\\t.thread_should_run\\t= cpuhp_should_run,\\n\\t.thread_fn\\t\\t= cpuhp_thread_fun,\\n\\t.thread_comm\\t\\t= \"cpuhp/%u\",\\n\\t.selfparking\\t\\t= true,\\n};\\n\\nvoid __init cpuhp_threads_init(void)\\n{\\n\\tBUG_ON(smpboot_register_percpu_thread(&cpuhp_threads));\\n\\tkthread_unpark(this_cpu_read(cpuhp_state.thread));\\n}\\n\\n#ifdef CONFIG_HOTPLUG_CPU\\n/**\\n * clear_tasks_mm_cpumask - Safely clear tasks\\' mm_cpumask for a CPU\\n * @cpu: a CPU id\\n *\\n * This function walks all processes, finds a valid mm struct for each one and\\n * then clears a corresponding bit in mm\\'s cpumask.  While this all sounds\\n * trivial, there are various non-obvious corner cases, which this function\\n * tries to solve in a safe manner.\\n *\\n * Also note that the function uses a somewhat relaxed locking scheme, so it may\\n * be called only for an already offlined CPU.\\n */\\nvoid clear_tasks_mm_cpumask(int cpu)\\n{\\n\\tstruct task_struct *p;\\n\\n\\t/*\\n\\t * This function is called after the cpu is taken down and marked\\n\\t * offline, so its not like new tasks will ever get this cpu set in\\n\\t * their mm mask. -- Peter Zijlstra\\n\\t * Thus, we may use rcu_read_lock() here, instead of grabbing\\n\\t * full-fledged tasklist_lock.\\n\\t */\\n\\tWARN_ON(cpu_online(cpu));\\n\\trcu_read_lock();\\n\\tfor_each_process(p) {\\n\\t\\tstruct task_struct *t;\\n\\n\\t\\t/*\\n\\t\\t * Main thread might exit, but other threads may still have\\n\\t\\t * a valid mm. Find one.\\n\\t\\t */\\n\\t\\tt = find_lock_task_mm(p);\\n\\t\\tif (!t)\\n\\t\\t\\tcontinue;\\n\\t\\tcpumask_clear_cpu(cpu, mm_cpumask(t->mm));\\n\\t\\ttask_unlock(t);\\n\\t}\\n\\trcu_read_unlock();\\n}\\n\\n/* Take this CPU down. */\\nstatic int take_cpu_down(void *_param)\\n{\\n\\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\\n\\tenum cpuhp_state target = max((int)st->target, CPUHP_AP_OFFLINE);\\n\\tint err, cpu = smp_processor_id();\\n\\tint ret;\\n\\n\\t/* Ensure this CPU doesn\\'t handle any more interrupts. */\\n\\terr = __cpu_disable();\\n\\tif (err < 0)\\n\\t\\treturn err;\\n\\n\\t/*\\n\\t * We get here while we are in CPUHP_TEARDOWN_CPU state and we must not\\n\\t * do this step again.\\n\\t */\\n\\tWARN_ON(st->state != CPUHP_TEARDOWN_CPU);\\n\\tst->state--;\\n\\t/* Invoke the former CPU_DYING callbacks */\\n\\tfor (; st->state > target; st->state--) {\\n\\t\\tret = cpuhp_invoke_callback(cpu, st->state, false, NULL, NULL);\\n\\t\\t/*\\n\\t\\t * DYING must not fail!\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(ret);\\n\\t}\\n\\n\\t/* Give up timekeeping duties */\\n\\ttick_handover_do_timer();\\n\\t/* Park the stopper thread */\\n\\tstop_machine_park(cpu);\\n\\treturn 0;\\n}\\n\\nstatic int takedown_cpu(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tint err;\\n\\n\\t/* Park the smpboot threads */\\n\\tkthread_park(per_cpu_ptr(&cpuhp_state, cpu)->thread);\\n\\n\\t/*\\n\\t * Prevent irq alloc/free while the dying cpu reorganizes the\\n\\t * interrupt affinities.\\n\\t */\\n\\tirq_lock_sparse();\\n\\n\\t/*\\n\\t * So now all preempt/rcu users must observe !cpu_active().\\n\\t */\\n\\terr = stop_machine_cpuslocked(take_cpu_down, NULL, cpumask_of(cpu));\\n\\tif (err) {\\n\\t\\t/* CPU refused to die */\\n\\t\\tirq_unlock_sparse();\\n\\t\\t/* Unpark the hotplug thread so we can rollback there */\\n\\t\\tkthread_unpark(per_cpu_ptr(&cpuhp_state, cpu)->thread);\\n\\t\\treturn err;\\n\\t}\\n\\tBUG_ON(cpu_online(cpu));\\n\\n\\t/*\\n\\t * The teardown callback for CPUHP_AP_SCHED_STARTING will have removed\\n\\t * all runnable tasks from the CPU, there\\'s only the idle task left now\\n\\t * that the migration thread is done doing the stop_machine thing.\\n\\t *\\n\\t * Wait for the stop thread to go away.\\n\\t */\\n\\twait_for_ap_thread(st, false);\\n\\tBUG_ON(st->state != CPUHP_AP_IDLE_DEAD);\\n\\n\\t/* Interrupts are moved away from the dying cpu, reenable alloc/free */\\n\\tirq_unlock_sparse();\\n\\n\\thotplug_cpu__broadcast_tick_pull(cpu);\\n\\t/* This actually kills the CPU. */\\n\\t__cpu_die(cpu);\\n\\n\\ttick_cleanup_dead_cpu(cpu);\\n\\trcutree_migrate_callbacks(cpu);\\n\\treturn 0;\\n}\\n\\nstatic void cpuhp_complete_idle_dead(void *arg)\\n{\\n\\tstruct cpuhp_cpu_state *st = arg;\\n\\n\\tcomplete_ap_thread(st, false);\\n}\\n\\nvoid cpuhp_report_idle_dead(void)\\n{\\n\\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\\n\\n\\tBUG_ON(st->state != CPUHP_AP_OFFLINE);\\n\\trcu_report_dead(smp_processor_id());\\n\\tst->state = CPUHP_AP_IDLE_DEAD;\\n\\t/*\\n\\t * We cannot call complete after rcu_report_dead() so we delegate it\\n\\t * to an online cpu.\\n\\t */\\n\\tsmp_call_function_single(cpumask_first(cpu_online_mask),\\n\\t\\t\\t\\t cpuhp_complete_idle_dead, st, 0);\\n}\\n\\nstatic void undo_cpu_down(unsigned int cpu, struct cpuhp_cpu_state *st)\\n{\\n\\tfor (st->state++; st->state < st->target; st->state++)\\n\\t\\tcpuhp_invoke_callback(cpu, st->state, true, NULL, NULL);\\n}\\n\\nstatic int cpuhp_down_callbacks(unsigned int cpu, struct cpuhp_cpu_state *st,\\n\\t\\t\\t\\tenum cpuhp_state target)\\n{\\n\\tenum cpuhp_state prev_state = st->state;\\n\\tint ret = 0;\\n\\n\\tfor (; st->state > target; st->state--) {\\n\\t\\tret = cpuhp_invoke_callback(cpu, st->state, false, NULL, NULL);\\n\\t\\tif (ret) {\\n\\t\\t\\tst->target = prev_state;\\n\\t\\t\\tif (st->state < prev_state)\\n\\t\\t\\t\\tundo_cpu_down(cpu, st);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\treturn ret;\\n}\\n\\n/* Requires cpu_add_remove_lock to be held */\\nstatic int __ref _cpu_down(unsigned int cpu, int tasks_frozen,\\n\\t\\t\\t   enum cpuhp_state target)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tint prev_state, ret = 0;\\n\\n\\tif (num_online_cpus() == 1)\\n\\t\\treturn -EBUSY;\\n\\n\\tif (!cpu_present(cpu))\\n\\t\\treturn -EINVAL;\\n\\n\\tcpus_write_lock();\\n\\n\\tcpuhp_tasks_frozen = tasks_frozen;\\n\\n\\tprev_state = cpuhp_set_state(st, target);\\n\\t/*\\n\\t * If the current CPU state is in the range of the AP hotplug thread,\\n\\t * then we need to kick the thread.\\n\\t */\\n\\tif (st->state > CPUHP_TEARDOWN_CPU) {\\n\\t\\tst->target = max((int)target, CPUHP_TEARDOWN_CPU);\\n\\t\\tret = cpuhp_kick_ap_work(cpu);\\n\\t\\t/*\\n\\t\\t * The AP side has done the error rollback already. Just\\n\\t\\t * return the error code..\\n\\t\\t */\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\n\\t\\t/*\\n\\t\\t * We might have stopped still in the range of the AP hotplug\\n\\t\\t * thread. Nothing to do anymore.\\n\\t\\t */\\n\\t\\tif (st->state > CPUHP_TEARDOWN_CPU)\\n\\t\\t\\tgoto out;\\n\\n\\t\\tst->target = target;\\n\\t}\\n\\t/*\\n\\t * The AP brought itself down to CPUHP_TEARDOWN_CPU. So we need\\n\\t * to do the further cleanups.\\n\\t */\\n\\tret = cpuhp_down_callbacks(cpu, st, target);\\n\\tif (ret && st->state == CPUHP_TEARDOWN_CPU && st->state < prev_state) {\\n\\t\\tcpuhp_reset_state(st, prev_state);\\n\\t\\t__cpuhp_kick_ap(st);\\n\\t}\\n\\nout:\\n\\tcpus_write_unlock();\\n\\t/*\\n\\t * Do post unplug cleanup. This is still protected against\\n\\t * concurrent CPU hotplug via cpu_add_remove_lock.\\n\\t */\\n\\tlockup_detector_cleanup();\\n\\tarch_smt_update();\\n\\treturn ret;\\n}\\n\\nstatic int cpu_down_maps_locked(unsigned int cpu, enum cpuhp_state target)\\n{\\n\\tif (cpu_hotplug_disabled)\\n\\t\\treturn -EBUSY;\\n\\treturn _cpu_down(cpu, 0, target);\\n}\\n\\nstatic int do_cpu_down(unsigned int cpu, enum cpuhp_state target)\\n{\\n\\tint err;\\n\\n\\tcpu_maps_update_begin();\\n\\terr = cpu_down_maps_locked(cpu, target);\\n\\tcpu_maps_update_done();\\n\\treturn err;\\n}\\n\\nint cpu_down(unsigned int cpu)\\n{\\n\\treturn do_cpu_down(cpu, CPUHP_OFFLINE);\\n}\\nEXPORT_SYMBOL(cpu_down);\\n\\n#else\\n#define takedown_cpu\\t\\tNULL\\n#endif /*CONFIG_HOTPLUG_CPU*/\\n\\n/**\\n * notify_cpu_starting(cpu) - Invoke the callbacks on the starting CPU\\n * @cpu: cpu that just started\\n *\\n * It must be called by the arch code on the new cpu, before the new cpu\\n * enables interrupts and before the \"boot\" cpu returns from __cpu_up().\\n */\\nvoid notify_cpu_starting(unsigned int cpu)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tenum cpuhp_state target = min((int)st->target, CPUHP_AP_ONLINE);\\n\\tint ret;\\n\\n\\trcu_cpu_starting(cpu);\\t/* Enables RCU usage on this CPU. */\\n\\tst->booted_once = true;\\n\\twhile (st->state < target) {\\n\\t\\tst->state++;\\n\\t\\tret = cpuhp_invoke_callback(cpu, st->state, true, NULL, NULL);\\n\\t\\t/*\\n\\t\\t * STARTING must not fail!\\n\\t\\t */\\n\\t\\tWARN_ON_ONCE(ret);\\n\\t}\\n}\\n\\n/*\\n * Called from the idle task. Wake up the controlling task which brings the\\n * stopper and the hotplug thread of the upcoming CPU up and then delegates\\n * the rest of the online bringup to the hotplug thread.\\n */\\nvoid cpuhp_online_idle(enum cpuhp_state state)\\n{\\n\\tstruct cpuhp_cpu_state *st = this_cpu_ptr(&cpuhp_state);\\n\\n\\t/* Happens for the boot cpu */\\n\\tif (state != CPUHP_AP_ONLINE_IDLE)\\n\\t\\treturn;\\n\\n\\tst->state = CPUHP_AP_ONLINE_IDLE;\\n\\tcomplete_ap_thread(st, true);\\n}\\n\\n/* Requires cpu_add_remove_lock to be held */\\nstatic int _cpu_up(unsigned int cpu, int tasks_frozen, enum cpuhp_state target)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\tstruct task_struct *idle;\\n\\tint ret = 0;\\n\\n\\tcpus_write_lock();\\n\\n\\tif (!cpu_present(cpu)) {\\n\\t\\tret = -EINVAL;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\t/*\\n\\t * The caller of do_cpu_up might have raced with another\\n\\t * caller. Ignore it for now.\\n\\t */\\n\\tif (st->state >= target)\\n\\t\\tgoto out;\\n\\n\\tif (st->state == CPUHP_OFFLINE) {\\n\\t\\t/* Let it fail before we try to bring the cpu up */\\n\\t\\tidle = idle_thread_get(cpu);\\n\\t\\tif (IS_ERR(idle)) {\\n\\t\\t\\tret = PTR_ERR(idle);\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\n\\n\\tcpuhp_tasks_frozen = tasks_frozen;\\n\\n\\tcpuhp_set_state(st, target);\\n\\t/*\\n\\t * If the current CPU state is in the range of the AP hotplug thread,\\n\\t * then we need to kick the thread once more.\\n\\t */\\n\\tif (st->state > CPUHP_BRINGUP_CPU) {\\n\\t\\tret = cpuhp_kick_ap_work(cpu);\\n\\t\\t/*\\n\\t\\t * The AP side has done the error rollback already. Just\\n\\t\\t * return the error code..\\n\\t\\t */\\n\\t\\tif (ret)\\n\\t\\t\\tgoto out;\\n\\t}\\n\\n\\t/*\\n\\t * Try to reach the target state. We max out on the BP at\\n\\t * CPUHP_BRINGUP_CPU. After that the AP hotplug thread is\\n\\t * responsible for bringing it up to the target state.\\n\\t */\\n\\ttarget = min((int)target, CPUHP_BRINGUP_CPU);\\n\\tret = cpuhp_up_callbacks(cpu, st, target);\\nout:\\n\\tcpus_write_unlock();\\n\\tarch_smt_update();\\n\\treturn ret;\\n}\\n\\nstatic int do_cpu_up(unsigned int cpu, enum cpuhp_state target)\\n{\\n\\tint err = 0;\\n\\n\\tif (!cpu_possible(cpu)) {\\n\\t\\tpr_err(\"can\\'t online cpu %d because it is not configured as may-hotadd at boot time\\\\n\",\\n\\t\\t       cpu);\\n#if defined(CONFIG_IA64)\\n\\t\\tpr_err(\"please check additional_cpus= boot parameter\\\\n\");\\n#endif\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\terr = try_online_node(cpu_to_node(cpu));\\n\\tif (err)\\n\\t\\treturn err;\\n\\n\\tcpu_maps_update_begin();\\n\\n\\tif (cpu_hotplug_disabled) {\\n\\t\\terr = -EBUSY;\\n\\t\\tgoto out;\\n\\t}\\n\\tif (!cpu_smt_allowed(cpu)) {\\n\\t\\terr = -EPERM;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\terr = _cpu_up(cpu, 0, target);\\nout:\\n\\tcpu_maps_update_done();\\n\\treturn err;\\n}\\n\\nint cpu_up(unsigned int cpu)\\n{\\n\\treturn do_cpu_up(cpu, CPUHP_ONLINE);\\n}\\nEXPORT_SYMBOL_GPL(cpu_up);\\n\\n#ifdef CONFIG_PM_SLEEP_SMP\\nstatic cpumask_var_t frozen_cpus;\\n\\nint freeze_secondary_cpus(int primary)\\n{\\n\\tint cpu, error = 0;\\n\\n\\tcpu_maps_update_begin();\\n\\tif (!cpu_online(primary))\\n\\t\\tprimary = cpumask_first(cpu_online_mask);\\n\\t/*\\n\\t * We take down all of the non-boot CPUs in one shot to avoid races\\n\\t * with the userspace trying to use the CPU hotplug at the same time\\n\\t */\\n\\tcpumask_clear(frozen_cpus);\\n\\n\\tpr_info(\"Disabling non-boot CPUs ...\\\\n\");\\n\\tfor_each_online_cpu(cpu) {\\n\\t\\tif (cpu == primary)\\n\\t\\t\\tcontinue;\\n\\t\\ttrace_suspend_resume(TPS(\"CPU_OFF\"), cpu, true);\\n\\t\\terror = _cpu_down(cpu, 1, CPUHP_OFFLINE);\\n\\t\\ttrace_suspend_resume(TPS(\"CPU_OFF\"), cpu, false);\\n\\t\\tif (!error)\\n\\t\\t\\tcpumask_set_cpu(cpu, frozen_cpus);\\n\\t\\telse {\\n\\t\\t\\tpr_err(\"Error taking CPU%d down: %d\\\\n\", cpu, error);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\tif (!error)\\n\\t\\tBUG_ON(num_online_cpus() > 1);\\n\\telse\\n\\t\\tpr_err(\"Non-boot CPUs are not disabled\\\\n\");\\n\\n\\t/*\\n\\t * Make sure the CPUs won\\'t be enabled by someone else. We need to do\\n\\t * this even in case of failure as all disable_nonboot_cpus() users are\\n\\t * supposed to do enable_nonboot_cpus() on the failure path.\\n\\t */\\n\\tcpu_hotplug_disabled++;\\n\\n\\tcpu_maps_update_done();\\n\\treturn error;\\n}\\n\\nvoid __weak arch_enable_nonboot_cpus_begin(void)\\n{\\n}\\n\\nvoid __weak arch_enable_nonboot_cpus_end(void)\\n{\\n}\\n\\nvoid enable_nonboot_cpus(void)\\n{\\n\\tint cpu, error;\\n\\n\\t/* Allow everyone to use the CPU hotplug again */\\n\\tcpu_maps_update_begin();\\n\\t__cpu_hotplug_enable();\\n\\tif (cpumask_empty(frozen_cpus))\\n\\t\\tgoto out;\\n\\n\\tpr_info(\"Enabling non-boot CPUs ...\\\\n\");\\n\\n\\tarch_enable_nonboot_cpus_begin();\\n\\n\\tfor_each_cpu(cpu, frozen_cpus) {\\n\\t\\ttrace_suspend_resume(TPS(\"CPU_ON\"), cpu, true);\\n\\t\\terror = _cpu_up(cpu, 1, CPUHP_ONLINE);\\n\\t\\ttrace_suspend_resume(TPS(\"CPU_ON\"), cpu, false);\\n\\t\\tif (!error) {\\n\\t\\t\\tpr_info(\"CPU%d is up\\\\n\", cpu);\\n\\t\\t\\tcontinue;\\n\\t\\t}\\n\\t\\tpr_warn(\"Error taking CPU%d up: %d\\\\n\", cpu, error);\\n\\t}\\n\\n\\tarch_enable_nonboot_cpus_end();\\n\\n\\tcpumask_clear(frozen_cpus);\\nout:\\n\\tcpu_maps_update_done();\\n}\\n\\nstatic int __init alloc_frozen_cpus(void)\\n{\\n\\tif (!alloc_cpumask_var(&frozen_cpus, GFP_KERNEL|__GFP_ZERO))\\n\\t\\treturn -ENOMEM;\\n\\treturn 0;\\n}\\ncore_initcall(alloc_frozen_cpus);\\n\\n/*\\n * When callbacks for CPU hotplug notifications are being executed, we must\\n * ensure that the state of the system with respect to the tasks being frozen\\n * or not, as reported by the notification, remains unchanged *throughout the\\n * duration* of the execution of the callbacks.\\n * Hence we need to prevent the freezer from racing with regular CPU hotplug.\\n *\\n * This synchronization is implemented by mutually excluding regular CPU\\n * hotplug and Suspend/Hibernate call paths by hooking onto the Suspend/\\n * Hibernate notifications.\\n */\\nstatic int\\ncpu_hotplug_pm_callback(struct notifier_block *nb,\\n\\t\\t\\tunsigned long action, void *ptr)\\n{\\n\\tswitch (action) {\\n\\n\\tcase PM_SUSPEND_PREPARE:\\n\\tcase PM_HIBERNATION_PREPARE:\\n\\t\\tcpu_hotplug_disable();\\n\\t\\tbreak;\\n\\n\\tcase PM_POST_SUSPEND:\\n\\tcase PM_POST_HIBERNATION:\\n\\t\\tcpu_hotplug_enable();\\n\\t\\tbreak;\\n\\n\\tdefault:\\n\\t\\treturn NOTIFY_DONE;\\n\\t}\\n\\n\\treturn NOTIFY_OK;\\n}\\n\\n\\nstatic int __init cpu_hotplug_pm_sync_init(void)\\n{\\n\\t/*\\n\\t * cpu_hotplug_pm_callback has higher priority than x86\\n\\t * bsp_pm_callback which depends on cpu_hotplug_pm_callback\\n\\t * to disable cpu hotplug to avoid cpu hotplug race.\\n\\t */\\n\\tpm_notifier(cpu_hotplug_pm_callback, 0);\\n\\treturn 0;\\n}\\ncore_initcall(cpu_hotplug_pm_sync_init);\\n\\n#endif /* CONFIG_PM_SLEEP_SMP */\\n\\nint __boot_cpu_id;\\n\\n#endif /* CONFIG_SMP */\\n\\n/* Boot processor state steps */\\nstatic struct cpuhp_step cpuhp_hp_states[] = {\\n\\t[CPUHP_OFFLINE] = {\\n\\t\\t.name\\t\\t\\t= \"offline\",\\n\\t\\t.startup.single\\t\\t= NULL,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t},\\n#ifdef CONFIG_SMP\\n\\t[CPUHP_CREATE_THREADS]= {\\n\\t\\t.name\\t\\t\\t= \"threads:prepare\",\\n\\t\\t.startup.single\\t\\t= smpboot_create_threads,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t\\t.cant_stop\\t\\t= true,\\n\\t},\\n\\t[CPUHP_PERF_PREPARE] = {\\n\\t\\t.name\\t\\t\\t= \"perf:prepare\",\\n\\t\\t.startup.single\\t\\t= perf_event_init_cpu,\\n\\t\\t.teardown.single\\t= perf_event_exit_cpu,\\n\\t},\\n\\t[CPUHP_WORKQUEUE_PREP] = {\\n\\t\\t.name\\t\\t\\t= \"workqueue:prepare\",\\n\\t\\t.startup.single\\t\\t= workqueue_prepare_cpu,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t},\\n\\t[CPUHP_HRTIMERS_PREPARE] = {\\n\\t\\t.name\\t\\t\\t= \"hrtimers:prepare\",\\n\\t\\t.startup.single\\t\\t= hrtimers_prepare_cpu,\\n\\t\\t.teardown.single\\t= hrtimers_dead_cpu,\\n\\t},\\n\\t[CPUHP_SMPCFD_PREPARE] = {\\n\\t\\t.name\\t\\t\\t= \"smpcfd:prepare\",\\n\\t\\t.startup.single\\t\\t= smpcfd_prepare_cpu,\\n\\t\\t.teardown.single\\t= smpcfd_dead_cpu,\\n\\t},\\n\\t[CPUHP_RELAY_PREPARE] = {\\n\\t\\t.name\\t\\t\\t= \"relay:prepare\",\\n\\t\\t.startup.single\\t\\t= relay_prepare_cpu,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t},\\n\\t[CPUHP_SLAB_PREPARE] = {\\n\\t\\t.name\\t\\t\\t= \"slab:prepare\",\\n\\t\\t.startup.single\\t\\t= slab_prepare_cpu,\\n\\t\\t.teardown.single\\t= slab_dead_cpu,\\n\\t},\\n\\t[CPUHP_RCUTREE_PREP] = {\\n\\t\\t.name\\t\\t\\t= \"RCU/tree:prepare\",\\n\\t\\t.startup.single\\t\\t= rcutree_prepare_cpu,\\n\\t\\t.teardown.single\\t= rcutree_dead_cpu,\\n\\t},\\n\\t/*\\n\\t * On the tear-down path, timers_dead_cpu() must be invoked\\n\\t * before blk_mq_queue_reinit_notify() from notify_dead(),\\n\\t * otherwise a RCU stall occurs.\\n\\t */\\n\\t[CPUHP_TIMERS_PREPARE] = {\\n\\t\\t.name\\t\\t\\t= \"timers:prepare\",\\n\\t\\t.startup.single\\t\\t= timers_prepare_cpu,\\n\\t\\t.teardown.single\\t= timers_dead_cpu,\\n\\t},\\n\\t/* Kicks the plugged cpu into life */\\n\\t[CPUHP_BRINGUP_CPU] = {\\n\\t\\t.name\\t\\t\\t= \"cpu:bringup\",\\n\\t\\t.startup.single\\t\\t= bringup_cpu,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t\\t.cant_stop\\t\\t= true,\\n\\t},\\n\\t/* Final state before CPU kills itself */\\n\\t[CPUHP_AP_IDLE_DEAD] = {\\n\\t\\t.name\\t\\t\\t= \"idle:dead\",\\n\\t},\\n\\t/*\\n\\t * Last state before CPU enters the idle loop to die. Transient state\\n\\t * for synchronization.\\n\\t */\\n\\t[CPUHP_AP_OFFLINE] = {\\n\\t\\t.name\\t\\t\\t= \"ap:offline\",\\n\\t\\t.cant_stop\\t\\t= true,\\n\\t},\\n\\t/* First state is scheduler control. Interrupts are disabled */\\n\\t[CPUHP_AP_SCHED_STARTING] = {\\n\\t\\t.name\\t\\t\\t= \"sched:starting\",\\n\\t\\t.startup.single\\t\\t= sched_cpu_starting,\\n\\t\\t.teardown.single\\t= sched_cpu_dying,\\n\\t},\\n\\t[CPUHP_AP_RCUTREE_DYING] = {\\n\\t\\t.name\\t\\t\\t= \"RCU/tree:dying\",\\n\\t\\t.startup.single\\t\\t= NULL,\\n\\t\\t.teardown.single\\t= rcutree_dying_cpu,\\n\\t},\\n\\t[CPUHP_AP_SMPCFD_DYING] = {\\n\\t\\t.name\\t\\t\\t= \"smpcfd:dying\",\\n\\t\\t.startup.single\\t\\t= NULL,\\n\\t\\t.teardown.single\\t= smpcfd_dying_cpu,\\n\\t},\\n\\t/* Entry state on starting. Interrupts enabled from here on. Transient\\n\\t * state for synchronsization */\\n\\t[CPUHP_AP_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"ap:online\",\\n\\t},\\n\\t/*\\n\\t * Handled on controll processor until the plugged processor manages\\n\\t * this itself.\\n\\t */\\n\\t[CPUHP_TEARDOWN_CPU] = {\\n\\t\\t.name\\t\\t\\t= \"cpu:teardown\",\\n\\t\\t.startup.single\\t\\t= NULL,\\n\\t\\t.teardown.single\\t= takedown_cpu,\\n\\t\\t.cant_stop\\t\\t= true,\\n\\t},\\n\\t/* Handle smpboot threads park/unpark */\\n\\t[CPUHP_AP_SMPBOOT_THREADS] = {\\n\\t\\t.name\\t\\t\\t= \"smpboot/threads:online\",\\n\\t\\t.startup.single\\t\\t= smpboot_unpark_threads,\\n\\t\\t.teardown.single\\t= smpboot_park_threads,\\n\\t},\\n\\t[CPUHP_AP_IRQ_AFFINITY_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"irq/affinity:online\",\\n\\t\\t.startup.single\\t\\t= irq_affinity_online_cpu,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t},\\n\\t[CPUHP_AP_PERF_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"perf:online\",\\n\\t\\t.startup.single\\t\\t= perf_event_init_cpu,\\n\\t\\t.teardown.single\\t= perf_event_exit_cpu,\\n\\t},\\n\\t[CPUHP_AP_WATCHDOG_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"lockup_detector:online\",\\n\\t\\t.startup.single\\t\\t= lockup_detector_online_cpu,\\n\\t\\t.teardown.single\\t= lockup_detector_offline_cpu,\\n\\t},\\n\\t[CPUHP_AP_WORKQUEUE_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"workqueue:online\",\\n\\t\\t.startup.single\\t\\t= workqueue_online_cpu,\\n\\t\\t.teardown.single\\t= workqueue_offline_cpu,\\n\\t},\\n\\t[CPUHP_AP_RCUTREE_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"RCU/tree:online\",\\n\\t\\t.startup.single\\t\\t= rcutree_online_cpu,\\n\\t\\t.teardown.single\\t= rcutree_offline_cpu,\\n\\t},\\n#endif\\n\\t/*\\n\\t * The dynamically registered state space is here\\n\\t */\\n\\n#ifdef CONFIG_SMP\\n\\t/* Last state is scheduler control setting the cpu active */\\n\\t[CPUHP_AP_ACTIVE] = {\\n\\t\\t.name\\t\\t\\t= \"sched:active\",\\n\\t\\t.startup.single\\t\\t= sched_cpu_activate,\\n\\t\\t.teardown.single\\t= sched_cpu_deactivate,\\n\\t},\\n#endif\\n\\n\\t/* CPU is fully up and running. */\\n\\t[CPUHP_ONLINE] = {\\n\\t\\t.name\\t\\t\\t= \"online\",\\n\\t\\t.startup.single\\t\\t= NULL,\\n\\t\\t.teardown.single\\t= NULL,\\n\\t},\\n};\\n\\n/* Sanity check for callbacks */\\nstatic int cpuhp_cb_check(enum cpuhp_state state)\\n{\\n\\tif (state <= CPUHP_OFFLINE || state >= CPUHP_ONLINE)\\n\\t\\treturn -EINVAL;\\n\\treturn 0;\\n}\\n\\n/*\\n * Returns a free for dynamic slot assignment of the Online state. The states\\n * are protected by the cpuhp_slot_states mutex and an empty slot is identified\\n * by having no name assigned.\\n */\\nstatic int cpuhp_reserve_state(enum cpuhp_state state)\\n{\\n\\tenum cpuhp_state i, end;\\n\\tstruct cpuhp_step *step;\\n\\n\\tswitch (state) {\\n\\tcase CPUHP_AP_ONLINE_DYN:\\n\\t\\tstep = cpuhp_hp_states + CPUHP_AP_ONLINE_DYN;\\n\\t\\tend = CPUHP_AP_ONLINE_DYN_END;\\n\\t\\tbreak;\\n\\tcase CPUHP_BP_PREPARE_DYN:\\n\\t\\tstep = cpuhp_hp_states + CPUHP_BP_PREPARE_DYN;\\n\\t\\tend = CPUHP_BP_PREPARE_DYN_END;\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\tfor (i = state; i <= end; i++, step++) {\\n\\t\\tif (!step->name)\\n\\t\\t\\treturn i;\\n\\t}\\n\\tWARN(1, \"No more dynamic states available for CPU hotplug\\\\n\");\\n\\treturn -ENOSPC;\\n}\\n\\nstatic int cpuhp_store_callbacks(enum cpuhp_state state, const char *name,\\n\\t\\t\\t\\t int (*startup)(unsigned int cpu),\\n\\t\\t\\t\\t int (*teardown)(unsigned int cpu),\\n\\t\\t\\t\\t bool multi_instance)\\n{\\n\\t/* (Un)Install the callbacks for further cpu hotplug operations */\\n\\tstruct cpuhp_step *sp;\\n\\tint ret = 0;\\n\\n\\t/*\\n\\t * If name is NULL, then the state gets removed.\\n\\t *\\n\\t * CPUHP_AP_ONLINE_DYN and CPUHP_BP_PREPARE_DYN are handed out on\\n\\t * the first allocation from these dynamic ranges, so the removal\\n\\t * would trigger a new allocation and clear the wrong (already\\n\\t * empty) state, leaving the callbacks of the to be cleared state\\n\\t * dangling, which causes wreckage on the next hotplug operation.\\n\\t */\\n\\tif (name && (state == CPUHP_AP_ONLINE_DYN ||\\n\\t\\t     state == CPUHP_BP_PREPARE_DYN)) {\\n\\t\\tret = cpuhp_reserve_state(state);\\n\\t\\tif (ret < 0)\\n\\t\\t\\treturn ret;\\n\\t\\tstate = ret;\\n\\t}\\n\\tsp = cpuhp_get_step(state);\\n\\tif (name && sp->name)\\n\\t\\treturn -EBUSY;\\n\\n\\tsp->startup.single = startup;\\n\\tsp->teardown.single = teardown;\\n\\tsp->name = name;\\n\\tsp->multi_instance = multi_instance;\\n\\tINIT_HLIST_HEAD(&sp->list);\\n\\treturn ret;\\n}\\n\\nstatic void *cpuhp_get_teardown_cb(enum cpuhp_state state)\\n{\\n\\treturn cpuhp_get_step(state)->teardown.single;\\n}\\n\\n/*\\n * Call the startup/teardown function for a step either on the AP or\\n * on the current CPU.\\n */\\nstatic int cpuhp_issue_call(int cpu, enum cpuhp_state state, bool bringup,\\n\\t\\t\\t    struct hlist_node *node)\\n{\\n\\tstruct cpuhp_step *sp = cpuhp_get_step(state);\\n\\tint ret;\\n\\n\\t/*\\n\\t * If there\\'s nothing to do, we done.\\n\\t * Relies on the union for multi_instance.\\n\\t */\\n\\tif ((bringup && !sp->startup.single) ||\\n\\t    (!bringup && !sp->teardown.single))\\n\\t\\treturn 0;\\n\\t/*\\n\\t * The non AP bound callbacks can fail on bringup. On teardown\\n\\t * e.g. module removal we crash for now.\\n\\t */\\n#ifdef CONFIG_SMP\\n\\tif (cpuhp_is_ap_state(state))\\n\\t\\tret = cpuhp_invoke_ap_callback(cpu, state, bringup, node);\\n\\telse\\n\\t\\tret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);\\n#else\\n\\tret = cpuhp_invoke_callback(cpu, state, bringup, node, NULL);\\n#endif\\n\\tBUG_ON(ret && !bringup);\\n\\treturn ret;\\n}\\n\\n/*\\n * Called from __cpuhp_setup_state on a recoverable failure.\\n *\\n * Note: The teardown callbacks for rollback are not allowed to fail!\\n */\\nstatic void cpuhp_rollback_install(int failedcpu, enum cpuhp_state state,\\n\\t\\t\\t\\t   struct hlist_node *node)\\n{\\n\\tint cpu;\\n\\n\\t/* Roll back the already executed steps on the other cpus */\\n\\tfor_each_present_cpu(cpu) {\\n\\t\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\t\\tint cpustate = st->state;\\n\\n\\t\\tif (cpu >= failedcpu)\\n\\t\\t\\tbreak;\\n\\n\\t\\t/* Did we invoke the startup call on that cpu ? */\\n\\t\\tif (cpustate >= state)\\n\\t\\t\\tcpuhp_issue_call(cpu, state, false, node);\\n\\t}\\n}\\n\\nint __cpuhp_state_add_instance_cpuslocked(enum cpuhp_state state,\\n\\t\\t\\t\\t\\t  struct hlist_node *node,\\n\\t\\t\\t\\t\\t  bool invoke)\\n{\\n\\tstruct cpuhp_step *sp;\\n\\tint cpu;\\n\\tint ret;\\n\\n\\tlockdep_assert_cpus_held();\\n\\n\\tsp = cpuhp_get_step(state);\\n\\tif (sp->multi_instance == false)\\n\\t\\treturn -EINVAL;\\n\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\n\\tif (!invoke || !sp->startup.multi)\\n\\t\\tgoto add_node;\\n\\n\\t/*\\n\\t * Try to call the startup callback for each present cpu\\n\\t * depending on the hotplug state of the cpu.\\n\\t */\\n\\tfor_each_present_cpu(cpu) {\\n\\t\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\t\\tint cpustate = st->state;\\n\\n\\t\\tif (cpustate < state)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tret = cpuhp_issue_call(cpu, state, true, node);\\n\\t\\tif (ret) {\\n\\t\\t\\tif (sp->teardown.multi)\\n\\t\\t\\t\\tcpuhp_rollback_install(cpu, state, node);\\n\\t\\t\\tgoto unlock;\\n\\t\\t}\\n\\t}\\nadd_node:\\n\\tret = 0;\\n\\thlist_add_head(node, &sp->list);\\nunlock:\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n\\treturn ret;\\n}\\n\\nint __cpuhp_state_add_instance(enum cpuhp_state state, struct hlist_node *node,\\n\\t\\t\\t       bool invoke)\\n{\\n\\tint ret;\\n\\n\\tcpus_read_lock();\\n\\tret = __cpuhp_state_add_instance_cpuslocked(state, node, invoke);\\n\\tcpus_read_unlock();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL_GPL(__cpuhp_state_add_instance);\\n\\n/**\\n * __cpuhp_setup_state_cpuslocked - Setup the callbacks for an hotplug machine state\\n * @state:\\t\\tThe state to setup\\n * @invoke:\\t\\tIf true, the startup function is invoked for cpus where\\n *\\t\\t\\tcpu state >= @state\\n * @startup:\\t\\tstartup callback function\\n * @teardown:\\t\\tteardown callback function\\n * @multi_instance:\\tState is set up for multiple instances which get\\n *\\t\\t\\tadded afterwards.\\n *\\n * The caller needs to hold cpus read locked while calling this function.\\n * Returns:\\n *   On success:\\n *      Positive state number if @state is CPUHP_AP_ONLINE_DYN\\n *      0 for all other states\\n *   On failure: proper (negative) error code\\n */\\nint __cpuhp_setup_state_cpuslocked(enum cpuhp_state state,\\n\\t\\t\\t\\t   const char *name, bool invoke,\\n\\t\\t\\t\\t   int (*startup)(unsigned int cpu),\\n\\t\\t\\t\\t   int (*teardown)(unsigned int cpu),\\n\\t\\t\\t\\t   bool multi_instance)\\n{\\n\\tint cpu, ret = 0;\\n\\tbool dynstate;\\n\\n\\tlockdep_assert_cpus_held();\\n\\n\\tif (cpuhp_cb_check(state) || !name)\\n\\t\\treturn -EINVAL;\\n\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\n\\tret = cpuhp_store_callbacks(state, name, startup, teardown,\\n\\t\\t\\t\\t    multi_instance);\\n\\n\\tdynstate = state == CPUHP_AP_ONLINE_DYN;\\n\\tif (ret > 0 && dynstate) {\\n\\t\\tstate = ret;\\n\\t\\tret = 0;\\n\\t}\\n\\n\\tif (ret || !invoke || !startup)\\n\\t\\tgoto out;\\n\\n\\t/*\\n\\t * Try to call the startup callback for each present cpu\\n\\t * depending on the hotplug state of the cpu.\\n\\t */\\n\\tfor_each_present_cpu(cpu) {\\n\\t\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\t\\tint cpustate = st->state;\\n\\n\\t\\tif (cpustate < state)\\n\\t\\t\\tcontinue;\\n\\n\\t\\tret = cpuhp_issue_call(cpu, state, true, NULL);\\n\\t\\tif (ret) {\\n\\t\\t\\tif (teardown)\\n\\t\\t\\t\\tcpuhp_rollback_install(cpu, state, NULL);\\n\\t\\t\\tcpuhp_store_callbacks(state, NULL, NULL, NULL, false);\\n\\t\\t\\tgoto out;\\n\\t\\t}\\n\\t}\\nout:\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n\\t/*\\n\\t * If the requested state is CPUHP_AP_ONLINE_DYN, return the\\n\\t * dynamically allocated state in case of success.\\n\\t */\\n\\tif (!ret && dynstate)\\n\\t\\treturn state;\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL(__cpuhp_setup_state_cpuslocked);\\n\\nint __cpuhp_setup_state(enum cpuhp_state state,\\n\\t\\t\\tconst char *name, bool invoke,\\n\\t\\t\\tint (*startup)(unsigned int cpu),\\n\\t\\t\\tint (*teardown)(unsigned int cpu),\\n\\t\\t\\tbool multi_instance)\\n{\\n\\tint ret;\\n\\n\\tcpus_read_lock();\\n\\tret = __cpuhp_setup_state_cpuslocked(state, name, invoke, startup,\\n\\t\\t\\t\\t\\t     teardown, multi_instance);\\n\\tcpus_read_unlock();\\n\\treturn ret;\\n}\\nEXPORT_SYMBOL(__cpuhp_setup_state);\\n\\nint __cpuhp_state_remove_instance(enum cpuhp_state state,\\n\\t\\t\\t\\t  struct hlist_node *node, bool invoke)\\n{\\n\\tstruct cpuhp_step *sp = cpuhp_get_step(state);\\n\\tint cpu;\\n\\n\\tBUG_ON(cpuhp_cb_check(state));\\n\\n\\tif (!sp->multi_instance)\\n\\t\\treturn -EINVAL;\\n\\n\\tcpus_read_lock();\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\n\\tif (!invoke || !cpuhp_get_teardown_cb(state))\\n\\t\\tgoto remove;\\n\\t/*\\n\\t * Call the teardown callback for each present cpu depending\\n\\t * on the hotplug state of the cpu. This function is not\\n\\t * allowed to fail currently!\\n\\t */\\n\\tfor_each_present_cpu(cpu) {\\n\\t\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\t\\tint cpustate = st->state;\\n\\n\\t\\tif (cpustate >= state)\\n\\t\\t\\tcpuhp_issue_call(cpu, state, false, node);\\n\\t}\\n\\nremove:\\n\\thlist_del(node);\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n\\tcpus_read_unlock();\\n\\n\\treturn 0;\\n}\\nEXPORT_SYMBOL_GPL(__cpuhp_state_remove_instance);\\n\\n/**\\n * __cpuhp_remove_state_cpuslocked - Remove the callbacks for an hotplug machine state\\n * @state:\\tThe state to remove\\n * @invoke:\\tIf true, the teardown function is invoked for cpus where\\n *\\t\\tcpu state >= @state\\n *\\n * The caller needs to hold cpus read locked while calling this function.\\n * The teardown callback is currently not allowed to fail. Think\\n * about module removal!\\n */\\nvoid __cpuhp_remove_state_cpuslocked(enum cpuhp_state state, bool invoke)\\n{\\n\\tstruct cpuhp_step *sp = cpuhp_get_step(state);\\n\\tint cpu;\\n\\n\\tBUG_ON(cpuhp_cb_check(state));\\n\\n\\tlockdep_assert_cpus_held();\\n\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\tif (sp->multi_instance) {\\n\\t\\tWARN(!hlist_empty(&sp->list),\\n\\t\\t     \"Error: Removing state %d which has instances left.\\\\n\",\\n\\t\\t     state);\\n\\t\\tgoto remove;\\n\\t}\\n\\n\\tif (!invoke || !cpuhp_get_teardown_cb(state))\\n\\t\\tgoto remove;\\n\\n\\t/*\\n\\t * Call the teardown callback for each present cpu depending\\n\\t * on the hotplug state of the cpu. This function is not\\n\\t * allowed to fail currently!\\n\\t */\\n\\tfor_each_present_cpu(cpu) {\\n\\t\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, cpu);\\n\\t\\tint cpustate = st->state;\\n\\n\\t\\tif (cpustate >= state)\\n\\t\\t\\tcpuhp_issue_call(cpu, state, false, NULL);\\n\\t}\\nremove:\\n\\tcpuhp_store_callbacks(state, NULL, NULL, NULL, false);\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n}\\nEXPORT_SYMBOL(__cpuhp_remove_state_cpuslocked);\\n\\nvoid __cpuhp_remove_state(enum cpuhp_state state, bool invoke)\\n{\\n\\tcpus_read_lock();\\n\\t__cpuhp_remove_state_cpuslocked(state, invoke);\\n\\tcpus_read_unlock();\\n}\\nEXPORT_SYMBOL(__cpuhp_remove_state);\\n\\n#if defined(CONFIG_SYSFS) && defined(CONFIG_HOTPLUG_CPU)\\nstatic ssize_t show_cpuhp_state(struct device *dev,\\n\\t\\t\\t\\tstruct device_attribute *attr, char *buf)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\\n\\n\\treturn sprintf(buf, \"%d\\\\n\", st->state);\\n}\\nstatic DEVICE_ATTR(state, 0444, show_cpuhp_state, NULL);\\n\\nstatic ssize_t write_cpuhp_target(struct device *dev,\\n\\t\\t\\t\\t  struct device_attribute *attr,\\n\\t\\t\\t\\t  const char *buf, size_t count)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\\n\\tstruct cpuhp_step *sp;\\n\\tint target, ret;\\n\\n\\tret = kstrtoint(buf, 10, &target);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n#ifdef CONFIG_CPU_HOTPLUG_STATE_CONTROL\\n\\tif (target < CPUHP_OFFLINE || target > CPUHP_ONLINE)\\n\\t\\treturn -EINVAL;\\n#else\\n\\tif (target != CPUHP_OFFLINE && target != CPUHP_ONLINE)\\n\\t\\treturn -EINVAL;\\n#endif\\n\\n\\tret = lock_device_hotplug_sysfs();\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\tsp = cpuhp_get_step(target);\\n\\tret = !sp->name || sp->cant_stop ? -EINVAL : 0;\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n\\tif (ret)\\n\\t\\tgoto out;\\n\\n\\tif (st->state < target)\\n\\t\\tret = do_cpu_up(dev->id, target);\\n\\telse\\n\\t\\tret = do_cpu_down(dev->id, target);\\nout:\\n\\tunlock_device_hotplug();\\n\\treturn ret ? ret : count;\\n}\\n\\nstatic ssize_t show_cpuhp_target(struct device *dev,\\n\\t\\t\\t\\t struct device_attribute *attr, char *buf)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\\n\\n\\treturn sprintf(buf, \"%d\\\\n\", st->target);\\n}\\nstatic DEVICE_ATTR(target, 0644, show_cpuhp_target, write_cpuhp_target);\\n\\n\\nstatic ssize_t write_cpuhp_fail(struct device *dev,\\n\\t\\t\\t\\tstruct device_attribute *attr,\\n\\t\\t\\t\\tconst char *buf, size_t count)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\\n\\tstruct cpuhp_step *sp;\\n\\tint fail, ret;\\n\\n\\tret = kstrtoint(buf, 10, &fail);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\t/*\\n\\t * Cannot fail STARTING/DYING callbacks.\\n\\t */\\n\\tif (cpuhp_is_atomic_state(fail))\\n\\t\\treturn -EINVAL;\\n\\n\\t/*\\n\\t * Cannot fail anything that doesn\\'t have callbacks.\\n\\t */\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\tsp = cpuhp_get_step(fail);\\n\\tif (!sp->startup.single && !sp->teardown.single)\\n\\t\\tret = -EINVAL;\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tst->fail = fail;\\n\\n\\treturn count;\\n}\\n\\nstatic ssize_t show_cpuhp_fail(struct device *dev,\\n\\t\\t\\t       struct device_attribute *attr, char *buf)\\n{\\n\\tstruct cpuhp_cpu_state *st = per_cpu_ptr(&cpuhp_state, dev->id);\\n\\n\\treturn sprintf(buf, \"%d\\\\n\", st->fail);\\n}\\n\\nstatic DEVICE_ATTR(fail, 0644, show_cpuhp_fail, write_cpuhp_fail);\\n\\nstatic struct attribute *cpuhp_cpu_attrs[] = {\\n\\t&dev_attr_state.attr,\\n\\t&dev_attr_target.attr,\\n\\t&dev_attr_fail.attr,\\n\\tNULL\\n};\\n\\nstatic const struct attribute_group cpuhp_cpu_attr_group = {\\n\\t.attrs = cpuhp_cpu_attrs,\\n\\t.name = \"hotplug\",\\n\\tNULL\\n};\\n\\nstatic ssize_t show_cpuhp_states(struct device *dev,\\n\\t\\t\\t\\t struct device_attribute *attr, char *buf)\\n{\\n\\tssize_t cur, res = 0;\\n\\tint i;\\n\\n\\tmutex_lock(&cpuhp_state_mutex);\\n\\tfor (i = CPUHP_OFFLINE; i <= CPUHP_ONLINE; i++) {\\n\\t\\tstruct cpuhp_step *sp = cpuhp_get_step(i);\\n\\n\\t\\tif (sp->name) {\\n\\t\\t\\tcur = sprintf(buf, \"%3d: %s\\\\n\", i, sp->name);\\n\\t\\t\\tbuf += cur;\\n\\t\\t\\tres += cur;\\n\\t\\t}\\n\\t}\\n\\tmutex_unlock(&cpuhp_state_mutex);\\n\\treturn res;\\n}\\nstatic DEVICE_ATTR(states, 0444, show_cpuhp_states, NULL);\\n\\nstatic struct attribute *cpuhp_cpu_root_attrs[] = {\\n\\t&dev_attr_states.attr,\\n\\tNULL\\n};\\n\\nstatic const struct attribute_group cpuhp_cpu_root_attr_group = {\\n\\t.attrs = cpuhp_cpu_root_attrs,\\n\\t.name = \"hotplug\",\\n\\tNULL\\n};\\n\\n#ifdef CONFIG_HOTPLUG_SMT\\n\\nstatic const char *smt_states[] = {\\n\\t[CPU_SMT_ENABLED]\\t\\t= \"on\",\\n\\t[CPU_SMT_DISABLED]\\t\\t= \"off\",\\n\\t[CPU_SMT_FORCE_DISABLED]\\t= \"forceoff\",\\n\\t[CPU_SMT_NOT_SUPPORTED]\\t\\t= \"notsupported\",\\n};\\n\\nstatic ssize_t\\nshow_smt_control(struct device *dev, struct device_attribute *attr, char *buf)\\n{\\n\\treturn snprintf(buf, PAGE_SIZE - 2, \"%s\\\\n\", smt_states[cpu_smt_control]);\\n}\\n\\nstatic void cpuhp_offline_cpu_device(unsigned int cpu)\\n{\\n\\tstruct device *dev = get_cpu_device(cpu);\\n\\n\\tdev->offline = true;\\n\\t/* Tell user space about the state change */\\n\\tkobject_uevent(&dev->kobj, KOBJ_OFFLINE);\\n}\\n\\nstatic void cpuhp_online_cpu_device(unsigned int cpu)\\n{\\n\\tstruct device *dev = get_cpu_device(cpu);\\n\\n\\tdev->offline = false;\\n\\t/* Tell user space about the state change */\\n\\tkobject_uevent(&dev->kobj, KOBJ_ONLINE);\\n}\\n\\nstatic int cpuhp_smt_disable(enum cpuhp_smt_control ctrlval)\\n{\\n\\tint cpu, ret = 0;\\n\\n\\tcpu_maps_update_begin();\\n\\tfor_each_online_cpu(cpu) {\\n\\t\\tif (topology_is_primary_thread(cpu))\\n\\t\\t\\tcontinue;\\n\\t\\tret = cpu_down_maps_locked(cpu, CPUHP_OFFLINE);\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\t\\t/*\\n\\t\\t * As this needs to hold the cpu maps lock it\\'s impossible\\n\\t\\t * to call device_offline() because that ends up calling\\n\\t\\t * cpu_down() which takes cpu maps lock. cpu maps lock\\n\\t\\t * needs to be held as this might race against in kernel\\n\\t\\t * abusers of the hotplug machinery (thermal management).\\n\\t\\t *\\n\\t\\t * So nothing would update device:offline state. That would\\n\\t\\t * leave the sysfs entry stale and prevent onlining after\\n\\t\\t * smt control has been changed to \\'off\\' again. This is\\n\\t\\t * called under the sysfs hotplug lock, so it is properly\\n\\t\\t * serialized against the regular offline usage.\\n\\t\\t */\\n\\t\\tcpuhp_offline_cpu_device(cpu);\\n\\t}\\n\\tif (!ret)\\n\\t\\tcpu_smt_control = ctrlval;\\n\\tcpu_maps_update_done();\\n\\treturn ret;\\n}\\n\\nstatic int cpuhp_smt_enable(void)\\n{\\n\\tint cpu, ret = 0;\\n\\n\\tcpu_maps_update_begin();\\n\\tcpu_smt_control = CPU_SMT_ENABLED;\\n\\tfor_each_present_cpu(cpu) {\\n\\t\\t/* Skip online CPUs and CPUs on offline nodes */\\n\\t\\tif (cpu_online(cpu) || !node_online(cpu_to_node(cpu)))\\n\\t\\t\\tcontinue;\\n\\t\\tret = _cpu_up(cpu, 0, CPUHP_ONLINE);\\n\\t\\tif (ret)\\n\\t\\t\\tbreak;\\n\\t\\t/* See comment in cpuhp_smt_disable() */\\n\\t\\tcpuhp_online_cpu_device(cpu);\\n\\t}\\n\\tcpu_maps_update_done();\\n\\treturn ret;\\n}\\n\\nstatic ssize_t\\nstore_smt_control(struct device *dev, struct device_attribute *attr,\\n\\t\\t  const char *buf, size_t count)\\n{\\n\\tint ctrlval, ret;\\n\\n\\tif (sysfs_streq(buf, \"on\"))\\n\\t\\tctrlval = CPU_SMT_ENABLED;\\n\\telse if (sysfs_streq(buf, \"off\"))\\n\\t\\tctrlval = CPU_SMT_DISABLED;\\n\\telse if (sysfs_streq(buf, \"forceoff\"))\\n\\t\\tctrlval = CPU_SMT_FORCE_DISABLED;\\n\\telse\\n\\t\\treturn -EINVAL;\\n\\n\\tif (cpu_smt_control == CPU_SMT_FORCE_DISABLED)\\n\\t\\treturn -EPERM;\\n\\n\\tif (cpu_smt_control == CPU_SMT_NOT_SUPPORTED)\\n\\t\\treturn -ENODEV;\\n\\n\\tret = lock_device_hotplug_sysfs();\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tif (ctrlval != cpu_smt_control) {\\n\\t\\tswitch (ctrlval) {\\n\\t\\tcase CPU_SMT_ENABLED:\\n\\t\\t\\tret = cpuhp_smt_enable();\\n\\t\\t\\tbreak;\\n\\t\\tcase CPU_SMT_DISABLED:\\n\\t\\tcase CPU_SMT_FORCE_DISABLED:\\n\\t\\t\\tret = cpuhp_smt_disable(ctrlval);\\n\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\tunlock_device_hotplug();\\n\\treturn ret ? ret : count;\\n}\\nstatic DEVICE_ATTR(control, 0644, show_smt_control, store_smt_control);\\n\\nstatic ssize_t\\nshow_smt_active(struct device *dev, struct device_attribute *attr, char *buf)\\n{\\n\\tbool active = topology_max_smt_threads() > 1;\\n\\n\\treturn snprintf(buf, PAGE_SIZE - 2, \"%d\\\\n\", active);\\n}\\nstatic DEVICE_ATTR(active, 0444, show_smt_active, NULL);\\n\\nstatic struct attribute *cpuhp_smt_attrs[] = {\\n\\t&dev_attr_control.attr,\\n\\t&dev_attr_active.attr,\\n\\tNULL\\n};\\n\\nstatic const struct attribute_group cpuhp_smt_attr_group = {\\n\\t.attrs = cpuhp_smt_attrs,\\n\\t.name = \"smt\",\\n\\tNULL\\n};\\n\\nstatic int __init cpu_smt_state_init(void)\\n{\\n\\treturn sysfs_create_group(&cpu_subsys.dev_root->kobj,\\n\\t\\t\\t\\t  &cpuhp_smt_attr_group);\\n}\\n\\n#else\\nstatic inline int cpu_smt_state_init(void) { return 0; }\\n#endif\\n\\nstatic int __init cpuhp_sysfs_init(void)\\n{\\n\\tint cpu, ret;\\n\\n\\tret = cpu_smt_state_init();\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tret = sysfs_create_group(&cpu_subsys.dev_root->kobj,\\n\\t\\t\\t\\t &cpuhp_cpu_root_attr_group);\\n\\tif (ret)\\n\\t\\treturn ret;\\n\\n\\tfor_each_possible_cpu(cpu) {\\n\\t\\tstruct device *dev = get_cpu_device(cpu);\\n\\n\\t\\tif (!dev)\\n\\t\\t\\tcontinue;\\n\\t\\tret = sysfs_create_group(&dev->kobj, &cpuhp_cpu_attr_group);\\n\\t\\tif (ret)\\n\\t\\t\\treturn ret;\\n\\t}\\n\\treturn 0;\\n}\\ndevice_initcall(cpuhp_sysfs_init);\\n#endif\\n\\n/*\\n * cpu_bit_bitmap[] is a special, \"compressed\" data structure that\\n * represents all NR_CPUS bits binary values of 1<<nr.\\n *\\n * It is used by cpumask_of() to get a constant address to a CPU\\n * mask value that has a single bit set only.\\n */\\n\\n/* cpu_bit_bitmap[0] is empty - so we can back into it */\\n#define MASK_DECLARE_1(x)\\t[x+1][0] = (1UL << (x))\\n#define MASK_DECLARE_2(x)\\tMASK_DECLARE_1(x), MASK_DECLARE_1(x+1)\\n#define MASK_DECLARE_4(x)\\tMASK_DECLARE_2(x), MASK_DECLARE_2(x+2)\\n#define MASK_DECLARE_8(x)\\tMASK_DECLARE_4(x), MASK_DECLARE_4(x+4)\\n\\nconst unsigned long cpu_bit_bitmap[BITS_PER_LONG+1][BITS_TO_LONGS(NR_CPUS)] = {\\n\\n\\tMASK_DECLARE_8(0),\\tMASK_DECLARE_8(8),\\n\\tMASK_DECLARE_8(16),\\tMASK_DECLARE_8(24),\\n#if BITS_PER_LONG > 32\\n\\tMASK_DECLARE_8(32),\\tMASK_DECLARE_8(40),\\n\\tMASK_DECLARE_8(48),\\tMASK_DECLARE_8(56),\\n#endif\\n};\\nEXPORT_SYMBOL_GPL(cpu_bit_bitmap);\\n\\nconst DECLARE_BITMAP(cpu_all_bits, NR_CPUS) = CPU_BITS_ALL;\\nEXPORT_SYMBOL(cpu_all_bits);\\n\\n#ifdef CONFIG_INIT_ALL_POSSIBLE\\nstruct cpumask __cpu_possible_mask __read_mostly\\n\\t= {CPU_BITS_ALL};\\n#else\\nstruct cpumask __cpu_possible_mask __read_mostly;\\n#endif\\nEXPORT_SYMBOL(__cpu_possible_mask);\\n\\nstruct cpumask __cpu_online_mask __read_mostly;\\nEXPORT_SYMBOL(__cpu_online_mask);\\n\\nstruct cpumask __cpu_present_mask __read_mostly;\\nEXPORT_SYMBOL(__cpu_present_mask);\\n\\nstruct cpumask __cpu_active_mask __read_mostly;\\nEXPORT_SYMBOL(__cpu_active_mask);\\n\\nvoid init_cpu_present(const struct cpumask *src)\\n{\\n\\tcpumask_copy(&__cpu_present_mask, src);\\n}\\n\\nvoid init_cpu_possible(const struct cpumask *src)\\n{\\n\\tcpumask_copy(&__cpu_possible_mask, src);\\n}\\n\\nvoid init_cpu_online(const struct cpumask *src)\\n{\\n\\tcpumask_copy(&__cpu_online_mask, src);\\n}\\n\\n/*\\n * Activate the first processor.\\n */\\nvoid __init boot_cpu_init(void)\\n{\\n\\tint cpu = smp_processor_id();\\n\\n\\t/* Mark the boot cpu \"present\", \"online\" etc for SMP and UP case */\\n\\tset_cpu_online(cpu, true);\\n\\tset_cpu_active(cpu, true);\\n\\tset_cpu_present(cpu, true);\\n\\tset_cpu_possible(cpu, true);\\n\\n#ifdef CONFIG_SMP\\n\\t__boot_cpu_id = cpu;\\n#endif\\n}\\n\\n/*\\n * Must be called _AFTER_ setting up the per_cpu areas\\n */\\nvoid __init boot_cpu_hotplug_init(void)\\n{\\n#ifdef CONFIG_SMP\\n\\tthis_cpu_write(cpuhp_state.booted_once, true);\\n#endif\\n\\tthis_cpu_write(cpuhp_state.state, CPUHP_ONLINE);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n#include <linux/elf.h>\\n#include <linux/fs.h>\\n#include <linux/mm.h>\\n#include <linux/binfmts.h>\\n\\nElf_Half __weak elf_core_extra_phdrs(void)\\n{\\n\\treturn 0;\\n}\\n\\nint __weak elf_core_write_extra_phdrs(struct coredump_params *cprm, loff_t offset)\\n{\\n\\treturn 1;\\n}\\n\\nint __weak elf_core_write_extra_data(struct coredump_params *cprm)\\n{\\n\\treturn 1;\\n}\\n\\nsize_t __weak elf_core_extra_data_size(void)\\n{\\n\\treturn 0;\\n}\\n\\n/* delayacct.c - per-task delay accounting\\n *\\n * Copyright (C) Shailabh Nagar, IBM Corp. 2006\\n *\\n * This program is free software;  you can redistribute it and/or modify\\n * it under the terms of the GNU General Public License as published by\\n * the Free Software Foundation; either version 2 of the License, or\\n * (at your option) any later version.\\n *\\n * This program is distributed in the hope that it would be useful, but\\n * WITHOUT ANY WARRANTY; without even the implied warranty of\\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See\\n * the GNU General Public License for more details.\\n */\\n\\n#include <linux/sched.h>\\n#include <linux/sched/task.h>\\n#include <linux/sched/cputime.h>\\n#include <linux/slab.h>\\n#include <linux/taskstats.h>\\n#include <linux/time.h>\\n#include <linux/sysctl.h>\\n#include <linux/delayacct.h>\\n#include <linux/module.h>\\n\\nint delayacct_on __read_mostly = 1;\\t/* Delay accounting turned on/off */\\nEXPORT_SYMBOL_GPL(delayacct_on);\\nstruct kmem_cache *delayacct_cache;\\n\\nstatic int __init delayacct_setup_disable(char *str)\\n{\\n\\tdelayacct_on = 0;\\n\\treturn 1;\\n}\\n__setup(\"nodelayacct\", delayacct_setup_disable);\\n\\nvoid delayacct_init(void)\\n{\\n\\tdelayacct_cache = KMEM_CACHE(task_delay_info, SLAB_PANIC|SLAB_ACCOUNT);\\n\\tdelayacct_tsk_init(&init_task);\\n}\\n\\nvoid __delayacct_tsk_init(struct task_struct *tsk)\\n{\\n\\ttsk->delays = kmem_cache_zalloc(delayacct_cache, GFP_KERNEL);\\n\\tif (tsk->delays)\\n\\t\\traw_spin_lock_init(&tsk->delays->lock);\\n}\\n\\n/*\\n * Finish delay accounting for a statistic using its timestamps (@start),\\n * accumalator (@total) and @count\\n */\\nstatic void delayacct_end(raw_spinlock_t *lock, u64 *start, u64 *total,\\n\\t\\t\\t  u32 *count)\\n{\\n\\ts64 ns = ktime_get_ns() - *start;\\n\\tunsigned long flags;\\n\\n\\tif (ns > 0) {\\n\\t\\traw_spin_lock_irqsave(lock, flags);\\n\\t\\t*total += ns;\\n\\t\\t(*count)++;\\n\\t\\traw_spin_unlock_irqrestore(lock, flags);\\n\\t}\\n}\\n\\nvoid __delayacct_blkio_start(void)\\n{\\n\\tcurrent->delays->blkio_start = ktime_get_ns();\\n}\\n\\n/*\\n * We cannot rely on the `current` macro, as we haven\\'t yet switched back to\\n * the process being woken.\\n */\\nvoid __delayacct_blkio_end(struct task_struct *p)\\n{\\n\\tstruct task_delay_info *delays = p->delays;\\n\\tu64 *total;\\n\\tu32 *count;\\n\\n\\tif (p->delays->flags & DELAYACCT_PF_SWAPIN) {\\n\\t\\ttotal = &delays->swapin_delay;\\n\\t\\tcount = &delays->swapin_count;\\n\\t} else {\\n\\t\\ttotal = &delays->blkio_delay;\\n\\t\\tcount = &delays->blkio_count;\\n\\t}\\n\\n\\tdelayacct_end(&delays->lock, &delays->blkio_start, total, count);\\n}\\n\\nint __delayacct_add_tsk(struct taskstats *d, struct task_struct *tsk)\\n{\\n\\tu64 utime, stime, stimescaled, utimescaled;\\n\\tunsigned long long t2, t3;\\n\\tunsigned long flags, t1;\\n\\ts64 tmp;\\n\\n\\ttask_cputime(tsk, &utime, &stime);\\n\\ttmp = (s64)d->cpu_run_real_total;\\n\\ttmp += utime + stime;\\n\\td->cpu_run_real_total = (tmp < (s64)d->cpu_run_real_total) ? 0 : tmp;\\n\\n\\ttask_cputime_scaled(tsk, &utimescaled, &stimescaled);\\n\\ttmp = (s64)d->cpu_scaled_run_real_total;\\n\\ttmp += utimescaled + stimescaled;\\n\\td->cpu_scaled_run_real_total =\\n\\t\\t(tmp < (s64)d->cpu_scaled_run_real_total) ? 0 : tmp;\\n\\n\\t/*\\n\\t * No locking available for sched_info (and too expensive to add one)\\n\\t * Mitigate by taking snapshot of values\\n\\t */\\n\\tt1 = tsk->sched_info.pcount;\\n\\tt2 = tsk->sched_info.run_delay;\\n\\tt3 = tsk->se.sum_exec_runtime;\\n\\n\\td->cpu_count += t1;\\n\\n\\ttmp = (s64)d->cpu_delay_total + t2;\\n\\td->cpu_delay_total = (tmp < (s64)d->cpu_delay_total) ? 0 : tmp;\\n\\n\\ttmp = (s64)d->cpu_run_virtual_total + t3;\\n\\td->cpu_run_virtual_total =\\n\\t\\t(tmp < (s64)d->cpu_run_virtual_total) ?\\t0 : tmp;\\n\\n\\t/* zero XXX_total, non-zero XXX_count implies XXX stat overflowed */\\n\\n\\traw_spin_lock_irqsave(&tsk->delays->lock, flags);\\n\\ttmp = d->blkio_delay_total + tsk->delays->blkio_delay;\\n\\td->blkio_delay_total = (tmp < d->blkio_delay_total) ? 0 : tmp;\\n\\ttmp = d->swapin_delay_total + tsk->delays->swapin_delay;\\n\\td->swapin_delay_total = (tmp < d->swapin_delay_total) ? 0 : tmp;\\n\\ttmp = d->freepages_delay_total + tsk->delays->freepages_delay;\\n\\td->freepages_delay_total = (tmp < d->freepages_delay_total) ? 0 : tmp;\\n\\ttmp = d->thrashing_delay_total + tsk->delays->thrashing_delay;\\n\\td->thrashing_delay_total = (tmp < d->thrashing_delay_total) ? 0 : tmp;\\n\\td->blkio_count += tsk->delays->blkio_count;\\n\\td->swapin_count += tsk->delays->swapin_count;\\n\\td->freepages_count += tsk->delays->freepages_count;\\n\\td->thrashing_count += tsk->delays->thrashing_count;\\n\\traw_spin_unlock_irqrestore(&tsk->delays->lock, flags);\\n\\n\\treturn 0;\\n}\\n\\n__u64 __delayacct_blkio_ticks(struct task_struct *tsk)\\n{\\n\\t__u64 ret;\\n\\tunsigned long flags;\\n\\n\\traw_spin_lock_irqsave(&tsk->delays->lock, flags);\\n\\tret = nsec_to_clock_t(tsk->delays->blkio_delay +\\n\\t\\t\\t\\ttsk->delays->swapin_delay);\\n\\traw_spin_unlock_irqrestore(&tsk->delays->lock, flags);\\n\\treturn ret;\\n}\\n\\nvoid __delayacct_freepages_start(void)\\n{\\n\\tcurrent->delays->freepages_start = ktime_get_ns();\\n}\\n\\nvoid __delayacct_freepages_end(void)\\n{\\n\\tdelayacct_end(\\n\\t\\t&current->delays->lock,\\n\\t\\t&current->delays->freepages_start,\\n\\t\\t&current->delays->freepages_delay,\\n\\t\\t&current->delays->freepages_count);\\n}\\n\\nvoid __delayacct_thrashing_start(void)\\n{\\n\\tcurrent->delays->thrashing_start = ktime_get_ns();\\n}\\n\\nvoid __delayacct_thrashing_end(void)\\n{\\n\\tdelayacct_end(&current->delays->lock,\\n\\t\\t      &current->delays->thrashing_start,\\n\\t\\t      &current->delays->thrashing_delay,\\n\\t\\t      &current->delays->thrashing_count);\\n}\\n\\n// SPDX-License-Identifier: GPL-2.0\\n#define pr_fmt(fmt) \"kcov: \" fmt\\n\\n#define DISABLE_BRANCH_PROFILING\\n#include <linux/atomic.h>\\n#include <linux/compiler.h>\\n#include <linux/errno.h>\\n#include <linux/export.h>\\n#include <linux/types.h>\\n#include <linux/file.h>\\n#include <linux/fs.h>\\n#include <linux/init.h>\\n#include <linux/mm.h>\\n#include <linux/preempt.h>\\n#include <linux/printk.h>\\n#include <linux/sched.h>\\n#include <linux/slab.h>\\n#include <linux/spinlock.h>\\n#include <linux/vmalloc.h>\\n#include <linux/debugfs.h>\\n#include <linux/uaccess.h>\\n#include <linux/kcov.h>\\n#include <asm/setup.h>\\n\\n/* Number of 64-bit words written per one comparison: */\\n#define KCOV_WORDS_PER_CMP 4\\n\\n/*\\n * kcov descriptor (one per opened debugfs file).\\n * State transitions of the descriptor:\\n *  - initial state after open()\\n *  - then there must be a single ioctl(KCOV_INIT_TRACE) call\\n *  - then, mmap() call (several calls are allowed but not useful)\\n *  - then, ioctl(KCOV_ENABLE, arg), where arg is\\n *\\tKCOV_TRACE_PC - to trace only the PCs\\n *\\tor\\n *\\tKCOV_TRACE_CMP - to trace only the comparison operands\\n *  - then, ioctl(KCOV_DISABLE) to disable the task.\\n * Enabling/disabling ioctls can be repeated (only one task a time allowed).\\n */\\nstruct kcov {\\n\\t/*\\n\\t * Reference counter. We keep one for:\\n\\t *  - opened file descriptor\\n\\t *  - task with enabled coverage (we can\\'t unwire it from another task)\\n\\t */\\n\\tatomic_t\\t\\trefcount;\\n\\t/* The lock protects mode, size, area and t. */\\n\\tspinlock_t\\t\\tlock;\\n\\tenum kcov_mode\\t\\tmode;\\n\\t/* Size of arena (in long\\'s for KCOV_MODE_TRACE). */\\n\\tunsigned\\t\\tsize;\\n\\t/* Coverage buffer shared with user space. */\\n\\tvoid\\t\\t\\t*area;\\n\\t/* Task for which we collect coverage, or NULL. */\\n\\tstruct task_struct\\t*t;\\n};\\n\\nstatic notrace bool check_kcov_mode(enum kcov_mode needed_mode, struct task_struct *t)\\n{\\n\\tunsigned int mode;\\n\\n\\t/*\\n\\t * We are interested in code coverage as a function of a syscall inputs,\\n\\t * so we ignore code executed in interrupts.\\n\\t */\\n\\tif (!in_task())\\n\\t\\treturn false;\\n\\tmode = READ_ONCE(t->kcov_mode);\\n\\t/*\\n\\t * There is some code that runs in interrupts but for which\\n\\t * in_interrupt() returns false (e.g. preempt_schedule_irq()).\\n\\t * READ_ONCE()/barrier() effectively provides load-acquire wrt\\n\\t * interrupts, there are paired barrier()/WRITE_ONCE() in\\n\\t * kcov_ioctl_locked().\\n\\t */\\n\\tbarrier();\\n\\treturn mode == needed_mode;\\n}\\n\\nstatic notrace unsigned long canonicalize_ip(unsigned long ip)\\n{\\n#ifdef CONFIG_RANDOMIZE_BASE\\n\\tip -= kaslr_offset();\\n#endif\\n\\treturn ip;\\n}\\n\\n/*\\n * Entry point from instrumented code.\\n * This is called once per basic-block/edge.\\n */\\nvoid notrace __sanitizer_cov_trace_pc(void)\\n{\\n\\tstruct task_struct *t;\\n\\tunsigned long *area;\\n\\tunsigned long ip = canonicalize_ip(_RET_IP_);\\n\\tunsigned long pos;\\n\\n\\tt = current;\\n\\tif (!check_kcov_mode(KCOV_MODE_TRACE_PC, t))\\n\\t\\treturn;\\n\\n\\tarea = t->kcov_area;\\n\\t/* The first 64-bit word is the number of subsequent PCs. */\\n\\tpos = READ_ONCE(area[0]) + 1;\\n\\tif (likely(pos < t->kcov_size)) {\\n\\t\\tarea[pos] = ip;\\n\\t\\tWRITE_ONCE(area[0], pos);\\n\\t}\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_pc);\\n\\n#ifdef CONFIG_KCOV_ENABLE_COMPARISONS\\nstatic void notrace write_comp_data(u64 type, u64 arg1, u64 arg2, u64 ip)\\n{\\n\\tstruct task_struct *t;\\n\\tu64 *area;\\n\\tu64 count, start_index, end_pos, max_pos;\\n\\n\\tt = current;\\n\\tif (!check_kcov_mode(KCOV_MODE_TRACE_CMP, t))\\n\\t\\treturn;\\n\\n\\tip = canonicalize_ip(ip);\\n\\n\\t/*\\n\\t * We write all comparison arguments and types as u64.\\n\\t * The buffer was allocated for t->kcov_size unsigned longs.\\n\\t */\\n\\tarea = (u64 *)t->kcov_area;\\n\\tmax_pos = t->kcov_size * sizeof(unsigned long);\\n\\n\\tcount = READ_ONCE(area[0]);\\n\\n\\t/* Every record is KCOV_WORDS_PER_CMP 64-bit words. */\\n\\tstart_index = 1 + count * KCOV_WORDS_PER_CMP;\\n\\tend_pos = (start_index + KCOV_WORDS_PER_CMP) * sizeof(u64);\\n\\tif (likely(end_pos <= max_pos)) {\\n\\t\\tarea[start_index] = type;\\n\\t\\tarea[start_index + 1] = arg1;\\n\\t\\tarea[start_index + 2] = arg2;\\n\\t\\tarea[start_index + 3] = ip;\\n\\t\\tWRITE_ONCE(area[0], count + 1);\\n\\t}\\n}\\n\\nvoid notrace __sanitizer_cov_trace_cmp1(u8 arg1, u8 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(0), arg1, arg2, _RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_cmp1);\\n\\nvoid notrace __sanitizer_cov_trace_cmp2(u16 arg1, u16 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(1), arg1, arg2, _RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_cmp2);\\n\\nvoid notrace __sanitizer_cov_trace_cmp4(u32 arg1, u32 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(2), arg1, arg2, _RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_cmp4);\\n\\nvoid notrace __sanitizer_cov_trace_cmp8(u64 arg1, u64 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(3), arg1, arg2, _RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_cmp8);\\n\\nvoid notrace __sanitizer_cov_trace_const_cmp1(u8 arg1, u8 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(0) | KCOV_CMP_CONST, arg1, arg2,\\n\\t\\t\\t_RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp1);\\n\\nvoid notrace __sanitizer_cov_trace_const_cmp2(u16 arg1, u16 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(1) | KCOV_CMP_CONST, arg1, arg2,\\n\\t\\t\\t_RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp2);\\n\\nvoid notrace __sanitizer_cov_trace_const_cmp4(u32 arg1, u32 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(2) | KCOV_CMP_CONST, arg1, arg2,\\n\\t\\t\\t_RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp4);\\n\\nvoid notrace __sanitizer_cov_trace_const_cmp8(u64 arg1, u64 arg2)\\n{\\n\\twrite_comp_data(KCOV_CMP_SIZE(3) | KCOV_CMP_CONST, arg1, arg2,\\n\\t\\t\\t_RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_const_cmp8);\\n\\nvoid notrace __sanitizer_cov_trace_switch(u64 val, u64 *cases)\\n{\\n\\tu64 i;\\n\\tu64 count = cases[0];\\n\\tu64 size = cases[1];\\n\\tu64 type = KCOV_CMP_CONST;\\n\\n\\tswitch (size) {\\n\\tcase 8:\\n\\t\\ttype |= KCOV_CMP_SIZE(0);\\n\\t\\tbreak;\\n\\tcase 16:\\n\\t\\ttype |= KCOV_CMP_SIZE(1);\\n\\t\\tbreak;\\n\\tcase 32:\\n\\t\\ttype |= KCOV_CMP_SIZE(2);\\n\\t\\tbreak;\\n\\tcase 64:\\n\\t\\ttype |= KCOV_CMP_SIZE(3);\\n\\t\\tbreak;\\n\\tdefault:\\n\\t\\treturn;\\n\\t}\\n\\tfor (i = 0; i < count; i++)\\n\\t\\twrite_comp_data(type, cases[i + 2], val, _RET_IP_);\\n}\\nEXPORT_SYMBOL(__sanitizer_cov_trace_switch);\\n#endif /* ifdef CONFIG_KCOV_ENABLE_COMPARISONS */\\n\\nstatic void kcov_get(struct kcov *kcov)\\n{\\n\\tatomic_inc(&kcov->refcount);\\n}\\n\\nstatic void kcov_put(struct kcov *kcov)\\n{\\n\\tif (atomic_dec_and_test(&kcov->refcount)) {\\n\\t\\tvfree(kcov->area);\\n\\t\\tkfree(kcov);\\n\\t}\\n}\\n\\nvoid kcov_task_init(struct task_struct *t)\\n{\\n\\tWRITE_ONCE(t->kcov_mode, KCOV_MODE_DISABLED);\\n\\tbarrier();\\n\\tt->kcov_size = 0;\\n\\tt->kcov_area = NULL;\\n\\tt->kcov = NULL;\\n}\\n\\nvoid kcov_task_exit(struct task_struct *t)\\n{\\n\\tstruct kcov *kcov;\\n\\n\\tkcov = t->kcov;\\n\\tif (kcov == NULL)\\n\\t\\treturn;\\n\\tspin_lock(&kcov->lock);\\n\\tif (WARN_ON(kcov->t != t)) {\\n\\t\\tspin_unlock(&kcov->lock);\\n\\t\\treturn;\\n\\t}\\n\\t/* Just to not leave dangling references behind. */\\n\\tkcov_task_init(t);\\n\\tkcov->t = NULL;\\n\\tkcov->mode = KCOV_MODE_INIT;\\n\\tspin_unlock(&kcov->lock);\\n\\tkcov_put(kcov);\\n}\\n\\nstatic int kcov_mmap(struct file *filep, struct vm_area_struct *vma)\\n{\\n\\tint res = 0;\\n\\tvoid *area;\\n\\tstruct kcov *kcov = vma->vm_file->private_data;\\n\\tunsigned long size, off;\\n\\tstruct page *page;\\n\\n\\tarea = vmalloc_user(vma->vm_end - vma->vm_start);\\n\\tif (!area)\\n\\t\\treturn -ENOMEM;\\n\\n\\tspin_lock(&kcov->lock);\\n\\tsize = kcov->size * sizeof(unsigned long);\\n\\tif (kcov->mode != KCOV_MODE_INIT || vma->vm_pgoff != 0 ||\\n\\t    vma->vm_end - vma->vm_start != size) {\\n\\t\\tres = -EINVAL;\\n\\t\\tgoto exit;\\n\\t}\\n\\tif (!kcov->area) {\\n\\t\\tkcov->area = area;\\n\\t\\tvma->vm_flags |= VM_DONTEXPAND;\\n\\t\\tspin_unlock(&kcov->lock);\\n\\t\\tfor (off = 0; off < size; off += PAGE_SIZE) {\\n\\t\\t\\tpage = vmalloc_to_page(kcov->area + off);\\n\\t\\t\\tif (vm_insert_page(vma, vma->vm_start + off, page))\\n\\t\\t\\t\\tWARN_ONCE(1, \"vm_insert_page() failed\");\\n\\t\\t}\\n\\t\\treturn 0;\\n\\t}\\nexit:\\n\\tspin_unlock(&kcov->lock);\\n\\tvfree(area);\\n\\treturn res;\\n}\\n\\nstatic int kcov_open(struct inode *inode, struct file *filep)\\n{\\n\\tstruct kcov *kcov;\\n\\n\\tkcov = kzalloc(sizeof(*kcov), GFP_KERNEL);\\n\\tif (!kcov)\\n\\t\\treturn -ENOMEM;\\n\\tkcov->mode = KCOV_MODE_DISABLED;\\n\\tatomic_set(&kcov->refcount, 1);\\n\\tspin_lock_init(&kcov->lock);\\n\\tfilep->private_data = kcov;\\n\\treturn nonseekable_open(inode, filep);\\n}\\n\\nstatic int kcov_close(struct inode *inode, struct file *filep)\\n{\\n\\tkcov_put(filep->private_data);\\n\\treturn 0;\\n}\\n\\n/*\\n * Fault in a lazily-faulted vmalloc area before it can be used by\\n * __santizer_cov_trace_pc(), to avoid recursion issues if any code on the\\n * vmalloc fault handling path is instrumented.\\n */\\nstatic void kcov_fault_in_area(struct kcov *kcov)\\n{\\n\\tunsigned long stride = PAGE_SIZE / sizeof(unsigned long);\\n\\tunsigned long *area = kcov->area;\\n\\tunsigned long offset;\\n\\n\\tfor (offset = 0; offset < kcov->size; offset += stride)\\n\\t\\tREAD_ONCE(area[offset]);\\n}\\n\\nstatic int kcov_ioctl_locked(struct kcov *kcov, unsigned int cmd,\\n\\t\\t\\t     unsigned long arg)\\n{\\n\\tstruct task_struct *t;\\n\\tunsigned long size, unused;\\n\\n\\tswitch (cmd) {\\n\\tcase KCOV_INIT_TRACE:\\n\\t\\t/*\\n\\t\\t * Enable kcov in trace mode and setup buffer size.\\n\\t\\t * Must happen before anything else.\\n\\t\\t */\\n\\t\\tif (kcov->mode != KCOV_MODE_DISABLED)\\n\\t\\t\\treturn -EBUSY;\\n\\t\\t/*\\n\\t\\t * Size must be at least 2 to hold current position and one PC.\\n\\t\\t * Later we allocate size * sizeof(unsigned long) memory,\\n\\t\\t * that must not overflow.\\n\\t\\t */\\n\\t\\tsize = arg;\\n\\t\\tif (size < 2 || size > INT_MAX / sizeof(unsigned long))\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tkcov->size = size;\\n\\t\\tkcov->mode = KCOV_MODE_INIT;\\n\\t\\treturn 0;\\n\\tcase KCOV_ENABLE:\\n\\t\\t/*\\n\\t\\t * Enable coverage for the current task.\\n\\t\\t * At this point user must have been enabled trace mode,\\n\\t\\t * and mmapped the file. Coverage collection is disabled only\\n\\t\\t * at task exit or voluntary by KCOV_DISABLE. After that it can\\n\\t\\t * be enabled for another task.\\n\\t\\t */\\n\\t\\tif (kcov->mode != KCOV_MODE_INIT || !kcov->area)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tt = current;\\n\\t\\tif (kcov->t != NULL || t->kcov != NULL)\\n\\t\\t\\treturn -EBUSY;\\n\\t\\tif (arg == KCOV_TRACE_PC)\\n\\t\\t\\tkcov->mode = KCOV_MODE_TRACE_PC;\\n\\t\\telse if (arg == KCOV_TRACE_CMP)\\n#ifdef CONFIG_KCOV_ENABLE_COMPARISONS\\n\\t\\t\\tkcov->mode = KCOV_MODE_TRACE_CMP;\\n#else\\n\\t\\treturn -ENOTSUPP;\\n#endif\\n\\t\\telse\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tkcov_fault_in_area(kcov);\\n\\t\\t/* Cache in task struct for performance. */\\n\\t\\tt->kcov_size = kcov->size;\\n\\t\\tt->kcov_area = kcov->area;\\n\\t\\t/* See comment in check_kcov_mode(). */\\n\\t\\tbarrier();\\n\\t\\tWRITE_ONCE(t->kcov_mode, kcov->mode);\\n\\t\\tt->kcov = kcov;\\n\\t\\tkcov->t = t;\\n\\t\\t/* This is put either in kcov_task_exit() or in KCOV_DISABLE. */\\n\\t\\tkcov_get(kcov);\\n\\t\\treturn 0;\\n\\tcase KCOV_DISABLE:\\n\\t\\t/* Disable coverage for the current task. */\\n\\t\\tunused = arg;\\n\\t\\tif (unused != 0 || current->kcov != kcov)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tt = current;\\n\\t\\tif (WARN_ON(kcov->t != t))\\n\\t\\t\\treturn -EINVAL;\\n\\t\\tkcov_task_init(t);\\n\\t\\tkcov->t = NULL;\\n\\t\\tkcov->mode = KCOV_MODE_INIT;\\n\\t\\tkcov_put(kcov);\\n\\t\\treturn 0;\\n\\tdefault:\\n\\t\\treturn -ENOTTY;\\n\\t}\\n}\\n\\nstatic long kcov_ioctl(struct file *filep, unsigned int cmd, unsigned long arg)\\n{\\n\\tstruct kcov *kcov;\\n\\tint res;\\n\\n\\tkcov = filep->private_data;\\n\\tspin_lock(&kcov->lock);\\n\\tres = kcov_ioctl_locked(kcov, cmd, arg);\\n\\tspin_unlock(&kcov->lock);\\n\\treturn res;\\n}\\n\\nstatic const struct file_operations kcov_fops = {\\n\\t.open\\t\\t= kcov_open,\\n\\t.unlocked_ioctl\\t= kcov_ioctl,\\n\\t.compat_ioctl\\t= kcov_ioctl,\\n\\t.mmap\\t\\t= kcov_mmap,\\n\\t.release        = kcov_close,\\n};\\n\\nstatic int __init kcov_init(void)\\n{\\n\\t/*\\n\\t * The kcov debugfs file won\\'t ever get removed and thus,\\n\\t * there is no need to protect it against removal races. The\\n\\t * use of debugfs_create_file_unsafe() is actually safe here.\\n\\t */\\n\\tif (!debugfs_create_file_unsafe(\"kcov\", 0600, NULL, NULL, &kcov_fops)) {\\n\\t\\tpr_err(\"failed to create kcov in debugfs\\\\n\");\\n\\t\\treturn -ENOMEM;\\n\\t}\\n\\treturn 0;\\n}\\n\\ndevice_initcall(kcov_init);\\n\\n/*\\n * sysctl.c: General linux system control interface\\n *\\n * Begun 24 March 1995, Stephen Tweedie\\n * Added /proc support, Dec 1995\\n * Added bdflush entry and intvec min/max checking, 2/23/96, Tom Dyas.\\n * Added hooks for /proc/sys/net (minor, minor patch), 96/4/1, Mike Shaver.\\n * Added kernel/java-{interpreter,appletviewer}, 96/5/10, Mike Shaver.\\n * Dynamic registration fixes, Stephen Tweedie.\\n * Added kswapd-interval, ctrl-alt-del, printk stuff, 1/8/97, Chris Horn.\\n * Made sysctl support optional via CONFIG_SYSCTL, 1/10/97, Chris\\n *  Horn.\\n * Added proc_doulongvec_ms_jiffies_minmax, 09/08/99, Carlos H. Bauer.\\n * Added proc_doulongvec_minmax, 09/08/99, Carlos H. Bauer.\\n * Changed linked lists to use list.h instead of lists.h, 02/24/00, Bill\\n *  Wendling.\\n * The list_for_each() macro wasn\\'t appropriate for the sysctl loop.\\n *  Removed it and replaced it with older style, 03/23/00, Bill Wendling\\n */\\n\\n#include <linux/module.h>\\n#include <linux/aio.h>\\n#include <linux/mm.h>\\n#include <linux/swap.h>\\n#include <linux/slab.h>\\n#include <linux/sysctl.h>\\n#include <linux/bitmap.h>\\n#include <linux/signal.h>\\n#include <linux/printk.h>\\n#include <linux/proc_fs.h>\\n#include <linux/security.h>\\n#include <linux/ctype.h>\\n#include <linux/kmemleak.h>\\n#include <linux/fs.h>\\n#include <linux/init.h>\\n#include <linux/kernel.h>\\n#include <linux/kobject.h>\\n#include <linux/net.h>\\n#include <linux/sysrq.h>\\n#include <linux/highuid.h>\\n#include <linux/writeback.h>\\n#include <linux/ratelimit.h>\\n#include <linux/compaction.h>\\n#include <linux/hugetlb.h>\\n#include <linux/initrd.h>\\n#include <linux/key.h>\\n#include <linux/times.h>\\n#include <linux/limits.h>\\n#include <linux/dcache.h>\\n#include <linux/dnotify.h>\\n#include <linux/syscalls.h>\\n#include <linux/vmstat.h>\\n#include <linux/nfs_fs.h>\\n#include <linux/acpi.h>\\n#include <linux/reboot.h>\\n#include <linux/ftrace.h>\\n#include <linux/perf_event.h>\\n#include <linux/kprobes.h>\\n#include <linux/pipe_fs_i.h>\\n#include <linux/oom.h>\\n#include <linux/kmod.h>\\n#include <linux/capability.h>\\n#include <linux/binfmts.h>\\n#include <linux/sched/sysctl.h>\\n#include <linux/sched/coredump.h>\\n#include <linux/kexec.h>\\n#include <linux/bpf.h>\\n#include <linux/mount.h>\\n\\n#include <linux/uaccess.h>\\n#include <asm/processor.h>\\n\\n#ifdef CONFIG_X86\\n#include <asm/nmi.h>\\n#include <asm/stacktrace.h>\\n#include <asm/io.h>\\n#endif\\n#ifdef CONFIG_SPARC\\n#include <asm/setup.h>\\n#endif\\n#ifdef CONFIG_BSD_PROCESS_ACCT\\n#include <linux/acct.h>\\n#endif\\n#ifdef CONFIG_RT_MUTEXES\\n#include <linux/rtmutex.h>\\n#endif\\n#if defined(CONFIG_PROVE_LOCKING) || defined(CONFIG_LOCK_STAT)\\n#include <linux/lockdep.h>\\n#endif\\n#ifdef CONFIG_CHR_DEV_SG\\n#include <scsi/sg.h>\\n#endif\\n#ifdef CONFIG_STACKLEAK_RUNTIME_DISABLE\\n#include <linux/stackleak.h>\\n#endif\\n#ifdef CONFIG_LOCKUP_DETECTOR\\n#include <linux/nmi.h>\\n#endif\\n\\n#if defined(CONFIG_SYSCTL)\\n\\n/* External variables not in a header file. */\\nextern int suid_dumpable;\\n#ifdef CONFIG_COREDUMP\\nextern int core_uses_pid;\\nextern char core_pattern[];\\nextern unsigned int core_pipe_limit;\\n#endif\\nextern int pid_max;\\nextern int pid_max_min, pid_max_max;\\nextern int percpu_pagelist_fraction;\\nextern int latencytop_enabled;\\nextern unsigned int sysctl_nr_open_min, sysctl_nr_open_max;\\n#ifndef CONFIG_MMU\\nextern int sysctl_nr_trim_pages;\\n#endif\\n\\n/* Constants used for minimum and  maximum */\\n#ifdef CONFIG_LOCKUP_DETECTOR\\nstatic int sixty = 60;\\n#endif\\n\\nstatic int __maybe_unused neg_one = -1;\\n\\nstatic int zero;\\nstatic int __maybe_unused one = 1;\\nstatic int __maybe_unused two = 2;\\nstatic int __maybe_unused four = 4;\\nstatic unsigned long one_ul = 1;\\nstatic int one_hundred = 100;\\nstatic int one_thousand = 1000;\\n#ifdef CONFIG_PRINTK\\nstatic int ten_thousand = 10000;\\n#endif\\n#ifdef CONFIG_PERF_EVENTS\\nstatic int six_hundred_forty_kb = 640 * 1024;\\n#endif\\n\\n/* this is needed for the proc_doulongvec_minmax of vm_dirty_bytes */\\nstatic unsigned long dirty_bytes_min = 2 * PAGE_SIZE;\\n\\n/* this is needed for the proc_dointvec_minmax for [fs_]overflow UID and GID */\\nstatic int maxolduid = 65535;\\nstatic int minolduid;\\n\\nstatic int ngroups_max = NGROUPS_MAX;\\nstatic const int cap_last_cap = CAP_LAST_CAP;\\n\\n/*\\n * This is needed for proc_doulongvec_minmax of sysctl_hung_task_timeout_secs\\n * and hung_task_check_interval_secs\\n */\\n#ifdef CONFIG_DETECT_HUNG_TASK\\nstatic unsigned long hung_task_timeout_max = (LONG_MAX/HZ);\\n#endif\\n\\n#ifdef CONFIG_INOTIFY_USER\\n#include <linux/inotify.h>\\n#endif\\n#ifdef CONFIG_SPARC\\n#endif\\n\\n#ifdef __hppa__\\nextern int pwrsw_enabled;\\n#endif\\n\\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_ALLOW\\nextern int unaligned_enabled;\\n#endif\\n\\n#ifdef CONFIG_IA64\\nextern int unaligned_dump_stack;\\n#endif\\n\\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_NO_WARN\\nextern int no_unaligned_warning;\\n#endif\\n\\n#ifdef CONFIG_PROC_SYSCTL\\n\\n/**\\n * enum sysctl_writes_mode - supported sysctl write modes\\n *\\n * @SYSCTL_WRITES_LEGACY: each write syscall must fully contain the sysctl value\\n * \\tto be written, and multiple writes on the same sysctl file descriptor\\n * \\twill rewrite the sysctl value, regardless of file position. No warning\\n * \\tis issued when the initial position is not 0.\\n * @SYSCTL_WRITES_WARN: same as above but warn when the initial file position is\\n * \\tnot 0.\\n * @SYSCTL_WRITES_STRICT: writes to numeric sysctl entries must always be at\\n * \\tfile position 0 and the value must be fully contained in the buffer\\n * \\tsent to the write syscall. If dealing with strings respect the file\\n * \\tposition, but restrict this to the max length of the buffer, anything\\n * \\tpassed the max lenght will be ignored. Multiple writes will append\\n * \\tto the buffer.\\n *\\n * These write modes control how current file position affects the behavior of\\n * updating sysctl values through the proc interface on each write.\\n */\\nenum sysctl_writes_mode {\\n\\tSYSCTL_WRITES_LEGACY\\t\\t= -1,\\n\\tSYSCTL_WRITES_WARN\\t\\t= 0,\\n\\tSYSCTL_WRITES_STRICT\\t\\t= 1,\\n};\\n\\nstatic enum sysctl_writes_mode sysctl_writes_strict = SYSCTL_WRITES_STRICT;\\n\\nstatic int proc_do_cad_pid(struct ctl_table *table, int write,\\n\\t\\t  void __user *buffer, size_t *lenp, loff_t *ppos);\\nstatic int proc_taint(struct ctl_table *table, int write,\\n\\t\\t\\t       void __user *buffer, size_t *lenp, loff_t *ppos);\\n#endif\\n\\n#ifdef CONFIG_PRINTK\\nstatic int proc_dointvec_minmax_sysadmin(struct ctl_table *table, int write,\\n\\t\\t\\t\\tvoid __user *buffer, size_t *lenp, loff_t *ppos);\\n#endif\\n\\nstatic int proc_dointvec_minmax_coredump(struct ctl_table *table, int write,\\n\\t\\tvoid __user *buffer, size_t *lenp, loff_t *ppos);\\n#ifdef CONFIG_COREDUMP\\nstatic int proc_dostring_coredump(struct ctl_table *table, int write,\\n\\t\\tvoid __user *buffer, size_t *lenp, loff_t *ppos);\\n#endif\\nstatic int proc_dopipe_max_size(struct ctl_table *table, int write,\\n\\t\\tvoid __user *buffer, size_t *lenp, loff_t *ppos);\\n\\n#ifdef CONFIG_MAGIC_SYSRQ\\n/* Note: sysrq code uses its own private copy */\\nstatic int __sysrq_enabled = CONFIG_MAGIC_SYSRQ_DEFAULT_ENABLE;\\n\\nstatic int sysrq_sysctl_handler(struct ctl_table *table, int write,\\n\\t\\t\\t\\tvoid __user *buffer, size_t *lenp,\\n\\t\\t\\t\\tloff_t *ppos)\\n{\\n\\tint error;\\n\\n\\terror = proc_dointvec(table, write, buffer, lenp, ppos);\\n\\tif (error)\\n\\t\\treturn error;\\n\\n\\tif (write)\\n\\t\\tsysrq_toggle_support(__sysrq_enabled);\\n\\n\\treturn 0;\\n}\\n\\n#endif\\n\\nstatic struct ctl_table kern_table[];\\nstatic struct ctl_table vm_table[];\\nstatic struct ctl_table fs_table[];\\nstatic struct ctl_table debug_table[];\\nstatic struct ctl_table dev_table[];\\nextern struct ctl_table random_table[];\\n#ifdef CONFIG_EPOLL\\nextern struct ctl_table epoll_table[];\\n#endif\\n\\n#ifdef CONFIG_FW_LOADER_USER_HELPER\\nextern struct ctl_table firmware_config_table[];\\n#endif\\n\\n#ifdef HAVE_ARCH_PICK_MMAP_LAYOUT\\nint sysctl_legacy_va_layout;\\n#endif\\n\\n/* The default sysctl tables: */\\n\\nstatic struct ctl_table sysctl_base_table[] = {\\n\\t{\\n\\t\\t.procname\\t= \"kernel\",\\n\\t\\t.mode\\t\\t= 0555,\\n\\t\\t.child\\t\\t= kern_table,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"vm\",\\n\\t\\t.mode\\t\\t= 0555,\\n\\t\\t.child\\t\\t= vm_table,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"fs\",\\n\\t\\t.mode\\t\\t= 0555,\\n\\t\\t.child\\t\\t= fs_table,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"debug\",\\n\\t\\t.mode\\t\\t= 0555,\\n\\t\\t.child\\t\\t= debug_table,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"dev\",\\n\\t\\t.mode\\t\\t= 0555,\\n\\t\\t.child\\t\\t= dev_table,\\n\\t},\\n\\t{ }\\n};\\n\\n#ifdef CONFIG_SCHED_DEBUG\\nstatic int min_sched_granularity_ns = 100000;\\t\\t/* 100 usecs */\\nstatic int max_sched_granularity_ns = NSEC_PER_SEC;\\t/* 1 second */\\nstatic int min_wakeup_granularity_ns;\\t\\t\\t/* 0 usecs */\\nstatic int max_wakeup_granularity_ns = NSEC_PER_SEC;\\t/* 1 second */\\n#ifdef CONFIG_SMP\\nstatic int min_sched_tunable_scaling = SCHED_TUNABLESCALING_NONE;\\nstatic int max_sched_tunable_scaling = SCHED_TUNABLESCALING_END-1;\\n#endif /* CONFIG_SMP */\\n#endif /* CONFIG_SCHED_DEBUG */\\n\\n#ifdef CONFIG_COMPACTION\\nstatic int min_extfrag_threshold;\\nstatic int max_extfrag_threshold = 1000;\\n#endif\\n\\nstatic struct ctl_table kern_table[] = {\\n\\t{\\n\\t\\t.procname\\t= \"sched_child_runs_first\",\\n\\t\\t.data\\t\\t= &sysctl_sched_child_runs_first,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#ifdef CONFIG_SCHED_DEBUG\\n\\t{\\n\\t\\t.procname\\t= \"sched_min_granularity_ns\",\\n\\t\\t.data\\t\\t= &sysctl_sched_min_granularity,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sched_proc_update_handler,\\n\\t\\t.extra1\\t\\t= &min_sched_granularity_ns,\\n\\t\\t.extra2\\t\\t= &max_sched_granularity_ns,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"sched_latency_ns\",\\n\\t\\t.data\\t\\t= &sysctl_sched_latency,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sched_proc_update_handler,\\n\\t\\t.extra1\\t\\t= &min_sched_granularity_ns,\\n\\t\\t.extra2\\t\\t= &max_sched_granularity_ns,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"sched_wakeup_granularity_ns\",\\n\\t\\t.data\\t\\t= &sysctl_sched_wakeup_granularity,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sched_proc_update_handler,\\n\\t\\t.extra1\\t\\t= &min_wakeup_granularity_ns,\\n\\t\\t.extra2\\t\\t= &max_wakeup_granularity_ns,\\n\\t},\\n#ifdef CONFIG_SMP\\n\\t{\\n\\t\\t.procname\\t= \"sched_tunable_scaling\",\\n\\t\\t.data\\t\\t= &sysctl_sched_tunable_scaling,\\n\\t\\t.maxlen\\t\\t= sizeof(enum sched_tunable_scaling),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sched_proc_update_handler,\\n\\t\\t.extra1\\t\\t= &min_sched_tunable_scaling,\\n\\t\\t.extra2\\t\\t= &max_sched_tunable_scaling,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"sched_migration_cost_ns\",\\n\\t\\t.data\\t\\t= &sysctl_sched_migration_cost,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"sched_nr_migrate\",\\n\\t\\t.data\\t\\t= &sysctl_sched_nr_migrate,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#ifdef CONFIG_SCHEDSTATS\\n\\t{\\n\\t\\t.procname\\t= \"sched_schedstats\",\\n\\t\\t.data\\t\\t= NULL,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sysctl_schedstats,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif /* CONFIG_SCHEDSTATS */\\n#endif /* CONFIG_SMP */\\n#ifdef CONFIG_NUMA_BALANCING\\n\\t{\\n\\t\\t.procname\\t= \"numa_balancing_scan_delay_ms\",\\n\\t\\t.data\\t\\t= &sysctl_numa_balancing_scan_delay,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"numa_balancing_scan_period_min_ms\",\\n\\t\\t.data\\t\\t= &sysctl_numa_balancing_scan_period_min,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"numa_balancing_scan_period_max_ms\",\\n\\t\\t.data\\t\\t= &sysctl_numa_balancing_scan_period_max,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"numa_balancing_scan_size_mb\",\\n\\t\\t.data\\t\\t= &sysctl_numa_balancing_scan_size,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &one,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"numa_balancing\",\\n\\t\\t.data\\t\\t= NULL, /* filled in by handler */\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sysctl_numa_balancing,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif /* CONFIG_NUMA_BALANCING */\\n#endif /* CONFIG_SCHED_DEBUG */\\n\\t{\\n\\t\\t.procname\\t= \"sched_rt_period_us\",\\n\\t\\t.data\\t\\t= &sysctl_sched_rt_period,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sched_rt_handler,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"sched_rt_runtime_us\",\\n\\t\\t.data\\t\\t= &sysctl_sched_rt_runtime,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sched_rt_handler,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"sched_rr_timeslice_ms\",\\n\\t\\t.data\\t\\t= &sysctl_sched_rr_timeslice,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sched_rr_handler,\\n\\t},\\n#ifdef CONFIG_SCHED_AUTOGROUP\\n\\t{\\n\\t\\t.procname\\t= \"sched_autogroup_enabled\",\\n\\t\\t.data\\t\\t= &sysctl_sched_autogroup_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif\\n#ifdef CONFIG_CFS_BANDWIDTH\\n\\t{\\n\\t\\t.procname\\t= \"sched_cfs_bandwidth_slice_us\",\\n\\t\\t.data\\t\\t= &sysctl_sched_cfs_bandwidth_slice,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &one,\\n\\t},\\n#endif\\n#ifdef CONFIG_PROVE_LOCKING\\n\\t{\\n\\t\\t.procname\\t= \"prove_locking\",\\n\\t\\t.data\\t\\t= &prove_locking,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_LOCK_STAT\\n\\t{\\n\\t\\t.procname\\t= \"lock_stat\",\\n\\t\\t.data\\t\\t= &lock_stat,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"panic\",\\n\\t\\t.data\\t\\t= &panic_timeout,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#ifdef CONFIG_COREDUMP\\n\\t{\\n\\t\\t.procname\\t= \"core_uses_pid\",\\n\\t\\t.data\\t\\t= &core_uses_pid,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"core_pattern\",\\n\\t\\t.data\\t\\t= core_pattern,\\n\\t\\t.maxlen\\t\\t= CORENAME_MAX_SIZE,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dostring_coredump,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"core_pipe_limit\",\\n\\t\\t.data\\t\\t= &core_pipe_limit,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_PROC_SYSCTL\\n\\t{\\n\\t\\t.procname\\t= \"tainted\",\\n\\t\\t.maxlen \\t= sizeof(long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_taint,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"sysctl_writes_strict\",\\n\\t\\t.data\\t\\t= &sysctl_writes_strict,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &neg_one,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif\\n#ifdef CONFIG_LATENCYTOP\\n\\t{\\n\\t\\t.procname\\t= \"latencytop\",\\n\\t\\t.data\\t\\t= &latencytop_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sysctl_latencytop,\\n\\t},\\n#endif\\n#ifdef CONFIG_BLK_DEV_INITRD\\n\\t{\\n\\t\\t.procname\\t= \"real-root-dev\",\\n\\t\\t.data\\t\\t= &real_root_dev,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"print-fatal-signals\",\\n\\t\\t.data\\t\\t= &print_fatal_signals,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#ifdef CONFIG_SPARC\\n\\t{\\n\\t\\t.procname\\t= \"reboot-cmd\",\\n\\t\\t.data\\t\\t= reboot_command,\\n\\t\\t.maxlen\\t\\t= 256,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dostring,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"stop-a\",\\n\\t\\t.data\\t\\t= &stop_a_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"scons-poweroff\",\\n\\t\\t.data\\t\\t= &scons_pwroff,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_SPARC64\\n\\t{\\n\\t\\t.procname\\t= \"tsb-ratio\",\\n\\t\\t.data\\t\\t= &sysctl_tsb_ratio,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef __hppa__\\n\\t{\\n\\t\\t.procname\\t= \"soft-power\",\\n\\t\\t.data\\t\\t= &pwrsw_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t \\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_ALLOW\\n\\t{\\n\\t\\t.procname\\t= \"unaligned-trap\",\\n\\t\\t.data\\t\\t= &unaligned_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"ctrl-alt-del\",\\n\\t\\t.data\\t\\t= &C_A_D,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#ifdef CONFIG_FUNCTION_TRACER\\n\\t{\\n\\t\\t.procname\\t= \"ftrace_enabled\",\\n\\t\\t.data\\t\\t= &ftrace_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= ftrace_enable_sysctl,\\n\\t},\\n#endif\\n#ifdef CONFIG_STACK_TRACER\\n\\t{\\n\\t\\t.procname\\t= \"stack_tracer_enabled\",\\n\\t\\t.data\\t\\t= &stack_tracer_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= stack_trace_sysctl,\\n\\t},\\n#endif\\n#ifdef CONFIG_TRACING\\n\\t{\\n\\t\\t.procname\\t= \"ftrace_dump_on_oops\",\\n\\t\\t.data\\t\\t= &ftrace_dump_on_oops,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"traceoff_on_warning\",\\n\\t\\t.data\\t\\t= &__disable_trace_on_warning,\\n\\t\\t.maxlen\\t\\t= sizeof(__disable_trace_on_warning),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"tracepoint_printk\",\\n\\t\\t.data\\t\\t= &tracepoint_printk,\\n\\t\\t.maxlen\\t\\t= sizeof(tracepoint_printk),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= tracepoint_printk_sysctl,\\n\\t},\\n#endif\\n#ifdef CONFIG_KEXEC_CORE\\n\\t{\\n\\t\\t.procname\\t= \"kexec_load_disabled\",\\n\\t\\t.data\\t\\t= &kexec_load_disabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t/* only handle a transition from default \"0\" to \"1\" */\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &one,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif\\n#ifdef CONFIG_MODULES\\n\\t{\\n\\t\\t.procname\\t= \"modprobe\",\\n\\t\\t.data\\t\\t= &modprobe_path,\\n\\t\\t.maxlen\\t\\t= KMOD_PATH_LEN,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dostring,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"modules_disabled\",\\n\\t\\t.data\\t\\t= &modules_disabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t/* only handle a transition from default \"0\" to \"1\" */\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &one,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif\\n#ifdef CONFIG_UEVENT_HELPER\\n\\t{\\n\\t\\t.procname\\t= \"hotplug\",\\n\\t\\t.data\\t\\t= &uevent_helper,\\n\\t\\t.maxlen\\t\\t= UEVENT_HELPER_PATH_LEN,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dostring,\\n\\t},\\n#endif\\n#ifdef CONFIG_CHR_DEV_SG\\n\\t{\\n\\t\\t.procname\\t= \"sg-big-buff\",\\n\\t\\t.data\\t\\t= &sg_big_buff,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_BSD_PROCESS_ACCT\\n\\t{\\n\\t\\t.procname\\t= \"acct\",\\n\\t\\t.data\\t\\t= &acct_parm,\\n\\t\\t.maxlen\\t\\t= 3*sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_MAGIC_SYSRQ\\n\\t{\\n\\t\\t.procname\\t= \"sysrq\",\\n\\t\\t.data\\t\\t= &__sysrq_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sysrq_sysctl_handler,\\n\\t},\\n#endif\\n#ifdef CONFIG_PROC_SYSCTL\\n\\t{\\n\\t\\t.procname\\t= \"cad_pid\",\\n\\t\\t.data\\t\\t= NULL,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= proc_do_cad_pid,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"threads-max\",\\n\\t\\t.data\\t\\t= NULL,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sysctl_max_threads,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"random\",\\n\\t\\t.mode\\t\\t= 0555,\\n\\t\\t.child\\t\\t= random_table,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"usermodehelper\",\\n\\t\\t.mode\\t\\t= 0555,\\n\\t\\t.child\\t\\t= usermodehelper_table,\\n\\t},\\n#ifdef CONFIG_FW_LOADER_USER_HELPER\\n\\t{\\n\\t\\t.procname\\t= \"firmware_config\",\\n\\t\\t.mode\\t\\t= 0555,\\n\\t\\t.child\\t\\t= firmware_config_table,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"overflowuid\",\\n\\t\\t.data\\t\\t= &overflowuid,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &minolduid,\\n\\t\\t.extra2\\t\\t= &maxolduid,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"overflowgid\",\\n\\t\\t.data\\t\\t= &overflowgid,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &minolduid,\\n\\t\\t.extra2\\t\\t= &maxolduid,\\n\\t},\\n#ifdef CONFIG_S390\\n#ifdef CONFIG_MATHEMU\\n\\t{\\n\\t\\t.procname\\t= \"ieee_emulation_warnings\",\\n\\t\\t.data\\t\\t= &sysctl_ieee_emulation_warnings,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"userprocess_debug\",\\n\\t\\t.data\\t\\t= &show_unhandled_signals,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"pid_max\",\\n\\t\\t.data\\t\\t= &pid_max,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &pid_max_min,\\n\\t\\t.extra2\\t\\t= &pid_max_max,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"panic_on_oops\",\\n\\t\\t.data\\t\\t= &panic_on_oops,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"panic_print\",\\n\\t\\t.data\\t\\t= &panic_print,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n#if defined CONFIG_PRINTK\\n\\t{\\n\\t\\t.procname\\t= \"printk\",\\n\\t\\t.data\\t\\t= &console_loglevel,\\n\\t\\t.maxlen\\t\\t= 4*sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"printk_ratelimit\",\\n\\t\\t.data\\t\\t= &printk_ratelimit_state.interval,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_jiffies,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"printk_ratelimit_burst\",\\n\\t\\t.data\\t\\t= &printk_ratelimit_state.burst,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"printk_delay\",\\n\\t\\t.data\\t\\t= &printk_delay_msec,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &ten_thousand,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"printk_devkmsg\",\\n\\t\\t.data\\t\\t= devkmsg_log_str,\\n\\t\\t.maxlen\\t\\t= DEVKMSG_STR_MAX_SIZE,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= devkmsg_sysctl_set_loglvl,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"dmesg_restrict\",\\n\\t\\t.data\\t\\t= &dmesg_restrict,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax_sysadmin,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"kptr_restrict\",\\n\\t\\t.data\\t\\t= &kptr_restrict,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax_sysadmin,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &two,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"ngroups_max\",\\n\\t\\t.data\\t\\t= &ngroups_max,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"cap_last_cap\",\\n\\t\\t.data\\t\\t= (void *)&cap_last_cap,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#if defined(CONFIG_LOCKUP_DETECTOR)\\n\\t{\\n\\t\\t.procname       = \"watchdog\",\\n\\t\\t.data\\t\\t= &watchdog_user_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler   = proc_watchdog,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"watchdog_thresh\",\\n\\t\\t.data\\t\\t= &watchdog_thresh,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_watchdog_thresh,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &sixty,\\n\\t},\\n\\t{\\n\\t\\t.procname       = \"nmi_watchdog\",\\n\\t\\t.data\\t\\t= &nmi_watchdog_user_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= NMI_WATCHDOG_SYSCTL_PERM,\\n\\t\\t.proc_handler   = proc_nmi_watchdog,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"watchdog_cpumask\",\\n\\t\\t.data\\t\\t= &watchdog_cpumask_bits,\\n\\t\\t.maxlen\\t\\t= NR_CPUS,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_watchdog_cpumask,\\n\\t},\\n#ifdef CONFIG_SOFTLOCKUP_DETECTOR\\n\\t{\\n\\t\\t.procname       = \"soft_watchdog\",\\n\\t\\t.data\\t\\t= &soft_watchdog_user_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler   = proc_soft_watchdog,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"softlockup_panic\",\\n\\t\\t.data\\t\\t= &softlockup_panic,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#ifdef CONFIG_SMP\\n\\t{\\n\\t\\t.procname\\t= \"softlockup_all_cpu_backtrace\",\\n\\t\\t.data\\t\\t= &sysctl_softlockup_all_cpu_backtrace,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif /* CONFIG_SMP */\\n#endif\\n#ifdef CONFIG_HARDLOCKUP_DETECTOR\\n\\t{\\n\\t\\t.procname\\t= \"hardlockup_panic\",\\n\\t\\t.data\\t\\t= &hardlockup_panic,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#ifdef CONFIG_SMP\\n\\t{\\n\\t\\t.procname\\t= \"hardlockup_all_cpu_backtrace\",\\n\\t\\t.data\\t\\t= &sysctl_hardlockup_all_cpu_backtrace,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif /* CONFIG_SMP */\\n#endif\\n#endif\\n\\n#if defined(CONFIG_X86_LOCAL_APIC) && defined(CONFIG_X86)\\n\\t{\\n\\t\\t.procname       = \"unknown_nmi_panic\",\\n\\t\\t.data           = &unknown_nmi_panic,\\n\\t\\t.maxlen         = sizeof (int),\\n\\t\\t.mode           = 0644,\\n\\t\\t.proc_handler   = proc_dointvec,\\n\\t},\\n#endif\\n#if defined(CONFIG_X86)\\n\\t{\\n\\t\\t.procname\\t= \"panic_on_unrecovered_nmi\",\\n\\t\\t.data\\t\\t= &panic_on_unrecovered_nmi,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"panic_on_io_nmi\",\\n\\t\\t.data\\t\\t= &panic_on_io_nmi,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#ifdef CONFIG_DEBUG_STACKOVERFLOW\\n\\t{\\n\\t\\t.procname\\t= \"panic_on_stackoverflow\",\\n\\t\\t.data\\t\\t= &sysctl_panic_on_stackoverflow,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"bootloader_type\",\\n\\t\\t.data\\t\\t= &bootloader_type,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"bootloader_version\",\\n\\t\\t.data\\t\\t= &bootloader_version,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"io_delay_type\",\\n\\t\\t.data\\t\\t= &io_delay_type,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#if defined(CONFIG_MMU)\\n\\t{\\n\\t\\t.procname\\t= \"randomize_va_space\",\\n\\t\\t.data\\t\\t= &randomize_va_space,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#if defined(CONFIG_S390) && defined(CONFIG_SMP)\\n\\t{\\n\\t\\t.procname\\t= \"spin_retry\",\\n\\t\\t.data\\t\\t= &spin_retry,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#if\\tdefined(CONFIG_ACPI_SLEEP) && defined(CONFIG_X86)\\n\\t{\\n\\t\\t.procname\\t= \"acpi_video_flags\",\\n\\t\\t.data\\t\\t= &acpi_realmode_flags,\\n\\t\\t.maxlen\\t\\t= sizeof (unsigned long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n#endif\\n#ifdef CONFIG_SYSCTL_ARCH_UNALIGN_NO_WARN\\n\\t{\\n\\t\\t.procname\\t= \"ignore-unaligned-usertrap\",\\n\\t\\t.data\\t\\t= &no_unaligned_warning,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t \\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_IA64\\n\\t{\\n\\t\\t.procname\\t= \"unaligned-dump-stack\",\\n\\t\\t.data\\t\\t= &unaligned_dump_stack,\\n\\t\\t.maxlen\\t\\t= sizeof (int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_DETECT_HUNG_TASK\\n\\t{\\n\\t\\t.procname\\t= \"hung_task_panic\",\\n\\t\\t.data\\t\\t= &sysctl_hung_task_panic,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"hung_task_check_count\",\\n\\t\\t.data\\t\\t= &sysctl_hung_task_check_count,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"hung_task_timeout_secs\",\\n\\t\\t.data\\t\\t= &sysctl_hung_task_timeout_secs,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dohung_task_timeout_secs,\\n\\t\\t.extra2\\t\\t= &hung_task_timeout_max,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"hung_task_check_interval_secs\",\\n\\t\\t.data\\t\\t= &sysctl_hung_task_check_interval_secs,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dohung_task_timeout_secs,\\n\\t\\t.extra2\\t\\t= &hung_task_timeout_max,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"hung_task_warnings\",\\n\\t\\t.data\\t\\t= &sysctl_hung_task_warnings,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &neg_one,\\n\\t},\\n#endif\\n#ifdef CONFIG_RT_MUTEXES\\n\\t{\\n\\t\\t.procname\\t= \"max_lock_depth\",\\n\\t\\t.data\\t\\t= &max_lock_depth,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"poweroff_cmd\",\\n\\t\\t.data\\t\\t= &poweroff_cmd,\\n\\t\\t.maxlen\\t\\t= POWEROFF_CMD_PATH_LEN,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dostring,\\n\\t},\\n#ifdef CONFIG_KEYS\\n\\t{\\n\\t\\t.procname\\t= \"keys\",\\n\\t\\t.mode\\t\\t= 0555,\\n\\t\\t.child\\t\\t= key_sysctls,\\n\\t},\\n#endif\\n#ifdef CONFIG_PERF_EVENTS\\n\\t/*\\n\\t * User-space scripts rely on the existence of this file\\n\\t * as a feature check for perf_events being enabled.\\n\\t *\\n\\t * So it\\'s an ABI, do not remove!\\n\\t */\\n\\t{\\n\\t\\t.procname\\t= \"perf_event_paranoid\",\\n\\t\\t.data\\t\\t= &sysctl_perf_event_paranoid,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_perf_event_paranoid),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"perf_event_mlock_kb\",\\n\\t\\t.data\\t\\t= &sysctl_perf_event_mlock,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_perf_event_mlock),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"perf_event_max_sample_rate\",\\n\\t\\t.data\\t\\t= &sysctl_perf_event_sample_rate,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_perf_event_sample_rate),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= perf_proc_update_handler,\\n\\t\\t.extra1\\t\\t= &one,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"perf_cpu_time_max_percent\",\\n\\t\\t.data\\t\\t= &sysctl_perf_cpu_time_max_percent,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_perf_cpu_time_max_percent),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= perf_cpu_time_max_percent_handler,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one_hundred,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"perf_event_max_stack\",\\n\\t\\t.data\\t\\t= &sysctl_perf_event_max_stack,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_perf_event_max_stack),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= perf_event_max_stack_handler,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &six_hundred_forty_kb,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"perf_event_max_contexts_per_stack\",\\n\\t\\t.data\\t\\t= &sysctl_perf_event_max_contexts_per_stack,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_perf_event_max_contexts_per_stack),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= perf_event_max_stack_handler,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one_thousand,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"panic_on_warn\",\\n\\t\\t.data\\t\\t= &panic_on_warn,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#if defined(CONFIG_SMP) && defined(CONFIG_NO_HZ_COMMON)\\n\\t{\\n\\t\\t.procname\\t= \"timer_migration\",\\n\\t\\t.data\\t\\t= &sysctl_timer_migration,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= timer_migration_handler,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif\\n#ifdef CONFIG_BPF_SYSCALL\\n\\t{\\n\\t\\t.procname\\t= \"unprivileged_bpf_disabled\",\\n\\t\\t.data\\t\\t= &sysctl_unprivileged_bpf_disabled,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_unprivileged_bpf_disabled),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t/* only handle a transition from default \"0\" to \"1\" */\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &one,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif\\n#if defined(CONFIG_TREE_RCU) || defined(CONFIG_PREEMPT_RCU)\\n\\t{\\n\\t\\t.procname\\t= \"panic_on_rcu_stall\",\\n\\t\\t.data\\t\\t= &sysctl_panic_on_rcu_stall,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_panic_on_rcu_stall),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif\\n#ifdef CONFIG_STACKLEAK_RUNTIME_DISABLE\\n\\t{\\n\\t\\t.procname\\t= \"stack_erasing\",\\n\\t\\t.data\\t\\t= NULL,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= stack_erasing_sysctl,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif\\n\\t{ }\\n};\\n\\nstatic struct ctl_table vm_table[] = {\\n\\t{\\n\\t\\t.procname\\t= \"overcommit_memory\",\\n\\t\\t.data\\t\\t= &sysctl_overcommit_memory,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_overcommit_memory),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &two,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"panic_on_oom\",\\n\\t\\t.data\\t\\t= &sysctl_panic_on_oom,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_panic_on_oom),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &two,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"oom_kill_allocating_task\",\\n\\t\\t.data\\t\\t= &sysctl_oom_kill_allocating_task,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_oom_kill_allocating_task),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"oom_dump_tasks\",\\n\\t\\t.data\\t\\t= &sysctl_oom_dump_tasks,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_oom_dump_tasks),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"overcommit_ratio\",\\n\\t\\t.data\\t\\t= &sysctl_overcommit_ratio,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_overcommit_ratio),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= overcommit_ratio_handler,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"overcommit_kbytes\",\\n\\t\\t.data\\t\\t= &sysctl_overcommit_kbytes,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_overcommit_kbytes),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= overcommit_kbytes_handler,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"page-cluster\", \\n\\t\\t.data\\t\\t= &page_cluster,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"dirty_background_ratio\",\\n\\t\\t.data\\t\\t= &dirty_background_ratio,\\n\\t\\t.maxlen\\t\\t= sizeof(dirty_background_ratio),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= dirty_background_ratio_handler,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one_hundred,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"dirty_background_bytes\",\\n\\t\\t.data\\t\\t= &dirty_background_bytes,\\n\\t\\t.maxlen\\t\\t= sizeof(dirty_background_bytes),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= dirty_background_bytes_handler,\\n\\t\\t.extra1\\t\\t= &one_ul,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"dirty_ratio\",\\n\\t\\t.data\\t\\t= &vm_dirty_ratio,\\n\\t\\t.maxlen\\t\\t= sizeof(vm_dirty_ratio),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= dirty_ratio_handler,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one_hundred,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"dirty_bytes\",\\n\\t\\t.data\\t\\t= &vm_dirty_bytes,\\n\\t\\t.maxlen\\t\\t= sizeof(vm_dirty_bytes),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= dirty_bytes_handler,\\n\\t\\t.extra1\\t\\t= &dirty_bytes_min,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"dirty_writeback_centisecs\",\\n\\t\\t.data\\t\\t= &dirty_writeback_interval,\\n\\t\\t.maxlen\\t\\t= sizeof(dirty_writeback_interval),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= dirty_writeback_centisecs_handler,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"dirty_expire_centisecs\",\\n\\t\\t.data\\t\\t= &dirty_expire_interval,\\n\\t\\t.maxlen\\t\\t= sizeof(dirty_expire_interval),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"dirtytime_expire_seconds\",\\n\\t\\t.data\\t\\t= &dirtytime_expire_interval,\\n\\t\\t.maxlen\\t\\t= sizeof(dirtytime_expire_interval),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= dirtytime_interval_handler,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"swappiness\",\\n\\t\\t.data\\t\\t= &vm_swappiness,\\n\\t\\t.maxlen\\t\\t= sizeof(vm_swappiness),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one_hundred,\\n\\t},\\n#ifdef CONFIG_HUGETLB_PAGE\\n\\t{\\n\\t\\t.procname\\t= \"nr_hugepages\",\\n\\t\\t.data\\t\\t= NULL,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= hugetlb_sysctl_handler,\\n\\t},\\n#ifdef CONFIG_NUMA\\n\\t{\\n\\t\\t.procname       = \"nr_hugepages_mempolicy\",\\n\\t\\t.data           = NULL,\\n\\t\\t.maxlen         = sizeof(unsigned long),\\n\\t\\t.mode           = 0644,\\n\\t\\t.proc_handler   = &hugetlb_mempolicy_sysctl_handler,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t\\t= \"numa_stat\",\\n\\t\\t.data\\t\\t\\t= &sysctl_vm_numa_stat,\\n\\t\\t.maxlen\\t\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sysctl_vm_numa_stat_handler,\\n\\t\\t.extra1\\t\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t\\t= &one,\\n\\t},\\n#endif\\n\\t {\\n\\t\\t.procname\\t= \"hugetlb_shm_group\",\\n\\t\\t.data\\t\\t= &sysctl_hugetlb_shm_group,\\n\\t\\t.maxlen\\t\\t= sizeof(gid_t),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t },\\n\\t{\\n\\t\\t.procname\\t= \"nr_overcommit_hugepages\",\\n\\t\\t.data\\t\\t= NULL,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= hugetlb_overcommit_handler,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"lowmem_reserve_ratio\",\\n\\t\\t.data\\t\\t= &sysctl_lowmem_reserve_ratio,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_lowmem_reserve_ratio),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= lowmem_reserve_ratio_sysctl_handler,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"drop_caches\",\\n\\t\\t.data\\t\\t= &sysctl_drop_caches,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= drop_caches_sysctl_handler,\\n\\t\\t.extra1\\t\\t= &one,\\n\\t\\t.extra2\\t\\t= &four,\\n\\t},\\n#ifdef CONFIG_COMPACTION\\n\\t{\\n\\t\\t.procname\\t= \"compact_memory\",\\n\\t\\t.data\\t\\t= &sysctl_compact_memory,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0200,\\n\\t\\t.proc_handler\\t= sysctl_compaction_handler,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"extfrag_threshold\",\\n\\t\\t.data\\t\\t= &sysctl_extfrag_threshold,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sysctl_extfrag_handler,\\n\\t\\t.extra1\\t\\t= &min_extfrag_threshold,\\n\\t\\t.extra2\\t\\t= &max_extfrag_threshold,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"compact_unevictable_allowed\",\\n\\t\\t.data\\t\\t= &sysctl_compact_unevictable_allowed,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n\\n#endif /* CONFIG_COMPACTION */\\n\\t{\\n\\t\\t.procname\\t= \"min_free_kbytes\",\\n\\t\\t.data\\t\\t= &min_free_kbytes,\\n\\t\\t.maxlen\\t\\t= sizeof(min_free_kbytes),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= min_free_kbytes_sysctl_handler,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"watermark_boost_factor\",\\n\\t\\t.data\\t\\t= &watermark_boost_factor,\\n\\t\\t.maxlen\\t\\t= sizeof(watermark_boost_factor),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= watermark_boost_factor_sysctl_handler,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"watermark_scale_factor\",\\n\\t\\t.data\\t\\t= &watermark_scale_factor,\\n\\t\\t.maxlen\\t\\t= sizeof(watermark_scale_factor),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= watermark_scale_factor_sysctl_handler,\\n\\t\\t.extra1\\t\\t= &one,\\n\\t\\t.extra2\\t\\t= &one_thousand,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"percpu_pagelist_fraction\",\\n\\t\\t.data\\t\\t= &percpu_pagelist_fraction,\\n\\t\\t.maxlen\\t\\t= sizeof(percpu_pagelist_fraction),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= percpu_pagelist_fraction_sysctl_handler,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n#ifdef CONFIG_MMU\\n\\t{\\n\\t\\t.procname\\t= \"max_map_count\",\\n\\t\\t.data\\t\\t= &sysctl_max_map_count,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_max_map_count),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n#else\\n\\t{\\n\\t\\t.procname\\t= \"nr_trim_pages\",\\n\\t\\t.data\\t\\t= &sysctl_nr_trim_pages,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_nr_trim_pages),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"laptop_mode\",\\n\\t\\t.data\\t\\t= &laptop_mode,\\n\\t\\t.maxlen\\t\\t= sizeof(laptop_mode),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_jiffies,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"block_dump\",\\n\\t\\t.data\\t\\t= &block_dump,\\n\\t\\t.maxlen\\t\\t= sizeof(block_dump),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"vfs_cache_pressure\",\\n\\t\\t.data\\t\\t= &sysctl_vfs_cache_pressure,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_vfs_cache_pressure),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n#ifdef HAVE_ARCH_PICK_MMAP_LAYOUT\\n\\t{\\n\\t\\t.procname\\t= \"legacy_va_layout\",\\n\\t\\t.data\\t\\t= &sysctl_legacy_va_layout,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_legacy_va_layout),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n#endif\\n#ifdef CONFIG_NUMA\\n\\t{\\n\\t\\t.procname\\t= \"zone_reclaim_mode\",\\n\\t\\t.data\\t\\t= &node_reclaim_mode,\\n\\t\\t.maxlen\\t\\t= sizeof(node_reclaim_mode),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"min_unmapped_ratio\",\\n\\t\\t.data\\t\\t= &sysctl_min_unmapped_ratio,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_min_unmapped_ratio),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sysctl_min_unmapped_ratio_sysctl_handler,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one_hundred,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"min_slab_ratio\",\\n\\t\\t.data\\t\\t= &sysctl_min_slab_ratio,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_min_slab_ratio),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= sysctl_min_slab_ratio_sysctl_handler,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one_hundred,\\n\\t},\\n#endif\\n#ifdef CONFIG_SMP\\n\\t{\\n\\t\\t.procname\\t= \"stat_interval\",\\n\\t\\t.data\\t\\t= &sysctl_stat_interval,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_stat_interval),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_jiffies,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"stat_refresh\",\\n\\t\\t.data\\t\\t= NULL,\\n\\t\\t.maxlen\\t\\t= 0,\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= vmstat_refresh,\\n\\t},\\n#endif\\n#ifdef CONFIG_MMU\\n\\t{\\n\\t\\t.procname\\t= \"mmap_min_addr\",\\n\\t\\t.data\\t\\t= &dac_mmap_min_addr,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned long),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= mmap_min_addr_handler,\\n\\t},\\n#endif\\n#ifdef CONFIG_NUMA\\n\\t{\\n\\t\\t.procname\\t= \"numa_zonelist_order\",\\n\\t\\t.data\\t\\t= &numa_zonelist_order,\\n\\t\\t.maxlen\\t\\t= NUMA_ZONELIST_ORDER_LEN,\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= numa_zonelist_order_handler,\\n\\t},\\n#endif\\n#if (defined(CONFIG_X86_32) && !defined(CONFIG_UML))|| \\\\\\n   (defined(CONFIG_SUPERH) && defined(CONFIG_VSYSCALL))\\n\\t{\\n\\t\\t.procname\\t= \"vdso_enabled\",\\n#ifdef CONFIG_X86_32\\n\\t\\t.data\\t\\t= &vdso32_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(vdso32_enabled),\\n#else\\n\\t\\t.data\\t\\t= &vdso_enabled,\\n\\t\\t.maxlen\\t\\t= sizeof(vdso_enabled),\\n#endif\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t},\\n#endif\\n#ifdef CONFIG_HIGHMEM\\n\\t{\\n\\t\\t.procname\\t= \"highmem_is_dirtyable\",\\n\\t\\t.data\\t\\t= &vm_highmem_is_dirtyable,\\n\\t\\t.maxlen\\t\\t= sizeof(vm_highmem_is_dirtyable),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif\\n#ifdef CONFIG_MEMORY_FAILURE\\n\\t{\\n\\t\\t.procname\\t= \"memory_failure_early_kill\",\\n\\t\\t.data\\t\\t= &sysctl_memory_failure_early_kill,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_memory_failure_early_kill),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"memory_failure_recovery\",\\n\\t\\t.data\\t\\t= &sysctl_memory_failure_recovery,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_memory_failure_recovery),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"user_reserve_kbytes\",\\n\\t\\t.data\\t\\t= &sysctl_user_reserve_kbytes,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_user_reserve_kbytes),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"admin_reserve_kbytes\",\\n\\t\\t.data\\t\\t= &sysctl_admin_reserve_kbytes,\\n\\t\\t.maxlen\\t\\t= sizeof(sysctl_admin_reserve_kbytes),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS\\n\\t{\\n\\t\\t.procname\\t= \"mmap_rnd_bits\",\\n\\t\\t.data\\t\\t= &mmap_rnd_bits,\\n\\t\\t.maxlen\\t\\t= sizeof(mmap_rnd_bits),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= (void *)&mmap_rnd_bits_min,\\n\\t\\t.extra2\\t\\t= (void *)&mmap_rnd_bits_max,\\n\\t},\\n#endif\\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS\\n\\t{\\n\\t\\t.procname\\t= \"mmap_rnd_compat_bits\",\\n\\t\\t.data\\t\\t= &mmap_rnd_compat_bits,\\n\\t\\t.maxlen\\t\\t= sizeof(mmap_rnd_compat_bits),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= (void *)&mmap_rnd_compat_bits_min,\\n\\t\\t.extra2\\t\\t= (void *)&mmap_rnd_compat_bits_max,\\n\\t},\\n#endif\\n\\t{ }\\n};\\n\\nstatic struct ctl_table fs_table[] = {\\n\\t{\\n\\t\\t.procname\\t= \"inode-nr\",\\n\\t\\t.data\\t\\t= &inodes_stat,\\n\\t\\t.maxlen\\t\\t= 2*sizeof(long),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_nr_inodes,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"inode-state\",\\n\\t\\t.data\\t\\t= &inodes_stat,\\n\\t\\t.maxlen\\t\\t= 7*sizeof(long),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_nr_inodes,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"file-nr\",\\n\\t\\t.data\\t\\t= &files_stat,\\n\\t\\t.maxlen\\t\\t= sizeof(files_stat),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_nr_files,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"file-max\",\\n\\t\\t.data\\t\\t= &files_stat.max_files,\\n\\t\\t.maxlen\\t\\t= sizeof(files_stat.max_files),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"nr_open\",\\n\\t\\t.data\\t\\t= &sysctl_nr_open,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &sysctl_nr_open_min,\\n\\t\\t.extra2\\t\\t= &sysctl_nr_open_max,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"dentry-state\",\\n\\t\\t.data\\t\\t= &dentry_stat,\\n\\t\\t.maxlen\\t\\t= 6*sizeof(long),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_nr_dentry,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"overflowuid\",\\n\\t\\t.data\\t\\t= &fs_overflowuid,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &minolduid,\\n\\t\\t.extra2\\t\\t= &maxolduid,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"overflowgid\",\\n\\t\\t.data\\t\\t= &fs_overflowgid,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &minolduid,\\n\\t\\t.extra2\\t\\t= &maxolduid,\\n\\t},\\n#ifdef CONFIG_FILE_LOCKING\\n\\t{\\n\\t\\t.procname\\t= \"leases-enable\",\\n\\t\\t.data\\t\\t= &leases_enable,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_DNOTIFY\\n\\t{\\n\\t\\t.procname\\t= \"dir-notify-enable\",\\n\\t\\t.data\\t\\t= &dir_notify_enable,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_MMU\\n#ifdef CONFIG_FILE_LOCKING\\n\\t{\\n\\t\\t.procname\\t= \"lease-break-time\",\\n\\t\\t.data\\t\\t= &lease_break_time,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec,\\n\\t},\\n#endif\\n#ifdef CONFIG_AIO\\n\\t{\\n\\t\\t.procname\\t= \"aio-nr\",\\n\\t\\t.data\\t\\t= &aio_nr,\\n\\t\\t.maxlen\\t\\t= sizeof(aio_nr),\\n\\t\\t.mode\\t\\t= 0444,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"aio-max-nr\",\\n\\t\\t.data\\t\\t= &aio_max_nr,\\n\\t\\t.maxlen\\t\\t= sizeof(aio_max_nr),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n#endif /* CONFIG_AIO */\\n#ifdef CONFIG_INOTIFY_USER\\n\\t{\\n\\t\\t.procname\\t= \"inotify\",\\n\\t\\t.mode\\t\\t= 0555,\\n\\t\\t.child\\t\\t= inotify_table,\\n\\t},\\n#endif\\t\\n#ifdef CONFIG_EPOLL\\n\\t{\\n\\t\\t.procname\\t= \"epoll\",\\n\\t\\t.mode\\t\\t= 0555,\\n\\t\\t.child\\t\\t= epoll_table,\\n\\t},\\n#endif\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"protected_symlinks\",\\n\\t\\t.data\\t\\t= &sysctl_protected_symlinks,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"protected_hardlinks\",\\n\\t\\t.data\\t\\t= &sysctl_protected_hardlinks,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"protected_fifos\",\\n\\t\\t.data\\t\\t= &sysctl_protected_fifos,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &two,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"protected_regular\",\\n\\t\\t.data\\t\\t= &sysctl_protected_regular,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0600,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &two,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"suid_dumpable\",\\n\\t\\t.data\\t\\t= &suid_dumpable,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax_coredump,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &two,\\n\\t},\\n#if defined(CONFIG_BINFMT_MISC) || defined(CONFIG_BINFMT_MISC_MODULE)\\n\\t{\\n\\t\\t.procname\\t= \"binfmt_misc\",\\n\\t\\t.mode\\t\\t= 0555,\\n\\t\\t.child\\t\\t= sysctl_mount_point,\\n\\t},\\n#endif\\n\\t{\\n\\t\\t.procname\\t= \"pipe-max-size\",\\n\\t\\t.data\\t\\t= &pipe_max_size,\\n\\t\\t.maxlen\\t\\t= sizeof(pipe_max_size),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dopipe_max_size,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"pipe-user-pages-hard\",\\n\\t\\t.data\\t\\t= &pipe_user_pages_hard,\\n\\t\\t.maxlen\\t\\t= sizeof(pipe_user_pages_hard),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"pipe-user-pages-soft\",\\n\\t\\t.data\\t\\t= &pipe_user_pages_soft,\\n\\t\\t.maxlen\\t\\t= sizeof(pipe_user_pages_soft),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_doulongvec_minmax,\\n\\t},\\n\\t{\\n\\t\\t.procname\\t= \"mount-max\",\\n\\t\\t.data\\t\\t= &sysctl_mount_max,\\n\\t\\t.maxlen\\t\\t= sizeof(unsigned int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec_minmax,\\n\\t\\t.extra1\\t\\t= &one,\\n\\t},\\n\\t{ }\\n};\\n\\nstatic struct ctl_table debug_table[] = {\\n#ifdef CONFIG_SYSCTL_EXCEPTION_TRACE\\n\\t{\\n\\t\\t.procname\\t= \"exception-trace\",\\n\\t\\t.data\\t\\t= &show_unhandled_signals,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_dointvec\\n\\t},\\n#endif\\n#if defined(CONFIG_OPTPROBES)\\n\\t{\\n\\t\\t.procname\\t= \"kprobes-optimization\",\\n\\t\\t.data\\t\\t= &sysctl_kprobes_optimization,\\n\\t\\t.maxlen\\t\\t= sizeof(int),\\n\\t\\t.mode\\t\\t= 0644,\\n\\t\\t.proc_handler\\t= proc_kprobes_optimization_handler,\\n\\t\\t.extra1\\t\\t= &zero,\\n\\t\\t.extra2\\t\\t= &one,\\n\\t},\\n#endif\\n\\t{ }\\n};\\n\\nstatic struct ctl_table dev_table[] = {\\n\\t{ }\\n};\\n\\nint __init sysctl_init(void)\\n{\\n\\tstruct ctl_table_header *hdr;\\n\\n\\thdr = register_sysctl_table(sysctl_base_table);\\n\\tkmemleak_not_leak(hdr);\\n\\treturn 0;\\n}\\n\\n#endif /* CONFIG_SYSCTL */\\n\\n/*\\n * /proc/sys support\\n */\\n\\n#ifdef CONFIG_PROC_SYSCTL\\n\\nstatic int _proc_do_string(char *data, int maxlen, int write,\\n\\t\\t\\t   char __user *buffer,\\n\\t\\t\\t   size_t *lenp, loff_t *ppos)\\n{\\n\\tsize_t len;\\n\\tchar __user *p;\\n\\tchar c;\\n\\n\\tif (!data || !maxlen || !*lenp) {\\n\\t\\t*lenp = 0;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\tif (write) {\\n\\t\\tif (sysctl_writes_strict == SYSCTL_WRITES_STRICT) {\\n\\t\\t\\t/* Only continue writes not past the end of buffer. */\\n\\t\\t\\tlen = strlen(data);\\n\\t\\t\\tif (len > maxlen - 1)\\n\\t\\t\\t\\tlen = maxlen - 1;\\n\\n\\t\\t\\tif (*ppos > len)\\n\\t\\t\\t\\treturn 0;\\n\\t\\t\\tlen = *ppos;\\n\\t\\t} else {\\n\\t\\t\\t/* Start writing from beginning of buffer. */\\n\\t\\t\\tlen = 0;\\n\\t\\t}\\n\\n\\t\\t*ppos += *lenp;\\n\\t\\tp = buffer;\\n\\t\\twhile ((p - buffer) < *lenp && len < maxlen - 1) {\\n\\t\\t\\tif (get_user(c, p++))\\n\\t\\t\\t\\treturn -EFAULT;\\n\\t\\t\\tif (c == 0 || c == \\'\\\\n\\')\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tdata[len++] = c;\\n\\t\\t}\\n\\t\\tdata[len] = 0;\\n\\t} else {\\n\\t\\tlen = strlen(data);\\n\\t\\tif (len > maxlen)\\n\\t\\t\\tlen = maxlen;\\n\\n\\t\\tif (*ppos > len) {\\n\\t\\t\\t*lenp = 0;\\n\\t\\t\\treturn 0;\\n\\t\\t}\\n\\n\\t\\tdata += *ppos;\\n\\t\\tlen  -= *ppos;\\n\\n\\t\\tif (len > *lenp)\\n\\t\\t\\tlen = *lenp;\\n\\t\\tif (len)\\n\\t\\t\\tif (copy_to_user(buffer, data, len))\\n\\t\\t\\t\\treturn -EFAULT;\\n\\t\\tif (len < *lenp) {\\n\\t\\t\\tif (put_user(\\'\\\\n\\', buffer + len))\\n\\t\\t\\t\\treturn -EFAULT;\\n\\t\\t\\tlen++;\\n\\t\\t}\\n\\t\\t*lenp = len;\\n\\t\\t*ppos += len;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic void warn_sysctl_write(struct ctl_table *table)\\n{\\n\\tpr_warn_once(\"%s wrote to %s when file position was not 0!\\\\n\"\\n\\t\\t\"This will not be supported in the future. To silence this\\\\n\"\\n\\t\\t\"warning, set kernel.sysctl_writes_strict = -1\\\\n\",\\n\\t\\tcurrent->comm, table->procname);\\n}\\n\\n/**\\n * proc_first_pos_non_zero_ignore - check if first position is allowed\\n * @ppos: file position\\n * @table: the sysctl table\\n *\\n * Returns true if the first position is non-zero and the sysctl_writes_strict\\n * mode indicates this is not allowed for numeric input types. String proc\\n * handlers can ignore the return value.\\n */\\nstatic bool proc_first_pos_non_zero_ignore(loff_t *ppos,\\n\\t\\t\\t\\t\\t   struct ctl_table *table)\\n{\\n\\tif (!*ppos)\\n\\t\\treturn false;\\n\\n\\tswitch (sysctl_writes_strict) {\\n\\tcase SYSCTL_WRITES_STRICT:\\n\\t\\treturn true;\\n\\tcase SYSCTL_WRITES_WARN:\\n\\t\\twarn_sysctl_write(table);\\n\\t\\treturn false;\\n\\tdefault:\\n\\t\\treturn false;\\n\\t}\\n}\\n\\n/**\\n * proc_dostring - read a string sysctl\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * Reads/writes a string from/to the user buffer. If the kernel\\n * buffer provided is not large enough to hold the string, the\\n * string is truncated. The copied string is %NULL-terminated.\\n * If the string is being read by the user process, it is copied\\n * and a newline \\'\\\\n\\' is added. It is truncated if the buffer is\\n * not large enough.\\n *\\n * Returns 0 on success.\\n */\\nint proc_dostring(struct ctl_table *table, int write,\\n\\t\\t  void __user *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tif (write)\\n\\t\\tproc_first_pos_non_zero_ignore(ppos, table);\\n\\n\\treturn _proc_do_string((char *)(table->data), table->maxlen, write,\\n\\t\\t\\t       (char __user *)buffer, lenp, ppos);\\n}\\n\\nstatic size_t proc_skip_spaces(char **buf)\\n{\\n\\tsize_t ret;\\n\\tchar *tmp = skip_spaces(*buf);\\n\\tret = tmp - *buf;\\n\\t*buf = tmp;\\n\\treturn ret;\\n}\\n\\nstatic void proc_skip_char(char **buf, size_t *size, const char v)\\n{\\n\\twhile (*size) {\\n\\t\\tif (**buf != v)\\n\\t\\t\\tbreak;\\n\\t\\t(*size)--;\\n\\t\\t(*buf)++;\\n\\t}\\n}\\n\\n#define TMPBUFLEN 22\\n/**\\n * proc_get_long - reads an ASCII formatted integer from a user buffer\\n *\\n * @buf: a kernel buffer\\n * @size: size of the kernel buffer\\n * @val: this is where the number will be stored\\n * @neg: set to %TRUE if number is negative\\n * @perm_tr: a vector which contains the allowed trailers\\n * @perm_tr_len: size of the perm_tr vector\\n * @tr: pointer to store the trailer character\\n *\\n * In case of success %0 is returned and @buf and @size are updated with\\n * the amount of bytes read. If @tr is non-NULL and a trailing\\n * character exists (size is non-zero after returning from this\\n * function), @tr is updated with the trailing character.\\n */\\nstatic int proc_get_long(char **buf, size_t *size,\\n\\t\\t\\t  unsigned long *val, bool *neg,\\n\\t\\t\\t  const char *perm_tr, unsigned perm_tr_len, char *tr)\\n{\\n\\tint len;\\n\\tchar *p, tmp[TMPBUFLEN];\\n\\n\\tif (!*size)\\n\\t\\treturn -EINVAL;\\n\\n\\tlen = *size;\\n\\tif (len > TMPBUFLEN - 1)\\n\\t\\tlen = TMPBUFLEN - 1;\\n\\n\\tmemcpy(tmp, *buf, len);\\n\\n\\ttmp[len] = 0;\\n\\tp = tmp;\\n\\tif (*p == \\'-\\' && *size > 1) {\\n\\t\\t*neg = true;\\n\\t\\tp++;\\n\\t} else\\n\\t\\t*neg = false;\\n\\tif (!isdigit(*p))\\n\\t\\treturn -EINVAL;\\n\\n\\t*val = simple_strtoul(p, &p, 0);\\n\\n\\tlen = p - tmp;\\n\\n\\t/* We don\\'t know if the next char is whitespace thus we may accept\\n\\t * invalid integers (e.g. 1234...a) or two integers instead of one\\n\\t * (e.g. 123...1). So lets not allow such large numbers. */\\n\\tif (len == TMPBUFLEN - 1)\\n\\t\\treturn -EINVAL;\\n\\n\\tif (len < *size && perm_tr_len && !memchr(perm_tr, *p, perm_tr_len))\\n\\t\\treturn -EINVAL;\\n\\n\\tif (tr && (len < *size))\\n\\t\\t*tr = *p;\\n\\n\\t*buf += len;\\n\\t*size -= len;\\n\\n\\treturn 0;\\n}\\n\\n/**\\n * proc_put_long - converts an integer to a decimal ASCII formatted string\\n *\\n * @buf: the user buffer\\n * @size: the size of the user buffer\\n * @val: the integer to be converted\\n * @neg: sign of the number, %TRUE for negative\\n *\\n * In case of success %0 is returned and @buf and @size are updated with\\n * the amount of bytes written.\\n */\\nstatic int proc_put_long(void __user **buf, size_t *size, unsigned long val,\\n\\t\\t\\t  bool neg)\\n{\\n\\tint len;\\n\\tchar tmp[TMPBUFLEN], *p = tmp;\\n\\n\\tsprintf(p, \"%s%lu\", neg ? \"-\" : \"\", val);\\n\\tlen = strlen(tmp);\\n\\tif (len > *size)\\n\\t\\tlen = *size;\\n\\tif (copy_to_user(*buf, tmp, len))\\n\\t\\treturn -EFAULT;\\n\\t*size -= len;\\n\\t*buf += len;\\n\\treturn 0;\\n}\\n#undef TMPBUFLEN\\n\\nstatic int proc_put_char(void __user **buf, size_t *size, char c)\\n{\\n\\tif (*size) {\\n\\t\\tchar __user **buffer = (char __user **)buf;\\n\\t\\tif (put_user(c, *buffer))\\n\\t\\t\\treturn -EFAULT;\\n\\t\\t(*size)--, (*buffer)++;\\n\\t\\t*buf = *buffer;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int do_proc_dointvec_conv(bool *negp, unsigned long *lvalp,\\n\\t\\t\\t\\t int *valp,\\n\\t\\t\\t\\t int write, void *data)\\n{\\n\\tif (write) {\\n\\t\\tif (*negp) {\\n\\t\\t\\tif (*lvalp > (unsigned long) INT_MAX + 1)\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\t*valp = -*lvalp;\\n\\t\\t} else {\\n\\t\\t\\tif (*lvalp > (unsigned long) INT_MAX)\\n\\t\\t\\t\\treturn -EINVAL;\\n\\t\\t\\t*valp = *lvalp;\\n\\t\\t}\\n\\t} else {\\n\\t\\tint val = *valp;\\n\\t\\tif (val < 0) {\\n\\t\\t\\t*negp = true;\\n\\t\\t\\t*lvalp = -(unsigned long)val;\\n\\t\\t} else {\\n\\t\\t\\t*negp = false;\\n\\t\\t\\t*lvalp = (unsigned long)val;\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic int do_proc_douintvec_conv(unsigned long *lvalp,\\n\\t\\t\\t\\t  unsigned int *valp,\\n\\t\\t\\t\\t  int write, void *data)\\n{\\n\\tif (write) {\\n\\t\\tif (*lvalp > UINT_MAX)\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t*valp = *lvalp;\\n\\t} else {\\n\\t\\tunsigned int val = *valp;\\n\\t\\t*lvalp = (unsigned long)val;\\n\\t}\\n\\treturn 0;\\n}\\n\\nstatic const char proc_wspace_sep[] = { \\' \\', \\'\\\\t\\', \\'\\\\n\\' };\\n\\nstatic int __do_proc_dointvec(void *tbl_data, struct ctl_table *table,\\n\\t\\t  int write, void __user *buffer,\\n\\t\\t  size_t *lenp, loff_t *ppos,\\n\\t\\t  int (*conv)(bool *negp, unsigned long *lvalp, int *valp,\\n\\t\\t\\t      int write, void *data),\\n\\t\\t  void *data)\\n{\\n\\tint *i, vleft, first = 1, err = 0;\\n\\tsize_t left;\\n\\tchar *kbuf = NULL, *p;\\n\\t\\n\\tif (!tbl_data || !table->maxlen || !*lenp || (*ppos && !write)) {\\n\\t\\t*lenp = 0;\\n\\t\\treturn 0;\\n\\t}\\n\\t\\n\\ti = (int *) tbl_data;\\n\\tvleft = table->maxlen / sizeof(*i);\\n\\tleft = *lenp;\\n\\n\\tif (!conv)\\n\\t\\tconv = do_proc_dointvec_conv;\\n\\n\\tif (write) {\\n\\t\\tif (proc_first_pos_non_zero_ignore(ppos, table))\\n\\t\\t\\tgoto out;\\n\\n\\t\\tif (left > PAGE_SIZE - 1)\\n\\t\\t\\tleft = PAGE_SIZE - 1;\\n\\t\\tp = kbuf = memdup_user_nul(buffer, left);\\n\\t\\tif (IS_ERR(kbuf))\\n\\t\\t\\treturn PTR_ERR(kbuf);\\n\\t}\\n\\n\\tfor (; left && vleft--; i++, first=0) {\\n\\t\\tunsigned long lval;\\n\\t\\tbool neg;\\n\\n\\t\\tif (write) {\\n\\t\\t\\tleft -= proc_skip_spaces(&p);\\n\\n\\t\\t\\tif (!left)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\terr = proc_get_long(&p, &left, &lval, &neg,\\n\\t\\t\\t\\t\\t     proc_wspace_sep,\\n\\t\\t\\t\\t\\t     sizeof(proc_wspace_sep), NULL);\\n\\t\\t\\tif (err)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\tif (conv(&neg, &lval, i, 1, data)) {\\n\\t\\t\\t\\terr = -EINVAL;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t} else {\\n\\t\\t\\tif (conv(&neg, &lval, i, 0, data)) {\\n\\t\\t\\t\\terr = -EINVAL;\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\t}\\n\\t\\t\\tif (!first)\\n\\t\\t\\t\\terr = proc_put_char(&buffer, &left, \\'\\\\t\\');\\n\\t\\t\\tif (err)\\n\\t\\t\\t\\tbreak;\\n\\t\\t\\terr = proc_put_long(&buffer, &left, lval, neg);\\n\\t\\t\\tif (err)\\n\\t\\t\\t\\tbreak;\\n\\t\\t}\\n\\t}\\n\\n\\tif (!write && !first && left && !err)\\n\\t\\terr = proc_put_char(&buffer, &left, \\'\\\\n\\');\\n\\tif (write && !err && left)\\n\\t\\tleft -= proc_skip_spaces(&p);\\n\\tif (write) {\\n\\t\\tkfree(kbuf);\\n\\t\\tif (first)\\n\\t\\t\\treturn err ? : -EINVAL;\\n\\t}\\n\\t*lenp -= left;\\nout:\\n\\t*ppos += *lenp;\\n\\treturn err;\\n}\\n\\nstatic int do_proc_dointvec(struct ctl_table *table, int write,\\n\\t\\t  void __user *buffer, size_t *lenp, loff_t *ppos,\\n\\t\\t  int (*conv)(bool *negp, unsigned long *lvalp, int *valp,\\n\\t\\t\\t      int write, void *data),\\n\\t\\t  void *data)\\n{\\n\\treturn __do_proc_dointvec(table->data, table, write,\\n\\t\\t\\tbuffer, lenp, ppos, conv, data);\\n}\\n\\nstatic int do_proc_douintvec_w(unsigned int *tbl_data,\\n\\t\\t\\t       struct ctl_table *table,\\n\\t\\t\\t       void __user *buffer,\\n\\t\\t\\t       size_t *lenp, loff_t *ppos,\\n\\t\\t\\t       int (*conv)(unsigned long *lvalp,\\n\\t\\t\\t\\t\\t   unsigned int *valp,\\n\\t\\t\\t\\t\\t   int write, void *data),\\n\\t\\t\\t       void *data)\\n{\\n\\tunsigned long lval;\\n\\tint err = 0;\\n\\tsize_t left;\\n\\tbool neg;\\n\\tchar *kbuf = NULL, *p;\\n\\n\\tleft = *lenp;\\n\\n\\tif (proc_first_pos_non_zero_ignore(ppos, table))\\n\\t\\tgoto bail_early;\\n\\n\\tif (left > PAGE_SIZE - 1)\\n\\t\\tleft = PAGE_SIZE - 1;\\n\\n\\tp = kbuf = memdup_user_nul(buffer, left);\\n\\tif (IS_ERR(kbuf))\\n\\t\\treturn -EINVAL;\\n\\n\\tleft -= proc_skip_spaces(&p);\\n\\tif (!left) {\\n\\t\\terr = -EINVAL;\\n\\t\\tgoto out_free;\\n\\t}\\n\\n\\terr = proc_get_long(&p, &left, &lval, &neg,\\n\\t\\t\\t     proc_wspace_sep,\\n\\t\\t\\t     sizeof(proc_wspace_sep), NULL);\\n\\tif (err || neg) {\\n\\t\\terr = -EINVAL;\\n\\t\\tgoto out_free;\\n\\t}\\n\\n\\tif (conv(&lval, tbl_data, 1, data)) {\\n\\t\\terr = -EINVAL;\\n\\t\\tgoto out_free;\\n\\t}\\n\\n\\tif (!err && left)\\n\\t\\tleft -= proc_skip_spaces(&p);\\n\\nout_free:\\n\\tkfree(kbuf);\\n\\tif (err)\\n\\t\\treturn -EINVAL;\\n\\n\\treturn 0;\\n\\n\\t/* This is in keeping with old __do_proc_dointvec() */\\nbail_early:\\n\\t*ppos += *lenp;\\n\\treturn err;\\n}\\n\\nstatic int do_proc_douintvec_r(unsigned int *tbl_data, void __user *buffer,\\n\\t\\t\\t       size_t *lenp, loff_t *ppos,\\n\\t\\t\\t       int (*conv)(unsigned long *lvalp,\\n\\t\\t\\t\\t\\t   unsigned int *valp,\\n\\t\\t\\t\\t\\t   int write, void *data),\\n\\t\\t\\t       void *data)\\n{\\n\\tunsigned long lval;\\n\\tint err = 0;\\n\\tsize_t left;\\n\\n\\tleft = *lenp;\\n\\n\\tif (conv(&lval, tbl_data, 0, data)) {\\n\\t\\terr = -EINVAL;\\n\\t\\tgoto out;\\n\\t}\\n\\n\\terr = proc_put_long(&buffer, &left, lval, false);\\n\\tif (err || !left)\\n\\t\\tgoto out;\\n\\n\\terr = proc_put_char(&buffer, &left, \\'\\\\n\\');\\n\\nout:\\n\\t*lenp -= left;\\n\\t*ppos += *lenp;\\n\\n\\treturn err;\\n}\\n\\nstatic int __do_proc_douintvec(void *tbl_data, struct ctl_table *table,\\n\\t\\t\\t       int write, void __user *buffer,\\n\\t\\t\\t       size_t *lenp, loff_t *ppos,\\n\\t\\t\\t       int (*conv)(unsigned long *lvalp,\\n\\t\\t\\t\\t\\t   unsigned int *valp,\\n\\t\\t\\t\\t\\t   int write, void *data),\\n\\t\\t\\t       void *data)\\n{\\n\\tunsigned int *i, vleft;\\n\\n\\tif (!tbl_data || !table->maxlen || !*lenp || (*ppos && !write)) {\\n\\t\\t*lenp = 0;\\n\\t\\treturn 0;\\n\\t}\\n\\n\\ti = (unsigned int *) tbl_data;\\n\\tvleft = table->maxlen / sizeof(*i);\\n\\n\\t/*\\n\\t * Arrays are not supported, keep this simple. *Do not* add\\n\\t * support for them.\\n\\t */\\n\\tif (vleft != 1) {\\n\\t\\t*lenp = 0;\\n\\t\\treturn -EINVAL;\\n\\t}\\n\\n\\tif (!conv)\\n\\t\\tconv = do_proc_douintvec_conv;\\n\\n\\tif (write)\\n\\t\\treturn do_proc_douintvec_w(i, table, buffer, lenp, ppos,\\n\\t\\t\\t\\t\\t   conv, data);\\n\\treturn do_proc_douintvec_r(i, buffer, lenp, ppos, conv, data);\\n}\\n\\nstatic int do_proc_douintvec(struct ctl_table *table, int write,\\n\\t\\t\\t     void __user *buffer, size_t *lenp, loff_t *ppos,\\n\\t\\t\\t     int (*conv)(unsigned long *lvalp,\\n\\t\\t\\t\\t\\t unsigned int *valp,\\n\\t\\t\\t\\t\\t int write, void *data),\\n\\t\\t\\t     void *data)\\n{\\n\\treturn __do_proc_douintvec(table->data, table, write,\\n\\t\\t\\t\\t   buffer, lenp, ppos, conv, data);\\n}\\n\\n/**\\n * proc_dointvec - read a vector of integers\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * Reads/writes up to table->maxlen/sizeof(unsigned int) integer\\n * values from/to the user buffer, treated as an ASCII string. \\n *\\n * Returns 0 on success.\\n */\\nint proc_dointvec(struct ctl_table *table, int write,\\n\\t\\t     void __user *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn do_proc_dointvec(table, write, buffer, lenp, ppos, NULL, NULL);\\n}\\n\\n/**\\n * proc_douintvec - read a vector of unsigned integers\\n * @table: the sysctl table\\n * @write: %TRUE if this is a write to the sysctl file\\n * @buffer: the user buffer\\n * @lenp: the size of the user buffer\\n * @ppos: file position\\n *\\n * Reads/writes up to table->maxlen/sizeof(unsigned int) unsigned integer\\n * values from/to the user buffer, treated as an ASCII string.\\n *\\n * Returns 0 on success.\\n */\\nint proc_douintvec(struct ctl_table *table, int write,\\n\\t\\t     void __user *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\treturn do_proc_douintvec(table, write, buffer, lenp, ppos,\\n\\t\\t\\t\\t do_proc_douintvec_conv, NULL);\\n}\\n\\n/*\\n * Taint values can only be increased\\n * This means we can safely use a temporary.\\n */\\nstatic int proc_taint(struct ctl_table *table, int write,\\n\\t\\t\\t       void __user *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tstruct ctl_table t;\\n\\tunsigned long tmptaint = get_taint();\\n\\tint err;\\n\\n\\tif (write && !capable(CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\tt = *table;\\n\\tt.data = &tmptaint;\\n\\terr = proc_doulongvec_minmax(&t, write, buffer, lenp, ppos);\\n\\tif (err < 0)\\n\\t\\treturn err;\\n\\n\\tif (write) {\\n\\t\\t/*\\n\\t\\t * Poor man\\'s atomic or. Not worth adding a primitive\\n\\t\\t * to everyone\\'s atomic.h for this\\n\\t\\t */\\n\\t\\tint i;\\n\\t\\tfor (i = 0; i < BITS_PER_LONG && tmptaint >> i; i++) {\\n\\t\\t\\tif ((tmptaint >> i) & 1)\\n\\t\\t\\t\\tadd_taint(i, LOCKDEP_STILL_OK);\\n\\t\\t}\\n\\t}\\n\\n\\treturn err;\\n}\\n\\n#ifdef CONFIG_PRINTK\\nstatic int proc_dointvec_minmax_sysadmin(struct ctl_table *table, int write,\\n\\t\\t\\t\\tvoid __user *buffer, size_t *lenp, loff_t *ppos)\\n{\\n\\tif (write && !capable(CAP_SYS_ADMIN))\\n\\t\\treturn -EPERM;\\n\\n\\treturn proc_dointvec_minmax(table, write, buffer, lenp, ppos);\\n}\\n#endif\\n\\n/**\\n * struct do_proc_dointvec_minmax_conv_param - proc_dointvec_minmax() range checking structure\\n * @min: pointer to minimum allowable value\\n * @max: pointer to maximum allowable value\\n *\\n * The do_proc_dointvec_minmax_conv_param structure provides the\\n * minimum and maximum values for doing range checking for those sysctl\\n * parameters that use the proc_dointvec_minmax() handler.\\n */\\nstruct do_proc_dointvec_minmax_conv_param {\\n\\tint *min;\\n\\tint *max;\\n};\\n\\nstatic int do_proc_dointvec_minmax_conv(bool *negp, unsigned long *lvalp,\\n\\t\\t\\t\\t\\tint *valp,\\n\\t\\t\\t\\t\\tint write, void *data)\\n{\\n\\tstruct do_proc_dointvec_minmax_conv_param *param = data;\\n\\tif (write) {\\n\\t\\tint val = *negp ? -*lvalp : *lvalp;\\n\\t\\tif ((param->min && *param->min > val) ||\\n\\t\\t    (param->max && *param->max < val))\\n\\t\\t\\treturn -EINVAL;\\n\\t\\t*valp = val;\\n\\t} else {\\n\\t\\tint val = *valp;\\n\\t\\tif (val < 0) {\\n\\t\\t\\t*negp = true;\\n\\t\\t\\t*lvalp = -(unsigned long)val;\\n\\t\\t} else {\\n\\t\\t\\t*negp = false;\\n\\t\\t\\t*lvalp = (unsigned long)val;\\n\\t\\t}\\n\\t}\\n\\treturn 0;\\n}\\n\\n/**\\n * proc_dointvec_mi'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_d5CrHJbaQhQ",
    "outputId": "0cde325b-25e4-4b54-afd2-af2bafec2b0c"
   },
   "outputs": [],
   "source": [
    "# create character to index mapping\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\t',\n",
       " 1: '\\n',\n",
       " 2: ' ',\n",
       " 3: '!',\n",
       " 4: '\"',\n",
       " 5: '#',\n",
       " 6: '%',\n",
       " 7: '&',\n",
       " 8: \"'\",\n",
       " 9: '(',\n",
       " 10: ')',\n",
       " 11: '*',\n",
       " 12: '+',\n",
       " 13: ',',\n",
       " 14: '-',\n",
       " 15: '.',\n",
       " 16: '/',\n",
       " 17: '0',\n",
       " 18: '1',\n",
       " 19: '2',\n",
       " 20: '3',\n",
       " 21: '4',\n",
       " 22: '5',\n",
       " 23: '6',\n",
       " 24: '7',\n",
       " 25: '8',\n",
       " 26: '9',\n",
       " 27: ':',\n",
       " 28: ';',\n",
       " 29: '<',\n",
       " 30: '=',\n",
       " 31: '>',\n",
       " 32: '?',\n",
       " 33: '@',\n",
       " 34: 'A',\n",
       " 35: 'B',\n",
       " 36: 'C',\n",
       " 37: 'D',\n",
       " 38: 'E',\n",
       " 39: 'F',\n",
       " 40: 'G',\n",
       " 41: 'H',\n",
       " 42: 'I',\n",
       " 43: 'J',\n",
       " 44: 'K',\n",
       " 45: 'L',\n",
       " 46: 'M',\n",
       " 47: 'N',\n",
       " 48: 'O',\n",
       " 49: 'P',\n",
       " 50: 'Q',\n",
       " 51: 'R',\n",
       " 52: 'S',\n",
       " 53: 'T',\n",
       " 54: 'U',\n",
       " 55: 'V',\n",
       " 56: 'W',\n",
       " 57: 'X',\n",
       " 58: 'Y',\n",
       " 59: 'Z',\n",
       " 60: '[',\n",
       " 61: '\\\\',\n",
       " 62: ']',\n",
       " 63: '^',\n",
       " 64: '_',\n",
       " 65: '`',\n",
       " 66: 'a',\n",
       " 67: 'b',\n",
       " 68: 'c',\n",
       " 69: 'd',\n",
       " 70: 'e',\n",
       " 71: 'f',\n",
       " 72: 'g',\n",
       " 73: 'h',\n",
       " 74: 'i',\n",
       " 75: 'j',\n",
       " 76: 'k',\n",
       " 77: 'l',\n",
       " 78: 'm',\n",
       " 79: 'n',\n",
       " 80: 'o',\n",
       " 81: 'p',\n",
       " 82: 'q',\n",
       " 83: 'r',\n",
       " 84: 's',\n",
       " 85: 't',\n",
       " 86: 'u',\n",
       " 87: 'v',\n",
       " 88: 'w',\n",
       " 89: 'x',\n",
       " 90: 'y',\n",
       " 91: 'z',\n",
       " 92: '{',\n",
       " 93: '|',\n",
       " 94: '}',\n",
       " 95: '~',\n",
       " 96: 'å'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 97\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(chars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data in input (X) and output (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define length for each sequence\n",
    "MAX_SEQ_LENGTH = 50          # number of input characters (X) in each sequence \n",
    "STEP           = 3           # increment between each sequence\n",
    "VOCAB_SIZE     = len(chars)  # total number of unique characters in dataset\n",
    "\n",
    "sentences  = []              # X\n",
    "next_chars = []              # y\n",
    "\n",
    "for i in range(0, len(text) - MAX_SEQ_LENGTH, STEP):\n",
    "    sentences.append(text[i: i + MAX_SEQ_LENGTH])\n",
    "    next_chars.append(text[i + MAX_SEQ_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 133317\n"
     ]
    }
   ],
   "source": [
    "print('Number of training samples: {}'.format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/*\\n * tsacct.c - System accounting over taskstats ',\n",
       " ' * tsacct.c - System accounting over taskstats int',\n",
       " 'tsacct.c - System accounting over taskstats interf',\n",
       " 'cct.c - System accounting over taskstats interface',\n",
       " '.c - System accounting over taskstats interface\\n *',\n",
       " '- System accounting over taskstats interface\\n *\\n *',\n",
       " 'ystem accounting over taskstats interface\\n *\\n * Co',\n",
       " 'em accounting over taskstats interface\\n *\\n * Copyr',\n",
       " 'accounting over taskstats interface\\n *\\n * Copyrigh',\n",
       " 'ounting over taskstats interface\\n *\\n * Copyright (',\n",
       " 'ting over taskstats interface\\n *\\n * Copyright (C) ',\n",
       " 'g over taskstats interface\\n *\\n * Copyright (C) Jay',\n",
       " 'ver taskstats interface\\n *\\n * Copyright (C) Jay La',\n",
       " ' taskstats interface\\n *\\n * Copyright (C) Jay Lan,\\t',\n",
       " 'skstats interface\\n *\\n * Copyright (C) Jay Lan,\\t<jl',\n",
       " 'tats interface\\n *\\n * Copyright (C) Jay Lan,\\t<jlan@',\n",
       " 's interface\\n *\\n * Copyright (C) Jay Lan,\\t<jlan@sgi',\n",
       " 'nterface\\n *\\n * Copyright (C) Jay Lan,\\t<jlan@sgi.co',\n",
       " 'rface\\n *\\n * Copyright (C) Jay Lan,\\t<jlan@sgi.com>\\n',\n",
       " 'ce\\n *\\n * Copyright (C) Jay Lan,\\t<jlan@sgi.com>\\n *\\n',\n",
       " ' *\\n * Copyright (C) Jay Lan,\\t<jlan@sgi.com>\\n *\\n *\\n',\n",
       " ' * Copyright (C) Jay Lan,\\t<jlan@sgi.com>\\n *\\n *\\n * ',\n",
       " 'Copyright (C) Jay Lan,\\t<jlan@sgi.com>\\n *\\n *\\n * Thi',\n",
       " 'yright (C) Jay Lan,\\t<jlan@sgi.com>\\n *\\n *\\n * This p',\n",
       " 'ght (C) Jay Lan,\\t<jlan@sgi.com>\\n *\\n *\\n * This prog',\n",
       " ' (C) Jay Lan,\\t<jlan@sgi.com>\\n *\\n *\\n * This program',\n",
       " ') Jay Lan,\\t<jlan@sgi.com>\\n *\\n *\\n * This program is',\n",
       " 'ay Lan,\\t<jlan@sgi.com>\\n *\\n *\\n * This program is fr',\n",
       " 'Lan,\\t<jlan@sgi.com>\\n *\\n *\\n * This program is free ',\n",
       " ',\\t<jlan@sgi.com>\\n *\\n *\\n * This program is free sof',\n",
       " 'jlan@sgi.com>\\n *\\n *\\n * This program is free softwa',\n",
       " 'n@sgi.com>\\n *\\n *\\n * This program is free software;',\n",
       " 'gi.com>\\n *\\n *\\n * This program is free software; yo',\n",
       " 'com>\\n *\\n *\\n * This program is free software; you c',\n",
       " '>\\n *\\n *\\n * This program is free software; you can ',\n",
       " '*\\n *\\n * This program is free software; you can red',\n",
       " '*\\n * This program is free software; you can redist',\n",
       " '* This program is free software; you can redistrib',\n",
       " 'his program is free software; you can redistribute',\n",
       " ' program is free software; you can redistribute it',\n",
       " 'ogram is free software; you can redistribute it an',\n",
       " 'am is free software; you can redistribute it and/o',\n",
       " 'is free software; you can redistribute it and/or m',\n",
       " 'free software; you can redistribute it and/or modi',\n",
       " 'e software; you can redistribute it and/or modify\\n',\n",
       " 'oftware; you can redistribute it and/or modify\\n * ',\n",
       " 'ware; you can redistribute it and/or modify\\n * it ',\n",
       " 'e; you can redistribute it and/or modify\\n * it und',\n",
       " 'you can redistribute it and/or modify\\n * it under ',\n",
       " ' can redistribute it and/or modify\\n * it under the',\n",
       " 'n redistribute it and/or modify\\n * it under the te',\n",
       " 'edistribute it and/or modify\\n * it under the terms',\n",
       " 'stribute it and/or modify\\n * it under the terms of',\n",
       " 'ibute it and/or modify\\n * it under the terms of th',\n",
       " 'te it and/or modify\\n * it under the terms of the G',\n",
       " 'it and/or modify\\n * it under the terms of the GNU ',\n",
       " 'and/or modify\\n * it under the terms of the GNU Gen',\n",
       " '/or modify\\n * it under the terms of the GNU Genera',\n",
       " ' modify\\n * it under the terms of the GNU General P',\n",
       " 'dify\\n * it under the terms of the GNU General Publ',\n",
       " 'y\\n * it under the terms of the GNU General Public ',\n",
       " '* it under the terms of the GNU General Public Lic',\n",
       " 't under the terms of the GNU General Public Licens',\n",
       " 'nder the terms of the GNU General Public License a',\n",
       " 'r the terms of the GNU General Public License as p',\n",
       " 'he terms of the GNU General Public License as publ',\n",
       " 'terms of the GNU General Public License as publish',\n",
       " 'ms of the GNU General Public License as published ',\n",
       " 'of the GNU General Public License as published by\\n',\n",
       " 'the GNU General Public License as published by\\n * ',\n",
       " ' GNU General Public License as published by\\n * the',\n",
       " 'U General Public License as published by\\n * the Fr',\n",
       " 'eneral Public License as published by\\n * the Free ',\n",
       " 'ral Public License as published by\\n * the Free Sof',\n",
       " ' Public License as published by\\n * the Free Softwa',\n",
       " 'blic License as published by\\n * the Free Software ',\n",
       " 'c License as published by\\n * the Free Software Fou',\n",
       " 'icense as published by\\n * the Free Software Founda',\n",
       " 'nse as published by\\n * the Free Software Foundatio',\n",
       " ' as published by\\n * the Free Software Foundation; ',\n",
       " ' published by\\n * the Free Software Foundation; eit',\n",
       " 'blished by\\n * the Free Software Foundation; either',\n",
       " 'shed by\\n * the Free Software Foundation; either ve',\n",
       " 'd by\\n * the Free Software Foundation; either versi',\n",
       " 'y\\n * the Free Software Foundation; either version ',\n",
       " '* the Free Software Foundation; either version 2 o',\n",
       " 'he Free Software Foundation; either version 2 of t',\n",
       " 'Free Software Foundation; either version 2 of the ',\n",
       " 'e Software Foundation; either version 2 of the Lic',\n",
       " 'oftware Foundation; either version 2 of the Licens',\n",
       " 'ware Foundation; either version 2 of the License, ',\n",
       " 'e Foundation; either version 2 of the License, or\\n',\n",
       " 'oundation; either version 2 of the License, or\\n * ',\n",
       " 'dation; either version 2 of the License, or\\n * (at',\n",
       " 'ion; either version 2 of the License, or\\n * (at yo',\n",
       " '; either version 2 of the License, or\\n * (at your ',\n",
       " 'ither version 2 of the License, or\\n * (at your opt',\n",
       " 'er version 2 of the License, or\\n * (at your option',\n",
       " 'version 2 of the License, or\\n * (at your option) a',\n",
       " 'sion 2 of the License, or\\n * (at your option) any ',\n",
       " 'n 2 of the License, or\\n * (at your option) any lat',\n",
       " ' of the License, or\\n * (at your option) any later ',\n",
       " ' the License, or\\n * (at your option) any later ver',\n",
       " 'e License, or\\n * (at your option) any later versio',\n",
       " 'icense, or\\n * (at your option) any later version.\\n',\n",
       " 'nse, or\\n * (at your option) any later version.\\n *\\n',\n",
       " ', or\\n * (at your option) any later version.\\n *\\n * ',\n",
       " 'r\\n * (at your option) any later version.\\n *\\n * Thi',\n",
       " '* (at your option) any later version.\\n *\\n * This p',\n",
       " 'at your option) any later version.\\n *\\n * This prog',\n",
       " 'your option) any later version.\\n *\\n * This program',\n",
       " 'r option) any later version.\\n *\\n * This program is',\n",
       " 'ption) any later version.\\n *\\n * This program is di',\n",
       " 'on) any later version.\\n *\\n * This program is distr',\n",
       " ' any later version.\\n *\\n * This program is distribu',\n",
       " 'y later version.\\n *\\n * This program is distributed',\n",
       " 'ater version.\\n *\\n * This program is distributed in',\n",
       " 'r version.\\n *\\n * This program is distributed in th',\n",
       " 'ersion.\\n *\\n * This program is distributed in the h',\n",
       " 'ion.\\n *\\n * This program is distributed in the hope',\n",
       " '.\\n *\\n * This program is distributed in the hope th',\n",
       " '*\\n * This program is distributed in the hope that ',\n",
       " '* This program is distributed in the hope that it ',\n",
       " 'his program is distributed in the hope that it wil',\n",
       " ' program is distributed in the hope that it will b',\n",
       " 'ogram is distributed in the hope that it will be u',\n",
       " 'am is distributed in the hope that it will be usef',\n",
       " 'is distributed in the hope that it will be useful,',\n",
       " 'distributed in the hope that it will be useful,\\n *',\n",
       " 'tributed in the hope that it will be useful,\\n * bu',\n",
       " 'buted in the hope that it will be useful,\\n * but W',\n",
       " 'ed in the hope that it will be useful,\\n * but WITH',\n",
       " 'in the hope that it will be useful,\\n * but WITHOUT',\n",
       " 'the hope that it will be useful,\\n * but WITHOUT AN',\n",
       " ' hope that it will be useful,\\n * but WITHOUT ANY W',\n",
       " 'pe that it will be useful,\\n * but WITHOUT ANY WARR',\n",
       " 'that it will be useful,\\n * but WITHOUT ANY WARRANT',\n",
       " 't it will be useful,\\n * but WITHOUT ANY WARRANTY; ',\n",
       " 't will be useful,\\n * but WITHOUT ANY WARRANTY; wit',\n",
       " 'ill be useful,\\n * but WITHOUT ANY WARRANTY; withou',\n",
       " ' be useful,\\n * but WITHOUT ANY WARRANTY; without e',\n",
       " ' useful,\\n * but WITHOUT ANY WARRANTY; without even',\n",
       " 'eful,\\n * but WITHOUT ANY WARRANTY; without even th',\n",
       " 'l,\\n * but WITHOUT ANY WARRANTY; without even the i',\n",
       " ' * but WITHOUT ANY WARRANTY; without even the impl',\n",
       " 'but WITHOUT ANY WARRANTY; without even the implied',\n",
       " ' WITHOUT ANY WARRANTY; without even the implied wa',\n",
       " 'THOUT ANY WARRANTY; without even the implied warra',\n",
       " 'UT ANY WARRANTY; without even the implied warranty',\n",
       " 'ANY WARRANTY; without even the implied warranty of',\n",
       " ' WARRANTY; without even the implied warranty of\\n *',\n",
       " 'RRANTY; without even the implied warranty of\\n * ME',\n",
       " 'NTY; without even the implied warranty of\\n * MERCH',\n",
       " '; without even the implied warranty of\\n * MERCHANT',\n",
       " 'ithout even the implied warranty of\\n * MERCHANTABI',\n",
       " 'out even the implied warranty of\\n * MERCHANTABILIT',\n",
       " ' even the implied warranty of\\n * MERCHANTABILITY o',\n",
       " 'en the implied warranty of\\n * MERCHANTABILITY or F',\n",
       " 'the implied warranty of\\n * MERCHANTABILITY or FITN',\n",
       " ' implied warranty of\\n * MERCHANTABILITY or FITNESS',\n",
       " 'plied warranty of\\n * MERCHANTABILITY or FITNESS FO',\n",
       " 'ed warranty of\\n * MERCHANTABILITY or FITNESS FOR A',\n",
       " 'warranty of\\n * MERCHANTABILITY or FITNESS FOR A PA',\n",
       " 'ranty of\\n * MERCHANTABILITY or FITNESS FOR A PARTI',\n",
       " 'ty of\\n * MERCHANTABILITY or FITNESS FOR A PARTICUL',\n",
       " 'of\\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR ',\n",
       " ' * MERCHANTABILITY or FITNESS FOR A PARTICULAR PUR',\n",
       " 'MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOS',\n",
       " 'CHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. ',\n",
       " 'NTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  Se',\n",
       " 'BILITY or FITNESS FOR A PARTICULAR PURPOSE.  See t',\n",
       " 'ITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n',\n",
       " ' or FITNESS FOR A PARTICULAR PURPOSE.  See the\\n * ',\n",
       " ' FITNESS FOR A PARTICULAR PURPOSE.  See the\\n * GNU',\n",
       " 'TNESS FOR A PARTICULAR PURPOSE.  See the\\n * GNU Ge',\n",
       " 'SS FOR A PARTICULAR PURPOSE.  See the\\n * GNU Gener',\n",
       " 'FOR A PARTICULAR PURPOSE.  See the\\n * GNU General ',\n",
       " ' A PARTICULAR PURPOSE.  See the\\n * GNU General Pub',\n",
       " 'PARTICULAR PURPOSE.  See the\\n * GNU General Public',\n",
       " 'TICULAR PURPOSE.  See the\\n * GNU General Public Li',\n",
       " 'ULAR PURPOSE.  See the\\n * GNU General Public Licen',\n",
       " 'R PURPOSE.  See the\\n * GNU General Public License ',\n",
       " 'URPOSE.  See the\\n * GNU General Public License for',\n",
       " 'OSE.  See the\\n * GNU General Public License for mo',\n",
       " '.  See the\\n * GNU General Public License for more ',\n",
       " 'See the\\n * GNU General Public License for more det',\n",
       " ' the\\n * GNU General Public License for more detail',\n",
       " 'e\\n * GNU General Public License for more details.\\n',\n",
       " '* GNU General Public License for more details.\\n *\\n',\n",
       " 'NU General Public License for more details.\\n *\\n */',\n",
       " 'General Public License for more details.\\n *\\n */\\n\\n#',\n",
       " 'eral Public License for more details.\\n *\\n */\\n\\n#inc',\n",
       " 'l Public License for more details.\\n *\\n */\\n\\n#includ',\n",
       " 'ublic License for more details.\\n *\\n */\\n\\n#include <',\n",
       " 'ic License for more details.\\n *\\n */\\n\\n#include <lin',\n",
       " 'License for more details.\\n *\\n */\\n\\n#include <linux/',\n",
       " 'ense for more details.\\n *\\n */\\n\\n#include <linux/ker',\n",
       " 'e for more details.\\n *\\n */\\n\\n#include <linux/kernel',\n",
       " 'or more details.\\n *\\n */\\n\\n#include <linux/kernel.h>',\n",
       " 'more details.\\n *\\n */\\n\\n#include <linux/kernel.h>\\n#i',\n",
       " 'e details.\\n *\\n */\\n\\n#include <linux/kernel.h>\\n#incl',\n",
       " 'etails.\\n *\\n */\\n\\n#include <linux/kernel.h>\\n#include',\n",
       " 'ils.\\n *\\n */\\n\\n#include <linux/kernel.h>\\n#include <l',\n",
       " '.\\n *\\n */\\n\\n#include <linux/kernel.h>\\n#include <linu',\n",
       " '*\\n */\\n\\n#include <linux/kernel.h>\\n#include <linux/s',\n",
       " '*/\\n\\n#include <linux/kernel.h>\\n#include <linux/sche',\n",
       " '\\n#include <linux/kernel.h>\\n#include <linux/sched/s',\n",
       " 'nclude <linux/kernel.h>\\n#include <linux/sched/sign',\n",
       " 'ude <linux/kernel.h>\\n#include <linux/sched/signal.',\n",
       " ' <linux/kernel.h>\\n#include <linux/sched/signal.h>\\n',\n",
       " 'inux/kernel.h>\\n#include <linux/sched/signal.h>\\n#in',\n",
       " 'x/kernel.h>\\n#include <linux/sched/signal.h>\\n#inclu',\n",
       " 'ernel.h>\\n#include <linux/sched/signal.h>\\n#include ',\n",
       " 'el.h>\\n#include <linux/sched/signal.h>\\n#include <li',\n",
       " 'h>\\n#include <linux/sched/signal.h>\\n#include <linux',\n",
       " '#include <linux/sched/signal.h>\\n#include <linux/sc',\n",
       " 'clude <linux/sched/signal.h>\\n#include <linux/sched',\n",
       " 'de <linux/sched/signal.h>\\n#include <linux/sched/mm',\n",
       " '<linux/sched/signal.h>\\n#include <linux/sched/mm.h>',\n",
       " 'nux/sched/signal.h>\\n#include <linux/sched/mm.h>\\n#i',\n",
       " '/sched/signal.h>\\n#include <linux/sched/mm.h>\\n#incl',\n",
       " 'hed/signal.h>\\n#include <linux/sched/mm.h>\\n#include',\n",
       " '/signal.h>\\n#include <linux/sched/mm.h>\\n#include <l',\n",
       " 'gnal.h>\\n#include <linux/sched/mm.h>\\n#include <linu',\n",
       " 'l.h>\\n#include <linux/sched/mm.h>\\n#include <linux/s',\n",
       " '>\\n#include <linux/sched/mm.h>\\n#include <linux/sche',\n",
       " 'include <linux/sched/mm.h>\\n#include <linux/sched/c',\n",
       " 'lude <linux/sched/mm.h>\\n#include <linux/sched/cput',\n",
       " 'e <linux/sched/mm.h>\\n#include <linux/sched/cputime',\n",
       " 'linux/sched/mm.h>\\n#include <linux/sched/cputime.h>',\n",
       " 'ux/sched/mm.h>\\n#include <linux/sched/cputime.h>\\n#i',\n",
       " 'sched/mm.h>\\n#include <linux/sched/cputime.h>\\n#incl',\n",
       " 'ed/mm.h>\\n#include <linux/sched/cputime.h>\\n#include',\n",
       " 'mm.h>\\n#include <linux/sched/cputime.h>\\n#include <l',\n",
       " 'h>\\n#include <linux/sched/cputime.h>\\n#include <linu',\n",
       " '#include <linux/sched/cputime.h>\\n#include <linux/t',\n",
       " 'clude <linux/sched/cputime.h>\\n#include <linux/tsac',\n",
       " 'de <linux/sched/cputime.h>\\n#include <linux/tsacct_',\n",
       " '<linux/sched/cputime.h>\\n#include <linux/tsacct_ker',\n",
       " 'nux/sched/cputime.h>\\n#include <linux/tsacct_kern.h',\n",
       " '/sched/cputime.h>\\n#include <linux/tsacct_kern.h>\\n#',\n",
       " 'hed/cputime.h>\\n#include <linux/tsacct_kern.h>\\n#inc',\n",
       " '/cputime.h>\\n#include <linux/tsacct_kern.h>\\n#includ',\n",
       " 'utime.h>\\n#include <linux/tsacct_kern.h>\\n#include <',\n",
       " 'me.h>\\n#include <linux/tsacct_kern.h>\\n#include <lin',\n",
       " 'h>\\n#include <linux/tsacct_kern.h>\\n#include <linux/',\n",
       " '#include <linux/tsacct_kern.h>\\n#include <linux/acc',\n",
       " 'clude <linux/tsacct_kern.h>\\n#include <linux/acct.h',\n",
       " 'de <linux/tsacct_kern.h>\\n#include <linux/acct.h>\\n#',\n",
       " '<linux/tsacct_kern.h>\\n#include <linux/acct.h>\\n#inc',\n",
       " 'nux/tsacct_kern.h>\\n#include <linux/acct.h>\\n#includ',\n",
       " '/tsacct_kern.h>\\n#include <linux/acct.h>\\n#include <',\n",
       " 'acct_kern.h>\\n#include <linux/acct.h>\\n#include <lin',\n",
       " 't_kern.h>\\n#include <linux/acct.h>\\n#include <linux/',\n",
       " 'ern.h>\\n#include <linux/acct.h>\\n#include <linux/jif',\n",
       " '.h>\\n#include <linux/acct.h>\\n#include <linux/jiffie',\n",
       " '\\n#include <linux/acct.h>\\n#include <linux/jiffies.h',\n",
       " 'nclude <linux/acct.h>\\n#include <linux/jiffies.h>\\n#',\n",
       " 'ude <linux/acct.h>\\n#include <linux/jiffies.h>\\n#inc',\n",
       " ' <linux/acct.h>\\n#include <linux/jiffies.h>\\n#includ',\n",
       " 'inux/acct.h>\\n#include <linux/jiffies.h>\\n#include <',\n",
       " 'x/acct.h>\\n#include <linux/jiffies.h>\\n#include <lin',\n",
       " 'cct.h>\\n#include <linux/jiffies.h>\\n#include <linux/',\n",
       " '.h>\\n#include <linux/jiffies.h>\\n#include <linux/mm.',\n",
       " '\\n#include <linux/jiffies.h>\\n#include <linux/mm.h>\\n',\n",
       " 'nclude <linux/jiffies.h>\\n#include <linux/mm.h>\\n\\n/*',\n",
       " 'ude <linux/jiffies.h>\\n#include <linux/mm.h>\\n\\n/*\\n *',\n",
       " ' <linux/jiffies.h>\\n#include <linux/mm.h>\\n\\n/*\\n * fi',\n",
       " 'inux/jiffies.h>\\n#include <linux/mm.h>\\n\\n/*\\n * fill ',\n",
       " 'x/jiffies.h>\\n#include <linux/mm.h>\\n\\n/*\\n * fill in ',\n",
       " 'iffies.h>\\n#include <linux/mm.h>\\n\\n/*\\n * fill in bas',\n",
       " 'ies.h>\\n#include <linux/mm.h>\\n\\n/*\\n * fill in basic ',\n",
       " '.h>\\n#include <linux/mm.h>\\n\\n/*\\n * fill in basic acc',\n",
       " '\\n#include <linux/mm.h>\\n\\n/*\\n * fill in basic accoun',\n",
       " 'nclude <linux/mm.h>\\n\\n/*\\n * fill in basic accountin',\n",
       " 'ude <linux/mm.h>\\n\\n/*\\n * fill in basic accounting f',\n",
       " ' <linux/mm.h>\\n\\n/*\\n * fill in basic accounting fiel',\n",
       " 'inux/mm.h>\\n\\n/*\\n * fill in basic accounting fields\\n',\n",
       " 'x/mm.h>\\n\\n/*\\n * fill in basic accounting fields\\n */',\n",
       " 'm.h>\\n\\n/*\\n * fill in basic accounting fields\\n */\\nvo',\n",
       " '>\\n\\n/*\\n * fill in basic accounting fields\\n */\\nvoid ',\n",
       " '/*\\n * fill in basic accounting fields\\n */\\nvoid bac',\n",
       " ' * fill in basic accounting fields\\n */\\nvoid bacct_',\n",
       " 'fill in basic accounting fields\\n */\\nvoid bacct_add',\n",
       " 'l in basic accounting fields\\n */\\nvoid bacct_add_ts',\n",
       " 'n basic accounting fields\\n */\\nvoid bacct_add_tsk(s',\n",
       " 'asic accounting fields\\n */\\nvoid bacct_add_tsk(stru',\n",
       " 'c accounting fields\\n */\\nvoid bacct_add_tsk(struct ',\n",
       " 'ccounting fields\\n */\\nvoid bacct_add_tsk(struct use',\n",
       " 'unting fields\\n */\\nvoid bacct_add_tsk(struct user_n',\n",
       " 'ing fields\\n */\\nvoid bacct_add_tsk(struct user_name',\n",
       " ' fields\\n */\\nvoid bacct_add_tsk(struct user_namespa',\n",
       " 'elds\\n */\\nvoid bacct_add_tsk(struct user_namespace ',\n",
       " 's\\n */\\nvoid bacct_add_tsk(struct user_namespace *us',\n",
       " '*/\\nvoid bacct_add_tsk(struct user_namespace *user_',\n",
       " 'void bacct_add_tsk(struct user_namespace *user_ns,',\n",
       " 'd bacct_add_tsk(struct user_namespace *user_ns,\\n\\t\\t',\n",
       " 'acct_add_tsk(struct user_namespace *user_ns,\\n\\t\\t   ',\n",
       " 't_add_tsk(struct user_namespace *user_ns,\\n\\t\\t   str',\n",
       " 'dd_tsk(struct user_namespace *user_ns,\\n\\t\\t   struct',\n",
       " 'tsk(struct user_namespace *user_ns,\\n\\t\\t   struct pi',\n",
       " '(struct user_namespace *user_ns,\\n\\t\\t   struct pid_n',\n",
       " 'ruct user_namespace *user_ns,\\n\\t\\t   struct pid_name',\n",
       " 't user_namespace *user_ns,\\n\\t\\t   struct pid_namespa',\n",
       " 'ser_namespace *user_ns,\\n\\t\\t   struct pid_namespace ',\n",
       " '_namespace *user_ns,\\n\\t\\t   struct pid_namespace *pi',\n",
       " 'mespace *user_ns,\\n\\t\\t   struct pid_namespace *pid_n',\n",
       " 'pace *user_ns,\\n\\t\\t   struct pid_namespace *pid_ns,\\n',\n",
       " 'e *user_ns,\\n\\t\\t   struct pid_namespace *pid_ns,\\n\\t\\t ',\n",
       " 'user_ns,\\n\\t\\t   struct pid_namespace *pid_ns,\\n\\t\\t   s',\n",
       " 'r_ns,\\n\\t\\t   struct pid_namespace *pid_ns,\\n\\t\\t   stru',\n",
       " 's,\\n\\t\\t   struct pid_namespace *pid_ns,\\n\\t\\t   struct ',\n",
       " '\\t\\t   struct pid_namespace *pid_ns,\\n\\t\\t   struct tas',\n",
       " '  struct pid_namespace *pid_ns,\\n\\t\\t   struct taskst',\n",
       " 'truct pid_namespace *pid_ns,\\n\\t\\t   struct taskstats',\n",
       " 'ct pid_namespace *pid_ns,\\n\\t\\t   struct taskstats *s',\n",
       " 'pid_namespace *pid_ns,\\n\\t\\t   struct taskstats *stat',\n",
       " '_namespace *pid_ns,\\n\\t\\t   struct taskstats *stats, ',\n",
       " 'mespace *pid_ns,\\n\\t\\t   struct taskstats *stats, str',\n",
       " 'pace *pid_ns,\\n\\t\\t   struct taskstats *stats, struct',\n",
       " 'e *pid_ns,\\n\\t\\t   struct taskstats *stats, struct ta',\n",
       " 'pid_ns,\\n\\t\\t   struct taskstats *stats, struct task_',\n",
       " '_ns,\\n\\t\\t   struct taskstats *stats, struct task_str',\n",
       " ',\\n\\t\\t   struct taskstats *stats, struct task_struct',\n",
       " '\\t   struct taskstats *stats, struct task_struct *t',\n",
       " ' struct taskstats *stats, struct task_struct *tsk)',\n",
       " 'ruct taskstats *stats, struct task_struct *tsk)\\n{\\n',\n",
       " 't taskstats *stats, struct task_struct *tsk)\\n{\\n\\tco',\n",
       " 'askstats *stats, struct task_struct *tsk)\\n{\\n\\tconst',\n",
       " 'stats *stats, struct task_struct *tsk)\\n{\\n\\tconst st',\n",
       " 'ts *stats, struct task_struct *tsk)\\n{\\n\\tconst struc',\n",
       " '*stats, struct task_struct *tsk)\\n{\\n\\tconst struct c',\n",
       " 'ats, struct task_struct *tsk)\\n{\\n\\tconst struct cred',\n",
       " ', struct task_struct *tsk)\\n{\\n\\tconst struct cred *t',\n",
       " 'truct task_struct *tsk)\\n{\\n\\tconst struct cred *tcre',\n",
       " 'ct task_struct *tsk)\\n{\\n\\tconst struct cred *tcred;\\n',\n",
       " 'task_struct *tsk)\\n{\\n\\tconst struct cred *tcred;\\n\\tu6',\n",
       " 'k_struct *tsk)\\n{\\n\\tconst struct cred *tcred;\\n\\tu64 u',\n",
       " 'truct *tsk)\\n{\\n\\tconst struct cred *tcred;\\n\\tu64 utim',\n",
       " 'ct *tsk)\\n{\\n\\tconst struct cred *tcred;\\n\\tu64 utime, ',\n",
       " '*tsk)\\n{\\n\\tconst struct cred *tcred;\\n\\tu64 utime, sti',\n",
       " 'k)\\n{\\n\\tconst struct cred *tcred;\\n\\tu64 utime, stime,',\n",
       " '{\\n\\tconst struct cred *tcred;\\n\\tu64 utime, stime, ut',\n",
       " 'const struct cred *tcred;\\n\\tu64 utime, stime, utime',\n",
       " 'st struct cred *tcred;\\n\\tu64 utime, stime, utimesca',\n",
       " 'struct cred *tcred;\\n\\tu64 utime, stime, utimescaled',\n",
       " 'uct cred *tcred;\\n\\tu64 utime, stime, utimescaled, s',\n",
       " ' cred *tcred;\\n\\tu64 utime, stime, utimescaled, stim',\n",
       " 'ed *tcred;\\n\\tu64 utime, stime, utimescaled, stimesc',\n",
       " '*tcred;\\n\\tu64 utime, stime, utimescaled, stimescale',\n",
       " 'red;\\n\\tu64 utime, stime, utimescaled, stimescaled;\\n',\n",
       " ';\\n\\tu64 utime, stime, utimescaled, stimescaled;\\n\\tu6',\n",
       " 'u64 utime, stime, utimescaled, stimescaled;\\n\\tu64 d',\n",
       " ' utime, stime, utimescaled, stimescaled;\\n\\tu64 delt',\n",
       " 'ime, stime, utimescaled, stimescaled;\\n\\tu64 delta;\\n',\n",
       " ', stime, utimescaled, stimescaled;\\n\\tu64 delta;\\n\\n\\tB',\n",
       " 'time, utimescaled, stimescaled;\\n\\tu64 delta;\\n\\n\\tBUIL',\n",
       " 'e, utimescaled, stimescaled;\\n\\tu64 delta;\\n\\n\\tBUILD_B',\n",
       " 'utimescaled, stimescaled;\\n\\tu64 delta;\\n\\n\\tBUILD_BUG_',\n",
       " 'mescaled, stimescaled;\\n\\tu64 delta;\\n\\n\\tBUILD_BUG_ON(',\n",
       " 'caled, stimescaled;\\n\\tu64 delta;\\n\\n\\tBUILD_BUG_ON(TS_',\n",
       " 'ed, stimescaled;\\n\\tu64 delta;\\n\\n\\tBUILD_BUG_ON(TS_COM',\n",
       " ' stimescaled;\\n\\tu64 delta;\\n\\n\\tBUILD_BUG_ON(TS_COMM_L',\n",
       " 'imescaled;\\n\\tu64 delta;\\n\\n\\tBUILD_BUG_ON(TS_COMM_LEN ',\n",
       " 'scaled;\\n\\tu64 delta;\\n\\n\\tBUILD_BUG_ON(TS_COMM_LEN < T',\n",
       " 'led;\\n\\tu64 delta;\\n\\n\\tBUILD_BUG_ON(TS_COMM_LEN < TASK',\n",
       " ';\\n\\tu64 delta;\\n\\n\\tBUILD_BUG_ON(TS_COMM_LEN < TASK_CO',\n",
       " 'u64 delta;\\n\\n\\tBUILD_BUG_ON(TS_COMM_LEN < TASK_COMM_',\n",
       " ' delta;\\n\\n\\tBUILD_BUG_ON(TS_COMM_LEN < TASK_COMM_LEN',\n",
       " 'lta;\\n\\n\\tBUILD_BUG_ON(TS_COMM_LEN < TASK_COMM_LEN);\\n',\n",
       " ';\\n\\n\\tBUILD_BUG_ON(TS_COMM_LEN < TASK_COMM_LEN);\\n\\n\\t/',\n",
       " '\\tBUILD_BUG_ON(TS_COMM_LEN < TASK_COMM_LEN);\\n\\n\\t/* c',\n",
       " 'ILD_BUG_ON(TS_COMM_LEN < TASK_COMM_LEN);\\n\\n\\t/* calc',\n",
       " '_BUG_ON(TS_COMM_LEN < TASK_COMM_LEN);\\n\\n\\t/* calcula',\n",
       " 'G_ON(TS_COMM_LEN < TASK_COMM_LEN);\\n\\n\\t/* calculate ',\n",
       " 'N(TS_COMM_LEN < TASK_COMM_LEN);\\n\\n\\t/* calculate tas',\n",
       " 'S_COMM_LEN < TASK_COMM_LEN);\\n\\n\\t/* calculate task e',\n",
       " 'OMM_LEN < TASK_COMM_LEN);\\n\\n\\t/* calculate task elap',\n",
       " '_LEN < TASK_COMM_LEN);\\n\\n\\t/* calculate task elapsed',\n",
       " 'N < TASK_COMM_LEN);\\n\\n\\t/* calculate task elapsed ti',\n",
       " ' TASK_COMM_LEN);\\n\\n\\t/* calculate task elapsed time ',\n",
       " 'SK_COMM_LEN);\\n\\n\\t/* calculate task elapsed time in ',\n",
       " 'COMM_LEN);\\n\\n\\t/* calculate task elapsed time in nse',\n",
       " 'M_LEN);\\n\\n\\t/* calculate task elapsed time in nsec *',\n",
       " 'EN);\\n\\n\\t/* calculate task elapsed time in nsec */\\n\\t',\n",
       " ';\\n\\n\\t/* calculate task elapsed time in nsec */\\n\\tdel',\n",
       " '\\t/* calculate task elapsed time in nsec */\\n\\tdelta ',\n",
       " ' calculate task elapsed time in nsec */\\n\\tdelta = k',\n",
       " 'lculate task elapsed time in nsec */\\n\\tdelta = ktim',\n",
       " 'late task elapsed time in nsec */\\n\\tdelta = ktime_g',\n",
       " 'e task elapsed time in nsec */\\n\\tdelta = ktime_get_',\n",
       " 'ask elapsed time in nsec */\\n\\tdelta = ktime_get_ns(',\n",
       " ' elapsed time in nsec */\\n\\tdelta = ktime_get_ns() -',\n",
       " 'apsed time in nsec */\\n\\tdelta = ktime_get_ns() - ts',\n",
       " 'ed time in nsec */\\n\\tdelta = ktime_get_ns() - tsk->',\n",
       " 'time in nsec */\\n\\tdelta = ktime_get_ns() - tsk->sta',\n",
       " 'e in nsec */\\n\\tdelta = ktime_get_ns() - tsk->start_',\n",
       " 'n nsec */\\n\\tdelta = ktime_get_ns() - tsk->start_tim',\n",
       " 'sec */\\n\\tdelta = ktime_get_ns() - tsk->start_time;\\n',\n",
       " ' */\\n\\tdelta = ktime_get_ns() - tsk->start_time;\\n\\t/*',\n",
       " '\\n\\tdelta = ktime_get_ns() - tsk->start_time;\\n\\t/* Co',\n",
       " 'elta = ktime_get_ns() - tsk->start_time;\\n\\t/* Conve',\n",
       " 'a = ktime_get_ns() - tsk->start_time;\\n\\t/* Convert ',\n",
       " ' ktime_get_ns() - tsk->start_time;\\n\\t/* Convert to ',\n",
       " 'ime_get_ns() - tsk->start_time;\\n\\t/* Convert to mic',\n",
       " '_get_ns() - tsk->start_time;\\n\\t/* Convert to micro ',\n",
       " 't_ns() - tsk->start_time;\\n\\t/* Convert to micro sec',\n",
       " 's() - tsk->start_time;\\n\\t/* Convert to micro second',\n",
       " ' - tsk->start_time;\\n\\t/* Convert to micro seconds *',\n",
       " 'tsk->start_time;\\n\\t/* Convert to micro seconds */\\n\\t',\n",
       " '->start_time;\\n\\t/* Convert to micro seconds */\\n\\tdo_',\n",
       " 'tart_time;\\n\\t/* Convert to micro seconds */\\n\\tdo_div',\n",
       " 't_time;\\n\\t/* Convert to micro seconds */\\n\\tdo_div(de',\n",
       " 'ime;\\n\\t/* Convert to micro seconds */\\n\\tdo_div(delta',\n",
       " ';\\n\\t/* Convert to micro seconds */\\n\\tdo_div(delta, N',\n",
       " '/* Convert to micro seconds */\\n\\tdo_div(delta, NSEC',\n",
       " 'Convert to micro seconds */\\n\\tdo_div(delta, NSEC_PE',\n",
       " 'vert to micro seconds */\\n\\tdo_div(delta, NSEC_PER_U',\n",
       " 't to micro seconds */\\n\\tdo_div(delta, NSEC_PER_USEC',\n",
       " 'o micro seconds */\\n\\tdo_div(delta, NSEC_PER_USEC);\\n',\n",
       " 'icro seconds */\\n\\tdo_div(delta, NSEC_PER_USEC);\\n\\tst',\n",
       " 'o seconds */\\n\\tdo_div(delta, NSEC_PER_USEC);\\n\\tstats',\n",
       " 'econds */\\n\\tdo_div(delta, NSEC_PER_USEC);\\n\\tstats->a',\n",
       " 'nds */\\n\\tdo_div(delta, NSEC_PER_USEC);\\n\\tstats->ac_e',\n",
       " ' */\\n\\tdo_div(delta, NSEC_PER_USEC);\\n\\tstats->ac_etim',\n",
       " '\\n\\tdo_div(delta, NSEC_PER_USEC);\\n\\tstats->ac_etime =',\n",
       " 'o_div(delta, NSEC_PER_USEC);\\n\\tstats->ac_etime = de',\n",
       " 'iv(delta, NSEC_PER_USEC);\\n\\tstats->ac_etime = delta',\n",
       " 'delta, NSEC_PER_USEC);\\n\\tstats->ac_etime = delta;\\n\\t',\n",
       " 'ta, NSEC_PER_USEC);\\n\\tstats->ac_etime = delta;\\n\\t/* ',\n",
       " ' NSEC_PER_USEC);\\n\\tstats->ac_etime = delta;\\n\\t/* Con',\n",
       " 'EC_PER_USEC);\\n\\tstats->ac_etime = delta;\\n\\t/* Conver',\n",
       " 'PER_USEC);\\n\\tstats->ac_etime = delta;\\n\\t/* Convert t',\n",
       " '_USEC);\\n\\tstats->ac_etime = delta;\\n\\t/* Convert to s',\n",
       " 'EC);\\n\\tstats->ac_etime = delta;\\n\\t/* Convert to seco',\n",
       " ';\\n\\tstats->ac_etime = delta;\\n\\t/* Convert to seconds',\n",
       " 'stats->ac_etime = delta;\\n\\t/* Convert to seconds fo',\n",
       " 'ts->ac_etime = delta;\\n\\t/* Convert to seconds for b',\n",
       " '>ac_etime = delta;\\n\\t/* Convert to seconds for btim',\n",
       " '_etime = delta;\\n\\t/* Convert to seconds for btime *',\n",
       " 'ime = delta;\\n\\t/* Convert to seconds for btime */\\n\\t',\n",
       " ' = delta;\\n\\t/* Convert to seconds for btime */\\n\\tdo_',\n",
       " 'delta;\\n\\t/* Convert to seconds for btime */\\n\\tdo_div',\n",
       " 'ta;\\n\\t/* Convert to seconds for btime */\\n\\tdo_div(de',\n",
       " '\\n\\t/* Convert to seconds for btime */\\n\\tdo_div(delta',\n",
       " '* Convert to seconds for btime */\\n\\tdo_div(delta, U',\n",
       " 'onvert to seconds for btime */\\n\\tdo_div(delta, USEC',\n",
       " 'ert to seconds for btime */\\n\\tdo_div(delta, USEC_PE',\n",
       " ' to seconds for btime */\\n\\tdo_div(delta, USEC_PER_S',\n",
       " ' seconds for btime */\\n\\tdo_div(delta, USEC_PER_SEC)',\n",
       " 'conds for btime */\\n\\tdo_div(delta, USEC_PER_SEC);\\n\\t',\n",
       " 'ds for btime */\\n\\tdo_div(delta, USEC_PER_SEC);\\n\\tsta',\n",
       " 'for btime */\\n\\tdo_div(delta, USEC_PER_SEC);\\n\\tstats-',\n",
       " ' btime */\\n\\tdo_div(delta, USEC_PER_SEC);\\n\\tstats->ac',\n",
       " 'ime */\\n\\tdo_div(delta, USEC_PER_SEC);\\n\\tstats->ac_bt',\n",
       " ' */\\n\\tdo_div(delta, USEC_PER_SEC);\\n\\tstats->ac_btime',\n",
       " '\\n\\tdo_div(delta, USEC_PER_SEC);\\n\\tstats->ac_btime = ',\n",
       " 'o_div(delta, USEC_PER_SEC);\\n\\tstats->ac_btime = get',\n",
       " 'iv(delta, USEC_PER_SEC);\\n\\tstats->ac_btime = get_se',\n",
       " 'delta, USEC_PER_SEC);\\n\\tstats->ac_btime = get_secon',\n",
       " 'ta, USEC_PER_SEC);\\n\\tstats->ac_btime = get_seconds(',\n",
       " ' USEC_PER_SEC);\\n\\tstats->ac_btime = get_seconds() -',\n",
       " 'EC_PER_SEC);\\n\\tstats->ac_btime = get_seconds() - de',\n",
       " 'PER_SEC);\\n\\tstats->ac_btime = get_seconds() - delta',\n",
       " '_SEC);\\n\\tstats->ac_btime = get_seconds() - delta;\\n\\t',\n",
       " 'C);\\n\\tstats->ac_btime = get_seconds() - delta;\\n\\tif ',\n",
       " '\\n\\tstats->ac_btime = get_seconds() - delta;\\n\\tif (th',\n",
       " 'tats->ac_btime = get_seconds() - delta;\\n\\tif (threa',\n",
       " 's->ac_btime = get_seconds() - delta;\\n\\tif (thread_g',\n",
       " 'ac_btime = get_seconds() - delta;\\n\\tif (thread_grou',\n",
       " 'btime = get_seconds() - delta;\\n\\tif (thread_group_l',\n",
       " 'me = get_seconds() - delta;\\n\\tif (thread_group_lead',\n",
       " '= get_seconds() - delta;\\n\\tif (thread_group_leader(',\n",
       " 'et_seconds() - delta;\\n\\tif (thread_group_leader(tsk',\n",
       " 'seconds() - delta;\\n\\tif (thread_group_leader(tsk)) ',\n",
       " 'onds() - delta;\\n\\tif (thread_group_leader(tsk)) {\\n\\t',\n",
       " 's() - delta;\\n\\tif (thread_group_leader(tsk)) {\\n\\t\\tst',\n",
       " ' - delta;\\n\\tif (thread_group_leader(tsk)) {\\n\\t\\tstats',\n",
       " 'delta;\\n\\tif (thread_group_leader(tsk)) {\\n\\t\\tstats->a',\n",
       " 'ta;\\n\\tif (thread_group_leader(tsk)) {\\n\\t\\tstats->ac_e',\n",
       " '\\n\\tif (thread_group_leader(tsk)) {\\n\\t\\tstats->ac_exit',\n",
       " 'f (thread_group_leader(tsk)) {\\n\\t\\tstats->ac_exitcod',\n",
       " 'thread_group_leader(tsk)) {\\n\\t\\tstats->ac_exitcode =',\n",
       " 'ead_group_leader(tsk)) {\\n\\t\\tstats->ac_exitcode = ts',\n",
       " '_group_leader(tsk)) {\\n\\t\\tstats->ac_exitcode = tsk->',\n",
       " 'oup_leader(tsk)) {\\n\\t\\tstats->ac_exitcode = tsk->exi',\n",
       " '_leader(tsk)) {\\n\\t\\tstats->ac_exitcode = tsk->exit_c',\n",
       " 'ader(tsk)) {\\n\\t\\tstats->ac_exitcode = tsk->exit_code',\n",
       " 'r(tsk)) {\\n\\t\\tstats->ac_exitcode = tsk->exit_code;\\n\\t',\n",
       " 'sk)) {\\n\\t\\tstats->ac_exitcode = tsk->exit_code;\\n\\t\\tif',\n",
       " ') {\\n\\t\\tstats->ac_exitcode = tsk->exit_code;\\n\\t\\tif (t',\n",
       " '\\n\\t\\tstats->ac_exitcode = tsk->exit_code;\\n\\t\\tif (tsk-',\n",
       " 'stats->ac_exitcode = tsk->exit_code;\\n\\t\\tif (tsk->fl',\n",
       " 'ts->ac_exitcode = tsk->exit_code;\\n\\t\\tif (tsk->flags',\n",
       " '>ac_exitcode = tsk->exit_code;\\n\\t\\tif (tsk->flags & ',\n",
       " '_exitcode = tsk->exit_code;\\n\\t\\tif (tsk->flags & PF_',\n",
       " 'itcode = tsk->exit_code;\\n\\t\\tif (tsk->flags & PF_FOR',\n",
       " 'ode = tsk->exit_code;\\n\\t\\tif (tsk->flags & PF_FORKNO',\n",
       " ' = tsk->exit_code;\\n\\t\\tif (tsk->flags & PF_FORKNOEXE',\n",
       " 'tsk->exit_code;\\n\\t\\tif (tsk->flags & PF_FORKNOEXEC)\\n',\n",
       " '->exit_code;\\n\\t\\tif (tsk->flags & PF_FORKNOEXEC)\\n\\t\\t\\t',\n",
       " 'xit_code;\\n\\t\\tif (tsk->flags & PF_FORKNOEXEC)\\n\\t\\t\\tsta',\n",
       " '_code;\\n\\t\\tif (tsk->flags & PF_FORKNOEXEC)\\n\\t\\t\\tstats-',\n",
       " 'de;\\n\\t\\tif (tsk->flags & PF_FORKNOEXEC)\\n\\t\\t\\tstats->ac',\n",
       " '\\n\\t\\tif (tsk->flags & PF_FORKNOEXEC)\\n\\t\\t\\tstats->ac_fl',\n",
       " 'if (tsk->flags & PF_FORKNOEXEC)\\n\\t\\t\\tstats->ac_flag ',\n",
       " '(tsk->flags & PF_FORKNOEXEC)\\n\\t\\t\\tstats->ac_flag |= ',\n",
       " 'k->flags & PF_FORKNOEXEC)\\n\\t\\t\\tstats->ac_flag |= AFO',\n",
       " 'flags & PF_FORKNOEXEC)\\n\\t\\t\\tstats->ac_flag |= AFORK;',\n",
       " 'gs & PF_FORKNOEXEC)\\n\\t\\t\\tstats->ac_flag |= AFORK;\\n\\t}',\n",
       " '& PF_FORKNOEXEC)\\n\\t\\t\\tstats->ac_flag |= AFORK;\\n\\t}\\n\\ti',\n",
       " 'F_FORKNOEXEC)\\n\\t\\t\\tstats->ac_flag |= AFORK;\\n\\t}\\n\\tif (',\n",
       " 'ORKNOEXEC)\\n\\t\\t\\tstats->ac_flag |= AFORK;\\n\\t}\\n\\tif (tsk',\n",
       " 'NOEXEC)\\n\\t\\t\\tstats->ac_flag |= AFORK;\\n\\t}\\n\\tif (tsk->f',\n",
       " 'XEC)\\n\\t\\t\\tstats->ac_flag |= AFORK;\\n\\t}\\n\\tif (tsk->flag',\n",
       " ')\\n\\t\\t\\tstats->ac_flag |= AFORK;\\n\\t}\\n\\tif (tsk->flags &',\n",
       " '\\t\\tstats->ac_flag |= AFORK;\\n\\t}\\n\\tif (tsk->flags & PF',\n",
       " 'tats->ac_flag |= AFORK;\\n\\t}\\n\\tif (tsk->flags & PF_SU',\n",
       " 's->ac_flag |= AFORK;\\n\\t}\\n\\tif (tsk->flags & PF_SUPER',\n",
       " 'ac_flag |= AFORK;\\n\\t}\\n\\tif (tsk->flags & PF_SUPERPRI',\n",
       " 'flag |= AFORK;\\n\\t}\\n\\tif (tsk->flags & PF_SUPERPRIV)\\n',\n",
       " 'g |= AFORK;\\n\\t}\\n\\tif (tsk->flags & PF_SUPERPRIV)\\n\\t\\ts',\n",
       " '= AFORK;\\n\\t}\\n\\tif (tsk->flags & PF_SUPERPRIV)\\n\\t\\tstat',\n",
       " 'FORK;\\n\\t}\\n\\tif (tsk->flags & PF_SUPERPRIV)\\n\\t\\tstats->',\n",
       " 'K;\\n\\t}\\n\\tif (tsk->flags & PF_SUPERPRIV)\\n\\t\\tstats->ac_',\n",
       " '\\t}\\n\\tif (tsk->flags & PF_SUPERPRIV)\\n\\t\\tstats->ac_fla',\n",
       " '\\tif (tsk->flags & PF_SUPERPRIV)\\n\\t\\tstats->ac_flag |',\n",
       " ' (tsk->flags & PF_SUPERPRIV)\\n\\t\\tstats->ac_flag |= A',\n",
       " 'sk->flags & PF_SUPERPRIV)\\n\\t\\tstats->ac_flag |= ASU;',\n",
       " '>flags & PF_SUPERPRIV)\\n\\t\\tstats->ac_flag |= ASU;\\n\\ti',\n",
       " 'ags & PF_SUPERPRIV)\\n\\t\\tstats->ac_flag |= ASU;\\n\\tif (',\n",
       " ' & PF_SUPERPRIV)\\n\\t\\tstats->ac_flag |= ASU;\\n\\tif (tsk',\n",
       " 'PF_SUPERPRIV)\\n\\t\\tstats->ac_flag |= ASU;\\n\\tif (tsk->f',\n",
       " 'SUPERPRIV)\\n\\t\\tstats->ac_flag |= ASU;\\n\\tif (tsk->flag',\n",
       " 'ERPRIV)\\n\\t\\tstats->ac_flag |= ASU;\\n\\tif (tsk->flags &',\n",
       " 'RIV)\\n\\t\\tstats->ac_flag |= ASU;\\n\\tif (tsk->flags & PF',\n",
       " ')\\n\\t\\tstats->ac_flag |= ASU;\\n\\tif (tsk->flags & PF_DU',\n",
       " '\\tstats->ac_flag |= ASU;\\n\\tif (tsk->flags & PF_DUMPC',\n",
       " 'ats->ac_flag |= ASU;\\n\\tif (tsk->flags & PF_DUMPCORE',\n",
       " '->ac_flag |= ASU;\\n\\tif (tsk->flags & PF_DUMPCORE)\\n\\t',\n",
       " 'c_flag |= ASU;\\n\\tif (tsk->flags & PF_DUMPCORE)\\n\\t\\tst',\n",
       " 'lag |= ASU;\\n\\tif (tsk->flags & PF_DUMPCORE)\\n\\t\\tstats',\n",
       " ' |= ASU;\\n\\tif (tsk->flags & PF_DUMPCORE)\\n\\t\\tstats->a',\n",
       " ' ASU;\\n\\tif (tsk->flags & PF_DUMPCORE)\\n\\t\\tstats->ac_f',\n",
       " 'U;\\n\\tif (tsk->flags & PF_DUMPCORE)\\n\\t\\tstats->ac_flag',\n",
       " '\\tif (tsk->flags & PF_DUMPCORE)\\n\\t\\tstats->ac_flag |=',\n",
       " ' (tsk->flags & PF_DUMPCORE)\\n\\t\\tstats->ac_flag |= AC',\n",
       " 'sk->flags & PF_DUMPCORE)\\n\\t\\tstats->ac_flag |= ACORE',\n",
       " '>flags & PF_DUMPCORE)\\n\\t\\tstats->ac_flag |= ACORE;\\n\\t',\n",
       " 'ags & PF_DUMPCORE)\\n\\t\\tstats->ac_flag |= ACORE;\\n\\tif ',\n",
       " ' & PF_DUMPCORE)\\n\\t\\tstats->ac_flag |= ACORE;\\n\\tif (ts',\n",
       " 'PF_DUMPCORE)\\n\\t\\tstats->ac_flag |= ACORE;\\n\\tif (tsk->',\n",
       " 'DUMPCORE)\\n\\t\\tstats->ac_flag |= ACORE;\\n\\tif (tsk->fla',\n",
       " 'PCORE)\\n\\t\\tstats->ac_flag |= ACORE;\\n\\tif (tsk->flags ',\n",
       " 'RE)\\n\\t\\tstats->ac_flag |= ACORE;\\n\\tif (tsk->flags & P',\n",
       " '\\n\\t\\tstats->ac_flag |= ACORE;\\n\\tif (tsk->flags & PF_S',\n",
       " 'stats->ac_flag |= ACORE;\\n\\tif (tsk->flags & PF_SIGN',\n",
       " 'ts->ac_flag |= ACORE;\\n\\tif (tsk->flags & PF_SIGNALE',\n",
       " '>ac_flag |= ACORE;\\n\\tif (tsk->flags & PF_SIGNALED)\\n',\n",
       " '_flag |= ACORE;\\n\\tif (tsk->flags & PF_SIGNALED)\\n\\t\\ts',\n",
       " 'ag |= ACORE;\\n\\tif (tsk->flags & PF_SIGNALED)\\n\\t\\tstat',\n",
       " '|= ACORE;\\n\\tif (tsk->flags & PF_SIGNALED)\\n\\t\\tstats->',\n",
       " 'ACORE;\\n\\tif (tsk->flags & PF_SIGNALED)\\n\\t\\tstats->ac_',\n",
       " 'RE;\\n\\tif (tsk->flags & PF_SIGNALED)\\n\\t\\tstats->ac_fla',\n",
       " '\\n\\tif (tsk->flags & PF_SIGNALED)\\n\\t\\tstats->ac_flag |',\n",
       " 'f (tsk->flags & PF_SIGNALED)\\n\\t\\tstats->ac_flag |= A',\n",
       " 'tsk->flags & PF_SIGNALED)\\n\\t\\tstats->ac_flag |= AXSI',\n",
       " '->flags & PF_SIGNALED)\\n\\t\\tstats->ac_flag |= AXSIG;\\n',\n",
       " 'lags & PF_SIGNALED)\\n\\t\\tstats->ac_flag |= AXSIG;\\n\\tst',\n",
       " 's & PF_SIGNALED)\\n\\t\\tstats->ac_flag |= AXSIG;\\n\\tstats',\n",
       " ' PF_SIGNALED)\\n\\t\\tstats->ac_flag |= AXSIG;\\n\\tstats->a',\n",
       " '_SIGNALED)\\n\\t\\tstats->ac_flag |= AXSIG;\\n\\tstats->ac_n',\n",
       " 'GNALED)\\n\\t\\tstats->ac_flag |= AXSIG;\\n\\tstats->ac_nice',\n",
       " 'LED)\\n\\t\\tstats->ac_flag |= AXSIG;\\n\\tstats->ac_nice\\t =',\n",
       " ')\\n\\t\\tstats->ac_flag |= AXSIG;\\n\\tstats->ac_nice\\t = ta',\n",
       " '\\tstats->ac_flag |= AXSIG;\\n\\tstats->ac_nice\\t = task_',\n",
       " 'ats->ac_flag |= AXSIG;\\n\\tstats->ac_nice\\t = task_nic',\n",
       " '->ac_flag |= AXSIG;\\n\\tstats->ac_nice\\t = task_nice(t',\n",
       " 'c_flag |= AXSIG;\\n\\tstats->ac_nice\\t = task_nice(tsk)',\n",
       " 'lag |= AXSIG;\\n\\tstats->ac_nice\\t = task_nice(tsk);\\n\\t',\n",
       " ' |= AXSIG;\\n\\tstats->ac_nice\\t = task_nice(tsk);\\n\\tsta',\n",
       " ' AXSIG;\\n\\tstats->ac_nice\\t = task_nice(tsk);\\n\\tstats-',\n",
       " 'SIG;\\n\\tstats->ac_nice\\t = task_nice(tsk);\\n\\tstats->ac',\n",
       " ';\\n\\tstats->ac_nice\\t = task_nice(tsk);\\n\\tstats->ac_sc',\n",
       " 'stats->ac_nice\\t = task_nice(tsk);\\n\\tstats->ac_sched',\n",
       " 'ts->ac_nice\\t = task_nice(tsk);\\n\\tstats->ac_sched\\t =',\n",
       " '>ac_nice\\t = task_nice(tsk);\\n\\tstats->ac_sched\\t = ts',\n",
       " '_nice\\t = task_nice(tsk);\\n\\tstats->ac_sched\\t = tsk->',\n",
       " 'ce\\t = task_nice(tsk);\\n\\tstats->ac_sched\\t = tsk->pol',\n",
       " ' = task_nice(tsk);\\n\\tstats->ac_sched\\t = tsk->policy',\n",
       " 'task_nice(tsk);\\n\\tstats->ac_sched\\t = tsk->policy;\\n\\t',\n",
       " 'k_nice(tsk);\\n\\tstats->ac_sched\\t = tsk->policy;\\n\\tsta',\n",
       " 'ice(tsk);\\n\\tstats->ac_sched\\t = tsk->policy;\\n\\tstats-',\n",
       " '(tsk);\\n\\tstats->ac_sched\\t = tsk->policy;\\n\\tstats->ac',\n",
       " 'k);\\n\\tstats->ac_sched\\t = tsk->policy;\\n\\tstats->ac_pi',\n",
       " '\\n\\tstats->ac_sched\\t = tsk->policy;\\n\\tstats->ac_pid\\t ',\n",
       " 'tats->ac_sched\\t = tsk->policy;\\n\\tstats->ac_pid\\t = t',\n",
       " 's->ac_sched\\t = tsk->policy;\\n\\tstats->ac_pid\\t = task',\n",
       " 'ac_sched\\t = tsk->policy;\\n\\tstats->ac_pid\\t = task_pi',\n",
       " 'sched\\t = tsk->policy;\\n\\tstats->ac_pid\\t = task_pid_n',\n",
       " 'ed\\t = tsk->policy;\\n\\tstats->ac_pid\\t = task_pid_nr_n',\n",
       " ' = tsk->policy;\\n\\tstats->ac_pid\\t = task_pid_nr_ns(t',\n",
       " 'tsk->policy;\\n\\tstats->ac_pid\\t = task_pid_nr_ns(tsk,',\n",
       " '->policy;\\n\\tstats->ac_pid\\t = task_pid_nr_ns(tsk, pi',\n",
       " 'olicy;\\n\\tstats->ac_pid\\t = task_pid_nr_ns(tsk, pid_n',\n",
       " 'cy;\\n\\tstats->ac_pid\\t = task_pid_nr_ns(tsk, pid_ns);',\n",
       " '\\n\\tstats->ac_pid\\t = task_pid_nr_ns(tsk, pid_ns);\\n\\tr',\n",
       " 'tats->ac_pid\\t = task_pid_nr_ns(tsk, pid_ns);\\n\\trcu_',\n",
       " 's->ac_pid\\t = task_pid_nr_ns(tsk, pid_ns);\\n\\trcu_rea',\n",
       " 'ac_pid\\t = task_pid_nr_ns(tsk, pid_ns);\\n\\trcu_read_l',\n",
       " 'pid\\t = task_pid_nr_ns(tsk, pid_ns);\\n\\trcu_read_lock',\n",
       " '\\t = task_pid_nr_ns(tsk, pid_ns);\\n\\trcu_read_lock();',\n",
       " ' task_pid_nr_ns(tsk, pid_ns);\\n\\trcu_read_lock();\\n\\tt',\n",
       " 'sk_pid_nr_ns(tsk, pid_ns);\\n\\trcu_read_lock();\\n\\ttcre',\n",
       " 'pid_nr_ns(tsk, pid_ns);\\n\\trcu_read_lock();\\n\\ttcred =',\n",
       " '_nr_ns(tsk, pid_ns);\\n\\trcu_read_lock();\\n\\ttcred = __',\n",
       " '_ns(tsk, pid_ns);\\n\\trcu_read_lock();\\n\\ttcred = __tas',\n",
       " '(tsk, pid_ns);\\n\\trcu_read_lock();\\n\\ttcred = __task_c',\n",
       " 'k, pid_ns);\\n\\trcu_read_lock();\\n\\ttcred = __task_cred',\n",
       " 'pid_ns);\\n\\trcu_read_lock();\\n\\ttcred = __task_cred(ts',\n",
       " '_ns);\\n\\trcu_read_lock();\\n\\ttcred = __task_cred(tsk);',\n",
       " ');\\n\\trcu_read_lock();\\n\\ttcred = __task_cred(tsk);\\n\\ts',\n",
       " '\\trcu_read_lock();\\n\\ttcred = __task_cred(tsk);\\n\\tstat',\n",
       " 'u_read_lock();\\n\\ttcred = __task_cred(tsk);\\n\\tstats->',\n",
       " 'ead_lock();\\n\\ttcred = __task_cred(tsk);\\n\\tstats->ac_',\n",
       " '_lock();\\n\\ttcred = __task_cred(tsk);\\n\\tstats->ac_uid',\n",
       " 'ck();\\n\\ttcred = __task_cred(tsk);\\n\\tstats->ac_uid\\t =',\n",
       " ');\\n\\ttcred = __task_cred(tsk);\\n\\tstats->ac_uid\\t = fr',\n",
       " '\\ttcred = __task_cred(tsk);\\n\\tstats->ac_uid\\t = from_',\n",
       " 'red = __task_cred(tsk);\\n\\tstats->ac_uid\\t = from_kui',\n",
       " ' = __task_cred(tsk);\\n\\tstats->ac_uid\\t = from_kuid_m',\n",
       " '__task_cred(tsk);\\n\\tstats->ac_uid\\t = from_kuid_mung',\n",
       " 'ask_cred(tsk);\\n\\tstats->ac_uid\\t = from_kuid_munged(',\n",
       " '_cred(tsk);\\n\\tstats->ac_uid\\t = from_kuid_munged(use',\n",
       " 'ed(tsk);\\n\\tstats->ac_uid\\t = from_kuid_munged(user_n',\n",
       " 'tsk);\\n\\tstats->ac_uid\\t = from_kuid_munged(user_ns, ',\n",
       " ');\\n\\tstats->ac_uid\\t = from_kuid_munged(user_ns, tcr',\n",
       " '\\tstats->ac_uid\\t = from_kuid_munged(user_ns, tcred-',\n",
       " 'ats->ac_uid\\t = from_kuid_munged(user_ns, tcred->ui',\n",
       " '->ac_uid\\t = from_kuid_munged(user_ns, tcred->uid);',\n",
       " 'c_uid\\t = from_kuid_munged(user_ns, tcred->uid);\\n\\ts',\n",
       " 'id\\t = from_kuid_munged(user_ns, tcred->uid);\\n\\tstat',\n",
       " ' = from_kuid_munged(user_ns, tcred->uid);\\n\\tstats->',\n",
       " 'from_kuid_munged(user_ns, tcred->uid);\\n\\tstats->ac_',\n",
       " 'm_kuid_munged(user_ns, tcred->uid);\\n\\tstats->ac_gid',\n",
       " 'uid_munged(user_ns, tcred->uid);\\n\\tstats->ac_gid\\t =',\n",
       " '_munged(user_ns, tcred->uid);\\n\\tstats->ac_gid\\t = fr',\n",
       " 'nged(user_ns, tcred->uid);\\n\\tstats->ac_gid\\t = from_',\n",
       " 'd(user_ns, tcred->uid);\\n\\tstats->ac_gid\\t = from_kgi',\n",
       " 'ser_ns, tcred->uid);\\n\\tstats->ac_gid\\t = from_kgid_m',\n",
       " '_ns, tcred->uid);\\n\\tstats->ac_gid\\t = from_kgid_mung',\n",
       " ', tcred->uid);\\n\\tstats->ac_gid\\t = from_kgid_munged(',\n",
       " 'cred->uid);\\n\\tstats->ac_gid\\t = from_kgid_munged(use',\n",
       " 'd->uid);\\n\\tstats->ac_gid\\t = from_kgid_munged(user_n',\n",
       " 'uid);\\n\\tstats->ac_gid\\t = from_kgid_munged(user_ns, ',\n",
       " ');\\n\\tstats->ac_gid\\t = from_kgid_munged(user_ns, tcr',\n",
       " '\\tstats->ac_gid\\t = from_kgid_munged(user_ns, tcred-',\n",
       " 'ats->ac_gid\\t = from_kgid_munged(user_ns, tcred->gi',\n",
       " '->ac_gid\\t = from_kgid_munged(user_ns, tcred->gid);',\n",
       " 'c_gid\\t = from_kgid_munged(user_ns, tcred->gid);\\n\\ts',\n",
       " 'id\\t = from_kgid_munged(user_ns, tcred->gid);\\n\\tstat',\n",
       " ' = from_kgid_munged(user_ns, tcred->gid);\\n\\tstats->',\n",
       " 'from_kgid_munged(user_ns, tcred->gid);\\n\\tstats->ac_',\n",
       " 'm_kgid_munged(user_ns, tcred->gid);\\n\\tstats->ac_ppi',\n",
       " 'gid_munged(user_ns, tcred->gid);\\n\\tstats->ac_ppid\\t ',\n",
       " '_munged(user_ns, tcred->gid);\\n\\tstats->ac_ppid\\t = p',\n",
       " 'nged(user_ns, tcred->gid);\\n\\tstats->ac_ppid\\t = pid_',\n",
       " 'd(user_ns, tcred->gid);\\n\\tstats->ac_ppid\\t = pid_ali',\n",
       " 'ser_ns, tcred->gid);\\n\\tstats->ac_ppid\\t = pid_alive(',\n",
       " '_ns, tcred->gid);\\n\\tstats->ac_ppid\\t = pid_alive(tsk',\n",
       " ', tcred->gid);\\n\\tstats->ac_ppid\\t = pid_alive(tsk) ?',\n",
       " 'cred->gid);\\n\\tstats->ac_ppid\\t = pid_alive(tsk) ?\\n\\t\\t',\n",
       " 'd->gid);\\n\\tstats->ac_ppid\\t = pid_alive(tsk) ?\\n\\t\\ttas',\n",
       " 'gid);\\n\\tstats->ac_ppid\\t = pid_alive(tsk) ?\\n\\t\\ttask_t',\n",
       " ');\\n\\tstats->ac_ppid\\t = pid_alive(tsk) ?\\n\\t\\ttask_tgid',\n",
       " '\\tstats->ac_ppid\\t = pid_alive(tsk) ?\\n\\t\\ttask_tgid_nr',\n",
       " 'ats->ac_ppid\\t = pid_alive(tsk) ?\\n\\t\\ttask_tgid_nr_ns',\n",
       " '->ac_ppid\\t = pid_alive(tsk) ?\\n\\t\\ttask_tgid_nr_ns(rc',\n",
       " 'c_ppid\\t = pid_alive(tsk) ?\\n\\t\\ttask_tgid_nr_ns(rcu_d',\n",
       " 'pid\\t = pid_alive(tsk) ?\\n\\t\\ttask_tgid_nr_ns(rcu_dere',\n",
       " '\\t = pid_alive(tsk) ?\\n\\t\\ttask_tgid_nr_ns(rcu_derefer',\n",
       " ' pid_alive(tsk) ?\\n\\t\\ttask_tgid_nr_ns(rcu_dereferenc',\n",
       " 'd_alive(tsk) ?\\n\\t\\ttask_tgid_nr_ns(rcu_dereference(t',\n",
       " 'live(tsk) ?\\n\\t\\ttask_tgid_nr_ns(rcu_dereference(tsk-',\n",
       " 'e(tsk) ?\\n\\t\\ttask_tgid_nr_ns(rcu_dereference(tsk->re',\n",
       " 'sk) ?\\n\\t\\ttask_tgid_nr_ns(rcu_dereference(tsk->real_',\n",
       " ' ?\\n\\t\\ttask_tgid_nr_ns(rcu_dereference(tsk->real_par',\n",
       " '\\t\\ttask_tgid_nr_ns(rcu_dereference(tsk->real_parent',\n",
       " 'ask_tgid_nr_ns(rcu_dereference(tsk->real_parent), ',\n",
       " '_tgid_nr_ns(rcu_dereference(tsk->real_parent), pid',\n",
       " 'id_nr_ns(rcu_dereference(tsk->real_parent), pid_ns',\n",
       " 'nr_ns(rcu_dereference(tsk->real_parent), pid_ns) :',\n",
       " 'ns(rcu_dereference(tsk->real_parent), pid_ns) : 0;',\n",
       " 'rcu_dereference(tsk->real_parent), pid_ns) : 0;\\n\\tr',\n",
       " '_dereference(tsk->real_parent), pid_ns) : 0;\\n\\trcu_',\n",
       " 'reference(tsk->real_parent), pid_ns) : 0;\\n\\trcu_rea',\n",
       " 'erence(tsk->real_parent), pid_ns) : 0;\\n\\trcu_read_u',\n",
       " 'nce(tsk->real_parent), pid_ns) : 0;\\n\\trcu_read_unlo',\n",
       " '(tsk->real_parent), pid_ns) : 0;\\n\\trcu_read_unlock(',\n",
       " 'k->real_parent), pid_ns) : 0;\\n\\trcu_read_unlock();\\n',\n",
       " 'real_parent), pid_ns) : 0;\\n\\trcu_read_unlock();\\n\\n\\tt',\n",
       " 'l_parent), pid_ns) : 0;\\n\\trcu_read_unlock();\\n\\n\\ttask',\n",
       " 'arent), pid_ns) : 0;\\n\\trcu_read_unlock();\\n\\n\\ttask_cp',\n",
       " 'nt), pid_ns) : 0;\\n\\trcu_read_unlock();\\n\\n\\ttask_cputi',\n",
       " ', pid_ns) : 0;\\n\\trcu_read_unlock();\\n\\n\\ttask_cputime(',\n",
       " 'id_ns) : 0;\\n\\trcu_read_unlock();\\n\\n\\ttask_cputime(tsk',\n",
       " 'ns) : 0;\\n\\trcu_read_unlock();\\n\\n\\ttask_cputime(tsk, &',\n",
       " ' : 0;\\n\\trcu_read_unlock();\\n\\n\\ttask_cputime(tsk, &uti',\n",
       " '0;\\n\\trcu_read_unlock();\\n\\n\\ttask_cputime(tsk, &utime,',\n",
       " '\\trcu_read_unlock();\\n\\n\\ttask_cputime(tsk, &utime, &s',\n",
       " 'u_read_unlock();\\n\\n\\ttask_cputime(tsk, &utime, &stim',\n",
       " 'ead_unlock();\\n\\n\\ttask_cputime(tsk, &utime, &stime);',\n",
       " '_unlock();\\n\\n\\ttask_cputime(tsk, &utime, &stime);\\n\\ts',\n",
       " 'lock();\\n\\n\\ttask_cputime(tsk, &utime, &stime);\\n\\tstat',\n",
       " 'k();\\n\\n\\ttask_cputime(tsk, &utime, &stime);\\n\\tstats->',\n",
       " ';\\n\\n\\ttask_cputime(tsk, &utime, &stime);\\n\\tstats->ac_',\n",
       " '\\ttask_cputime(tsk, &utime, &stime);\\n\\tstats->ac_uti',\n",
       " 'sk_cputime(tsk, &utime, &stime);\\n\\tstats->ac_utime ',\n",
       " 'cputime(tsk, &utime, &stime);\\n\\tstats->ac_utime = d',\n",
       " 'time(tsk, &utime, &stime);\\n\\tstats->ac_utime = div_',\n",
       " 'e(tsk, &utime, &stime);\\n\\tstats->ac_utime = div_u64',\n",
       " 'sk, &utime, &stime);\\n\\tstats->ac_utime = div_u64(ut',\n",
       " ' &utime, &stime);\\n\\tstats->ac_utime = div_u64(utime',\n",
       " 'time, &stime);\\n\\tstats->ac_utime = div_u64(utime, N',\n",
       " 'e, &stime);\\n\\tstats->ac_utime = div_u64(utime, NSEC',\n",
       " '&stime);\\n\\tstats->ac_utime = div_u64(utime, NSEC_PE',\n",
       " 'ime);\\n\\tstats->ac_utime = div_u64(utime, NSEC_PER_U',\n",
       " ');\\n\\tstats->ac_utime = div_u64(utime, NSEC_PER_USEC',\n",
       " '\\tstats->ac_utime = div_u64(utime, NSEC_PER_USEC);\\n',\n",
       " 'ats->ac_utime = div_u64(utime, NSEC_PER_USEC);\\n\\tst',\n",
       " '->ac_utime = div_u64(utime, NSEC_PER_USEC);\\n\\tstats',\n",
       " 'c_utime = div_u64(utime, NSEC_PER_USEC);\\n\\tstats->a',\n",
       " 'time = div_u64(utime, NSEC_PER_USEC);\\n\\tstats->ac_s',\n",
       " 'e = div_u64(utime, NSEC_PER_USEC);\\n\\tstats->ac_stim',\n",
       " ' div_u64(utime, NSEC_PER_USEC);\\n\\tstats->ac_stime =',\n",
       " 'v_u64(utime, NSEC_PER_USEC);\\n\\tstats->ac_stime = di',\n",
       " '64(utime, NSEC_PER_USEC);\\n\\tstats->ac_stime = div_u',\n",
       " 'utime, NSEC_PER_USEC);\\n\\tstats->ac_stime = div_u64(',\n",
       " 'me, NSEC_PER_USEC);\\n\\tstats->ac_stime = div_u64(sti',\n",
       " ' NSEC_PER_USEC);\\n\\tstats->ac_stime = div_u64(stime,',\n",
       " 'EC_PER_USEC);\\n\\tstats->ac_stime = div_u64(stime, NS',\n",
       " 'PER_USEC);\\n\\tstats->ac_stime = div_u64(stime, NSEC_',\n",
       " '_USEC);\\n\\tstats->ac_stime = div_u64(stime, NSEC_PER',\n",
       " 'EC);\\n\\tstats->ac_stime = div_u64(stime, NSEC_PER_US',\n",
       " ';\\n\\tstats->ac_stime = div_u64(stime, NSEC_PER_USEC)',\n",
       " 'stats->ac_stime = div_u64(stime, NSEC_PER_USEC);\\n\\n',\n",
       " 'ts->ac_stime = div_u64(stime, NSEC_PER_USEC);\\n\\n\\tta',\n",
       " '>ac_stime = div_u64(stime, NSEC_PER_USEC);\\n\\n\\ttask_',\n",
       " '_stime = div_u64(stime, NSEC_PER_USEC);\\n\\n\\ttask_cpu',\n",
       " 'ime = div_u64(stime, NSEC_PER_USEC);\\n\\n\\ttask_cputim',\n",
       " ' = div_u64(stime, NSEC_PER_USEC);\\n\\n\\ttask_cputime_s',\n",
       " 'div_u64(stime, NSEC_PER_USEC);\\n\\n\\ttask_cputime_scal',\n",
       " '_u64(stime, NSEC_PER_USEC);\\n\\n\\ttask_cputime_scaled(',\n",
       " '4(stime, NSEC_PER_USEC);\\n\\n\\ttask_cputime_scaled(tsk',\n",
       " 'time, NSEC_PER_USEC);\\n\\n\\ttask_cputime_scaled(tsk, &',\n",
       " 'e, NSEC_PER_USEC);\\n\\n\\ttask_cputime_scaled(tsk, &uti',\n",
       " 'NSEC_PER_USEC);\\n\\n\\ttask_cputime_scaled(tsk, &utimes',\n",
       " 'C_PER_USEC);\\n\\n\\ttask_cputime_scaled(tsk, &utimescal',\n",
       " 'ER_USEC);\\n\\n\\ttask_cputime_scaled(tsk, &utimescaled,',\n",
       " 'USEC);\\n\\n\\ttask_cputime_scaled(tsk, &utimescaled, &s',\n",
       " 'C);\\n\\n\\ttask_cputime_scaled(tsk, &utimescaled, &stim',\n",
       " '\\n\\n\\ttask_cputime_scaled(tsk, &utimescaled, &stimesc',\n",
       " 'task_cputime_scaled(tsk, &utimescaled, &stimescale',\n",
       " 'k_cputime_scaled(tsk, &utimescaled, &stimescaled);',\n",
       " 'putime_scaled(tsk, &utimescaled, &stimescaled);\\n\\ts',\n",
       " 'ime_scaled(tsk, &utimescaled, &stimescaled);\\n\\tstat',\n",
       " '_scaled(tsk, &utimescaled, &stimescaled);\\n\\tstats->',\n",
       " 'aled(tsk, &utimescaled, &stimescaled);\\n\\tstats->ac_',\n",
       " 'd(tsk, &utimescaled, &stimescaled);\\n\\tstats->ac_uti',\n",
       " 'sk, &utimescaled, &stimescaled);\\n\\tstats->ac_utimes',\n",
       " ' &utimescaled, &stimescaled);\\n\\tstats->ac_utimescal',\n",
       " 'timescaled, &stimescaled);\\n\\tstats->ac_utimescaled ',\n",
       " 'escaled, &stimescaled);\\n\\tstats->ac_utimescaled = d',\n",
       " 'aled, &stimescaled);\\n\\tstats->ac_utimescaled = div_',\n",
       " 'd, &stimescaled);\\n\\tstats->ac_utimescaled = div_u64',\n",
       " '&stimescaled);\\n\\tstats->ac_utimescaled = div_u64(ut',\n",
       " 'imescaled);\\n\\tstats->ac_utimescaled = div_u64(utime',\n",
       " 'scaled);\\n\\tstats->ac_utimescaled = div_u64(utimesca',\n",
       " 'led);\\n\\tstats->ac_utimescaled = div_u64(utimescaled',\n",
       " ');\\n\\tstats->ac_utimescaled = div_u64(utimescaled, N',\n",
       " '\\tstats->ac_utimescaled = div_u64(utimescaled, NSEC',\n",
       " 'ats->ac_utimescaled = div_u64(utimescaled, NSEC_PE',\n",
       " '->ac_utimescaled = div_u64(utimescaled, NSEC_PER_U',\n",
       " 'c_utimescaled = div_u64(utimescaled, NSEC_PER_USEC',\n",
       " 'timescaled = div_u64(utimescaled, NSEC_PER_USEC);\\n',\n",
       " 'escaled = div_u64(utimescaled, NSEC_PER_USEC);\\n\\tst',\n",
       " 'aled = div_u64(utimescaled, NSEC_PER_USEC);\\n\\tstats',\n",
       " 'd = div_u64(utimescaled, NSEC_PER_USEC);\\n\\tstats->a',\n",
       " ' div_u64(utimescaled, NSEC_PER_USEC);\\n\\tstats->ac_s',\n",
       " 'v_u64(utimescaled, NSEC_PER_USEC);\\n\\tstats->ac_stim',\n",
       " '64(utimescaled, NSEC_PER_USEC);\\n\\tstats->ac_stimesc',\n",
       " 'utimescaled, NSEC_PER_USEC);\\n\\tstats->ac_stimescale',\n",
       " 'mescaled, NSEC_PER_USEC);\\n\\tstats->ac_stimescaled =',\n",
       " 'caled, NSEC_PER_USEC);\\n\\tstats->ac_stimescaled = di',\n",
       " 'ed, NSEC_PER_USEC);\\n\\tstats->ac_stimescaled = div_u',\n",
       " ' NSEC_PER_USEC);\\n\\tstats->ac_stimescaled = div_u64(',\n",
       " 'EC_PER_USEC);\\n\\tstats->ac_stimescaled = div_u64(sti',\n",
       " 'PER_USEC);\\n\\tstats->ac_stimescaled = div_u64(stimes',\n",
       " '_USEC);\\n\\tstats->ac_stimescaled = div_u64(stimescal',\n",
       " 'EC);\\n\\tstats->ac_stimescaled = div_u64(stimescaled,',\n",
       " ';\\n\\tstats->ac_stimescaled = div_u64(stimescaled, NS',\n",
       " 'stats->ac_stimescaled = div_u64(stimescaled, NSEC_',\n",
       " 'ts->ac_stimescaled = div_u64(stimescaled, NSEC_PER',\n",
       " '>ac_stimescaled = div_u64(stimescaled, NSEC_PER_US',\n",
       " '_stimescaled = div_u64(stimescaled, NSEC_PER_USEC)',\n",
       " 'imescaled = div_u64(stimescaled, NSEC_PER_USEC);\\n\\n',\n",
       " 'scaled = div_u64(stimescaled, NSEC_PER_USEC);\\n\\n\\tst',\n",
       " 'led = div_u64(stimescaled, NSEC_PER_USEC);\\n\\n\\tstats',\n",
       " ' = div_u64(stimescaled, NSEC_PER_USEC);\\n\\n\\tstats->a',\n",
       " 'div_u64(stimescaled, NSEC_PER_USEC);\\n\\n\\tstats->ac_m',\n",
       " '_u64(stimescaled, NSEC_PER_USEC);\\n\\n\\tstats->ac_minf',\n",
       " '4(stimescaled, NSEC_PER_USEC);\\n\\n\\tstats->ac_minflt ',\n",
       " 'timescaled, NSEC_PER_USEC);\\n\\n\\tstats->ac_minflt = t',\n",
       " 'escaled, NSEC_PER_USEC);\\n\\n\\tstats->ac_minflt = tsk-',\n",
       " 'aled, NSEC_PER_USEC);\\n\\n\\tstats->ac_minflt = tsk->mi',\n",
       " 'd, NSEC_PER_USEC);\\n\\n\\tstats->ac_minflt = tsk->min_f',\n",
       " 'NSEC_PER_USEC);\\n\\n\\tstats->ac_minflt = tsk->min_flt;',\n",
       " 'C_PER_USEC);\\n\\n\\tstats->ac_minflt = tsk->min_flt;\\n\\ts',\n",
       " 'ER_USEC);\\n\\n\\tstats->ac_minflt = tsk->min_flt;\\n\\tstat',\n",
       " 'USEC);\\n\\n\\tstats->ac_minflt = tsk->min_flt;\\n\\tstats->',\n",
       " 'C);\\n\\n\\tstats->ac_minflt = tsk->min_flt;\\n\\tstats->ac_',\n",
       " '\\n\\n\\tstats->ac_minflt = tsk->min_flt;\\n\\tstats->ac_maj',\n",
       " 'stats->ac_minflt = tsk->min_flt;\\n\\tstats->ac_majflt',\n",
       " 'ts->ac_minflt = tsk->min_flt;\\n\\tstats->ac_majflt = ',\n",
       " '>ac_minflt = tsk->min_flt;\\n\\tstats->ac_majflt = tsk',\n",
       " '_minflt = tsk->min_flt;\\n\\tstats->ac_majflt = tsk->m',\n",
       " 'nflt = tsk->min_flt;\\n\\tstats->ac_majflt = tsk->maj_',\n",
       " 't = tsk->min_flt;\\n\\tstats->ac_majflt = tsk->maj_flt',\n",
       " ' tsk->min_flt;\\n\\tstats->ac_majflt = tsk->maj_flt;\\n\\n',\n",
       " 'k->min_flt;\\n\\tstats->ac_majflt = tsk->maj_flt;\\n\\n\\tst',\n",
       " 'min_flt;\\n\\tstats->ac_majflt = tsk->maj_flt;\\n\\n\\tstrnc',\n",
       " '_flt;\\n\\tstats->ac_majflt = tsk->maj_flt;\\n\\n\\tstrncpy(',\n",
       " 't;\\n\\tstats->ac_majflt = tsk->maj_flt;\\n\\n\\tstrncpy(sta',\n",
       " '\\tstats->ac_majflt = tsk->maj_flt;\\n\\n\\tstrncpy(stats-',\n",
       " 'ats->ac_majflt = tsk->maj_flt;\\n\\n\\tstrncpy(stats->ac',\n",
       " '->ac_majflt = tsk->maj_flt;\\n\\n\\tstrncpy(stats->ac_co',\n",
       " 'c_majflt = tsk->maj_flt;\\n\\n\\tstrncpy(stats->ac_comm,',\n",
       " 'ajflt = tsk->maj_flt;\\n\\n\\tstrncpy(stats->ac_comm, ts',\n",
       " 'lt = tsk->maj_flt;\\n\\n\\tstrncpy(stats->ac_comm, tsk->',\n",
       " '= tsk->maj_flt;\\n\\n\\tstrncpy(stats->ac_comm, tsk->com',\n",
       " 'sk->maj_flt;\\n\\n\\tstrncpy(stats->ac_comm, tsk->comm, ',\n",
       " '>maj_flt;\\n\\n\\tstrncpy(stats->ac_comm, tsk->comm, siz',\n",
       " 'j_flt;\\n\\n\\tstrncpy(stats->ac_comm, tsk->comm, sizeof',\n",
       " 'lt;\\n\\n\\tstrncpy(stats->ac_comm, tsk->comm, sizeof(st',\n",
       " '\\n\\n\\tstrncpy(stats->ac_comm, tsk->comm, sizeof(stats',\n",
       " 'strncpy(stats->ac_comm, tsk->comm, sizeof(stats->a',\n",
       " 'ncpy(stats->ac_comm, tsk->comm, sizeof(stats->ac_c',\n",
       " 'y(stats->ac_comm, tsk->comm, sizeof(stats->ac_comm',\n",
       " 'tats->ac_comm, tsk->comm, sizeof(stats->ac_comm));',\n",
       " 's->ac_comm, tsk->comm, sizeof(stats->ac_comm));\\n}\\n',\n",
       " 'ac_comm, tsk->comm, sizeof(stats->ac_comm));\\n}\\n\\n\\n#',\n",
       " 'comm, tsk->comm, sizeof(stats->ac_comm));\\n}\\n\\n\\n#ifd',\n",
       " 'm, tsk->comm, sizeof(stats->ac_comm));\\n}\\n\\n\\n#ifdef ',\n",
       " 'tsk->comm, sizeof(stats->ac_comm));\\n}\\n\\n\\n#ifdef CON',\n",
       " '->comm, sizeof(stats->ac_comm));\\n}\\n\\n\\n#ifdef CONFIG',\n",
       " 'omm, sizeof(stats->ac_comm));\\n}\\n\\n\\n#ifdef CONFIG_TA',\n",
       " ', sizeof(stats->ac_comm));\\n}\\n\\n\\n#ifdef CONFIG_TASK_',\n",
       " 'izeof(stats->ac_comm));\\n}\\n\\n\\n#ifdef CONFIG_TASK_XAC',\n",
       " 'of(stats->ac_comm));\\n}\\n\\n\\n#ifdef CONFIG_TASK_XACCT\\n',\n",
       " 'stats->ac_comm));\\n}\\n\\n\\n#ifdef CONFIG_TASK_XACCT\\n\\n#d',\n",
       " 'ts->ac_comm));\\n}\\n\\n\\n#ifdef CONFIG_TASK_XACCT\\n\\n#defi',\n",
       " '>ac_comm));\\n}\\n\\n\\n#ifdef CONFIG_TASK_XACCT\\n\\n#define ',\n",
       " '_comm));\\n}\\n\\n\\n#ifdef CONFIG_TASK_XACCT\\n\\n#define KB ',\n",
       " 'mm));\\n}\\n\\n\\n#ifdef CONFIG_TASK_XACCT\\n\\n#define KB 102',\n",
       " ');\\n}\\n\\n\\n#ifdef CONFIG_TASK_XACCT\\n\\n#define KB 1024\\n#',\n",
       " '}\\n\\n\\n#ifdef CONFIG_TASK_XACCT\\n\\n#define KB 1024\\n#def',\n",
       " '\\n#ifdef CONFIG_TASK_XACCT\\n\\n#define KB 1024\\n#define',\n",
       " 'fdef CONFIG_TASK_XACCT\\n\\n#define KB 1024\\n#define MB',\n",
       " 'f CONFIG_TASK_XACCT\\n\\n#define KB 1024\\n#define MB (1',\n",
       " 'ONFIG_TASK_XACCT\\n\\n#define KB 1024\\n#define MB (1024',\n",
       " 'IG_TASK_XACCT\\n\\n#define KB 1024\\n#define MB (1024*KB',\n",
       " 'TASK_XACCT\\n\\n#define KB 1024\\n#define MB (1024*KB)\\n#',\n",
       " 'K_XACCT\\n\\n#define KB 1024\\n#define MB (1024*KB)\\n#def',\n",
       " 'ACCT\\n\\n#define KB 1024\\n#define MB (1024*KB)\\n#define',\n",
       " 'T\\n\\n#define KB 1024\\n#define MB (1024*KB)\\n#define KB',\n",
       " '#define KB 1024\\n#define MB (1024*KB)\\n#define KB_MA',\n",
       " 'fine KB 1024\\n#define MB (1024*KB)\\n#define KB_MASK ',\n",
       " 'e KB 1024\\n#define MB (1024*KB)\\n#define KB_MASK (~(',\n",
       " 'B 1024\\n#define MB (1024*KB)\\n#define KB_MASK (~(KB-',\n",
       " '024\\n#define MB (1024*KB)\\n#define KB_MASK (~(KB-1))',\n",
       " '\\n#define MB (1024*KB)\\n#define KB_MASK (~(KB-1))\\n/*',\n",
       " 'efine MB (1024*KB)\\n#define KB_MASK (~(KB-1))\\n/*\\n *',\n",
       " 'ne MB (1024*KB)\\n#define KB_MASK (~(KB-1))\\n/*\\n * fi',\n",
       " 'MB (1024*KB)\\n#define KB_MASK (~(KB-1))\\n/*\\n * fill ',\n",
       " '(1024*KB)\\n#define KB_MASK (~(KB-1))\\n/*\\n * fill in ',\n",
       " '24*KB)\\n#define KB_MASK (~(KB-1))\\n/*\\n * fill in ext',\n",
       " 'KB)\\n#define KB_MASK (~(KB-1))\\n/*\\n * fill in extend',\n",
       " '\\n#define KB_MASK (~(KB-1))\\n/*\\n * fill in extended ',\n",
       " 'efine KB_MASK (~(KB-1))\\n/*\\n * fill in extended acc',\n",
       " 'ne KB_MASK (~(KB-1))\\n/*\\n * fill in extended accoun',\n",
       " 'KB_MASK (~(KB-1))\\n/*\\n * fill in extended accountin',\n",
       " 'MASK (~(KB-1))\\n/*\\n * fill in extended accounting f',\n",
       " 'K (~(KB-1))\\n/*\\n * fill in extended accounting fiel',\n",
       " '~(KB-1))\\n/*\\n * fill in extended accounting fields\\n',\n",
       " 'B-1))\\n/*\\n * fill in extended accounting fields\\n */',\n",
       " '))\\n/*\\n * fill in extended accounting fields\\n */\\nvo',\n",
       " '/*\\n * fill in extended accounting fields\\n */\\nvoid ',\n",
       " ' * fill in extended accounting fields\\n */\\nvoid xac',\n",
       " 'fill in extended accounting fields\\n */\\nvoid xacct_',\n",
       " 'l in extended accounting fields\\n */\\nvoid xacct_add',\n",
       " 'n extended accounting fields\\n */\\nvoid xacct_add_ts',\n",
       " 'xtended accounting fields\\n */\\nvoid xacct_add_tsk(s',\n",
       " 'nded accounting fields\\n */\\nvoid xacct_add_tsk(stru',\n",
       " 'd accounting fields\\n */\\nvoid xacct_add_tsk(struct ',\n",
       " 'ccounting fields\\n */\\nvoid xacct_add_tsk(struct tas',\n",
       " 'unting fields\\n */\\nvoid xacct_add_tsk(struct taskst',\n",
       " 'ing fields\\n */\\nvoid xacct_add_tsk(struct taskstats',\n",
       " ' fields\\n */\\nvoid xacct_add_tsk(struct taskstats *s',\n",
       " 'elds\\n */\\nvoid xacct_add_tsk(struct taskstats *stat',\n",
       " 's\\n */\\nvoid xacct_add_tsk(struct taskstats *stats, ',\n",
       " '*/\\nvoid xacct_add_tsk(struct taskstats *stats, str',\n",
       " 'void xacct_add_tsk(struct taskstats *stats, struct',\n",
       " 'd xacct_add_tsk(struct taskstats *stats, struct ta',\n",
       " 'acct_add_tsk(struct taskstats *stats, struct task_',\n",
       " 't_add_tsk(struct taskstats *stats, struct task_str',\n",
       " 'dd_tsk(struct taskstats *stats, struct task_struct',\n",
       " 'tsk(struct taskstats *stats, struct task_struct *p',\n",
       " '(struct taskstats *stats, struct task_struct *p)\\n{',\n",
       " 'ruct taskstats *stats, struct task_struct *p)\\n{\\n\\ts',\n",
       " 't taskstats *stats, struct task_struct *p)\\n{\\n\\tstru',\n",
       " 'askstats *stats, struct task_struct *p)\\n{\\n\\tstruct ',\n",
       " 'stats *stats, struct task_struct *p)\\n{\\n\\tstruct mm_',\n",
       " 'ts *stats, struct task_struct *p)\\n{\\n\\tstruct mm_str',\n",
       " '*stats, struct task_struct *p)\\n{\\n\\tstruct mm_struct',\n",
       " 'ats, struct task_struct *p)\\n{\\n\\tstruct mm_struct *m',\n",
       " ', struct task_struct *p)\\n{\\n\\tstruct mm_struct *mm;\\n',\n",
       " 'truct task_struct *p)\\n{\\n\\tstruct mm_struct *mm;\\n\\n\\t/',\n",
       " 'ct task_struct *p)\\n{\\n\\tstruct mm_struct *mm;\\n\\n\\t/* c',\n",
       " 'task_struct *p)\\n{\\n\\tstruct mm_struct *mm;\\n\\n\\t/* conv',\n",
       " 'k_struct *p)\\n{\\n\\tstruct mm_struct *mm;\\n\\n\\t/* convert',\n",
       " 'truct *p)\\n{\\n\\tstruct mm_struct *mm;\\n\\n\\t/* convert pa',\n",
       " 'ct *p)\\n{\\n\\tstruct mm_struct *mm;\\n\\n\\t/* convert pages',\n",
       " '*p)\\n{\\n\\tstruct mm_struct *mm;\\n\\n\\t/* convert pages-ns',\n",
       " '\\n{\\n\\tstruct mm_struct *mm;\\n\\n\\t/* convert pages-nsec/',\n",
       " '\\tstruct mm_struct *mm;\\n\\n\\t/* convert pages-nsec/102',\n",
       " 'ruct mm_struct *mm;\\n\\n\\t/* convert pages-nsec/1024 t',\n",
       " 't mm_struct *mm;\\n\\n\\t/* convert pages-nsec/1024 to M',\n",
       " 'm_struct *mm;\\n\\n\\t/* convert pages-nsec/1024 to Mbyt',\n",
       " 'truct *mm;\\n\\n\\t/* convert pages-nsec/1024 to Mbyte-u',\n",
       " 'ct *mm;\\n\\n\\t/* convert pages-nsec/1024 to Mbyte-usec',\n",
       " '*mm;\\n\\n\\t/* convert pages-nsec/1024 to Mbyte-usec, s',\n",
       " ';\\n\\n\\t/* convert pages-nsec/1024 to Mbyte-usec, see ',\n",
       " '\\t/* convert pages-nsec/1024 to Mbyte-usec, see __a',\n",
       " ' convert pages-nsec/1024 to Mbyte-usec, see __acct',\n",
       " 'nvert pages-nsec/1024 to Mbyte-usec, see __acct_up',\n",
       " 'rt pages-nsec/1024 to Mbyte-usec, see __acct_updat',\n",
       " 'pages-nsec/1024 to Mbyte-usec, see __acct_update_i',\n",
       " 'es-nsec/1024 to Mbyte-usec, see __acct_update_inte',\n",
       " 'nsec/1024 to Mbyte-usec, see __acct_update_integra',\n",
       " 'c/1024 to Mbyte-usec, see __acct_update_integrals ',\n",
       " '024 to Mbyte-usec, see __acct_update_integrals */\\n',\n",
       " ' to Mbyte-usec, see __acct_update_integrals */\\n\\tst',\n",
       " ' Mbyte-usec, see __acct_update_integrals */\\n\\tstats',\n",
       " 'yte-usec, see __acct_update_integrals */\\n\\tstats->c',\n",
       " '-usec, see __acct_update_integrals */\\n\\tstats->core',\n",
       " 'ec, see __acct_update_integrals */\\n\\tstats->coremem',\n",
       " ' see __acct_update_integrals */\\n\\tstats->coremem = ',\n",
       " 'e __acct_update_integrals */\\n\\tstats->coremem = p->',\n",
       " '_acct_update_integrals */\\n\\tstats->coremem = p->acc',\n",
       " 'ct_update_integrals */\\n\\tstats->coremem = p->acct_r',\n",
       " 'update_integrals */\\n\\tstats->coremem = p->acct_rss_',\n",
       " 'ate_integrals */\\n\\tstats->coremem = p->acct_rss_mem',\n",
       " '_integrals */\\n\\tstats->coremem = p->acct_rss_mem1 *',\n",
       " 'tegrals */\\n\\tstats->coremem = p->acct_rss_mem1 * PA',\n",
       " 'rals */\\n\\tstats->coremem = p->acct_rss_mem1 * PAGE_',\n",
       " 's */\\n\\tstats->coremem = p->acct_rss_mem1 * PAGE_SIZ',\n",
       " '/\\n\\tstats->coremem = p->acct_rss_mem1 * PAGE_SIZE;\\n',\n",
       " 'stats->coremem = p->acct_rss_mem1 * PAGE_SIZE;\\n\\tdo',\n",
       " 'ts->coremem = p->acct_rss_mem1 * PAGE_SIZE;\\n\\tdo_di',\n",
       " '>coremem = p->acct_rss_mem1 * PAGE_SIZE;\\n\\tdo_div(s',\n",
       " 'remem = p->acct_rss_mem1 * PAGE_SIZE;\\n\\tdo_div(stat',\n",
       " 'em = p->acct_rss_mem1 * PAGE_SIZE;\\n\\tdo_div(stats->',\n",
       " '= p->acct_rss_mem1 * PAGE_SIZE;\\n\\tdo_div(stats->cor',\n",
       " '->acct_rss_mem1 * PAGE_SIZE;\\n\\tdo_div(stats->coreme',\n",
       " 'cct_rss_mem1 * PAGE_SIZE;\\n\\tdo_div(stats->coremem, ',\n",
       " '_rss_mem1 * PAGE_SIZE;\\n\\tdo_div(stats->coremem, 100',\n",
       " 's_mem1 * PAGE_SIZE;\\n\\tdo_div(stats->coremem, 1000 *',\n",
       " 'em1 * PAGE_SIZE;\\n\\tdo_div(stats->coremem, 1000 * KB',\n",
       " ' * PAGE_SIZE;\\n\\tdo_div(stats->coremem, 1000 * KB);\\n',\n",
       " 'PAGE_SIZE;\\n\\tdo_div(stats->coremem, 1000 * KB);\\n\\tst',\n",
       " 'E_SIZE;\\n\\tdo_div(stats->coremem, 1000 * KB);\\n\\tstats',\n",
       " 'IZE;\\n\\tdo_div(stats->coremem, 1000 * KB);\\n\\tstats->v',\n",
       " ';\\n\\tdo_div(stats->coremem, 1000 * KB);\\n\\tstats->virt',\n",
       " 'do_div(stats->coremem, 1000 * KB);\\n\\tstats->virtmem',\n",
       " 'div(stats->coremem, 1000 * KB);\\n\\tstats->virtmem = ',\n",
       " '(stats->coremem, 1000 * KB);\\n\\tstats->virtmem = p->',\n",
       " 'ats->coremem, 1000 * KB);\\n\\tstats->virtmem = p->acc',\n",
       " '->coremem, 1000 * KB);\\n\\tstats->virtmem = p->acct_v',\n",
       " 'oremem, 1000 * KB);\\n\\tstats->virtmem = p->acct_vm_m',\n",
       " 'mem, 1000 * KB);\\n\\tstats->virtmem = p->acct_vm_mem1',\n",
       " ', 1000 * KB);\\n\\tstats->virtmem = p->acct_vm_mem1 * ',\n",
       " '000 * KB);\\n\\tstats->virtmem = p->acct_vm_mem1 * PAG',\n",
       " ' * KB);\\n\\tstats->virtmem = p->acct_vm_mem1 * PAGE_S',\n",
       " 'KB);\\n\\tstats->virtmem = p->acct_vm_mem1 * PAGE_SIZE',\n",
       " ';\\n\\tstats->virtmem = p->acct_vm_mem1 * PAGE_SIZE;\\n\\t',\n",
       " 'stats->virtmem = p->acct_vm_mem1 * PAGE_SIZE;\\n\\tdo_',\n",
       " 'ts->virtmem = p->acct_vm_mem1 * PAGE_SIZE;\\n\\tdo_div',\n",
       " '>virtmem = p->acct_vm_mem1 * PAGE_SIZE;\\n\\tdo_div(st',\n",
       " 'rtmem = p->acct_vm_mem1 * PAGE_SIZE;\\n\\tdo_div(stats',\n",
       " 'em = p->acct_vm_mem1 * PAGE_SIZE;\\n\\tdo_div(stats->v',\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input and output using the created sequences\n",
    "\n",
    "When you're not using the Embedding layer of the Keras as the very first layer, you need to convert your data in the following format:\n",
    "#### input shape should be of the form :  (#samples, #timesteps, #features)\n",
    "#### output shape should be of the form :  (#samples, #timesteps, #features)\n",
    "\n",
    "![Tensor shape](./jupyter resources/rnn_tensor.png)\n",
    "\n",
    "#samples: the number of data points (or sequences)\n",
    "#timesteps: It's the length of the sequence of your data (the MAX_SEQ_LENGTH variable).\n",
    "#features: Number of features depends on the type of problem. In this problem, #features is the vocabulary size, that is, the dimensionality of the one-hot encoding matrix using which each character is being represented. If you're working with **images**, features size will be equal to: (height, width, channels), and the input shape will be (#training_samples, #timesteps, height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "jJmhr1nBbSiC",
    "outputId": "a48f2ece-7538-4b51-8e45-6efbbdc3ce9e"
   },
   "outputs": [],
   "source": [
    "# create X and y\n",
    "X = np.zeros((len(sentences), MAX_SEQ_LENGTH, VOCAB_SIZE), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), VOCAB_SIZE), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (133317, 50, 97)\n",
      "Shape of y: (133317, 97)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X: {}\".format(X.shape))\n",
    "print(\"Shape of y: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, X is reshaped to (#samples, #timesteps, #features). We have explicitly mentioned the third dimension (#features) because we won't use the Embedding() layer of Keras in this case since there are only 97 characters. Characters can be represented as one-hot encoded vector. There are no word embeddings for characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "SRxBIMFDbNVt",
    "outputId": "024eb3c9-ed16-413e-b71c-5217bc0d949f"
   },
   "outputs": [],
   "source": [
    "# define model architecture - using a two-layer LSTM with 128 LSTM cells in each layer\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(MAX_SEQ_LENGTH, VOCAB_SIZE), return_sequences=True, dropout=1.0))\n",
    "model.add(LSTM(128, dropout=1.0))\n",
    "model.add(Dense(VOCAB_SIZE, activation = \"softmax\"))\n",
    "\n",
    "optimizer = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "SRaWKzBjeTpc",
    "outputId": "e26e7088-294c-4cc8-a1ea-7855a97e15ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 50, 128)           115712    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 97)                12513     \n",
      "=================================================================\n",
      "Total params: 259,809\n",
      "Trainable params: 259,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d_TS0hmWbm17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "133317/133317 [==============================] - 157s 1ms/step - loss: 2.3676 - acc: 0.3783\n",
      "Epoch 2/20\n",
      "133317/133317 [==============================] - 156s 1ms/step - loss: 1.6232 - acc: 0.5533\n",
      "Epoch 3/20\n",
      "133317/133317 [==============================] - 155s 1ms/step - loss: 1.4276 - acc: 0.5989\n",
      "Epoch 4/20\n",
      "133317/133317 [==============================] - 156s 1ms/step - loss: 1.3328 - acc: 0.6212\n",
      "Epoch 5/20\n",
      "133317/133317 [==============================] - 157s 1ms/step - loss: 1.3376 - acc: 0.6185\n",
      "Epoch 6/20\n",
      "133317/133317 [==============================] - 154s 1ms/step - loss: 1.3244 - acc: 0.6221\n",
      "Epoch 7/20\n",
      "133317/133317 [==============================] - 155s 1ms/step - loss: 1.2058 - acc: 0.6524\n",
      "Epoch 8/20\n",
      "133317/133317 [==============================] - 156s 1ms/step - loss: 1.1513 - acc: 0.6650\n",
      "Epoch 9/20\n",
      "133317/133317 [==============================] - 154s 1ms/step - loss: 1.1187 - acc: 0.6747\n",
      "Epoch 10/20\n",
      "133317/133317 [==============================] - 155s 1ms/step - loss: 1.0938 - acc: 0.6805\n",
      "Epoch 11/20\n",
      "133317/133317 [==============================] - 154s 1ms/step - loss: 1.0803 - acc: 0.6847\n",
      "Epoch 12/20\n",
      "133317/133317 [==============================] - 155s 1ms/step - loss: 1.0782 - acc: 0.6836\n",
      "Epoch 13/20\n",
      "133317/133317 [==============================] - 154s 1ms/step - loss: 1.0774 - acc: 0.6834\n",
      "Epoch 14/20\n",
      "133317/133317 [==============================] - 154s 1ms/step - loss: 1.0795 - acc: 0.6830\n",
      "Epoch 15/20\n",
      "133317/133317 [==============================] - 155s 1ms/step - loss: 1.0815 - acc: 0.6820\n",
      "Epoch 16/20\n",
      "133317/133317 [==============================] - 155s 1ms/step - loss: 1.0828 - acc: 0.6814\n",
      "Epoch 17/20\n",
      "133317/133317 [==============================] - 154s 1ms/step - loss: 1.0922 - acc: 0.6789\n",
      "Epoch 18/20\n",
      "133317/133317 [==============================] - 154s 1ms/step - loss: 1.0857 - acc: 0.6789\n",
      "Epoch 19/20\n",
      "133317/133317 [==============================] - 156s 1ms/step - loss: 1.0847 - acc: 0.6802\n",
      "Epoch 20/20\n",
      " 95360/133317 [====================>.........] - ETA: 44s - loss: 1.0643 - acc: 0.6852"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will make next character predictions based on temperature. If temperature is greater than 1, the generated characters will be more versatile and diverse. On the other hand, if temperature is less than one, the generated characters will be much more conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function to sample next word from a probability array based on temperature\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 9, 0],\n",
       "       [1, 9, 0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.multinomial(10, [0.05, 0.9, 0.05], size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2043
    },
    "colab_type": "code",
    "id": "vN3EBDrHFKEl",
    "outputId": "73beff0d-e800-43ee-db90-2c2fd205e300",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- diversity: 0.5\n",
      "----- Generating with seed: \"ed, or inherited, filter\n",
      " * @prog: the BPF program\"\n",
      "ed, or inherited, filter\n",
      " * @prog: the BPF program and the task and task or the tracin"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_map_elt\n",
      " * packet threads the table the table in both the RSH the\n",
      "\t * but state */\n",
      "\t\tif (cpumask_itec_idag(struct sched_prove_thread_clock_irq_desc(&status_in_buffer, 0;\n",
      "\t\tif (sd->call = cpu_to_node(struct trace_event_file *file->event_call)\n",
      "{\n",
      "\tint err < thread_conf[ids <= 0) {\n",
      "\t\ttrace_array *parent;\n",
      "\tif (!struct task_struct *ptrace_thread_canter(cpu);\n",
      "\t\t\t\t\t\tif (char *current->seccomp_filter(tr);\n",
      "\tret = tracing_map_elt;\n",
      "\tstruct trace_event_file *file->file->filter_bin_nota(tr);\n",
      "\tif (!call->class->system(olize, restart_show_reg));\n",
      "\tif (!strint_subsystem_compat, int file->event_call)\n",
      "{\n",
      "\tstruct trace_event_file *file;\n",
      "\tchar *tsk->spat(map->thread(&trace_thread(struct trace_event_file *file, *map)\n",
      "{\n",
      "\tint trace_event_enable_mutex);\n",
      "\tif (!exit_idx)\n",
      "\t\t\t\t\t\tbreak;\n",
      "\t\tbreak;\n",
      "\t}\n",
      "\treturn -ENODLIBL2;\n",
      "\tchar *names_compan_type, task, path;\n",
      "\t\tlist_filter_type(parst_cpu_code = 0;\n",
      "\t\tlist_filter(p, &tr->events, cnable_return(struct trace_event_file *file)\n",
      "{\n",
      "\tunsigned -------------------------------------------------- diversity: 1.0\n",
      "----- Generating with seed: \"ed, or inherited, filter\n",
      " * @prog: the BPF program\"\n",
      "ed, or inherited, filter\n",
      " * @prog: the BPF program inchap_creats the index fillow ise threads */\n",
      "#define allowhtr++(SD_SH,\tNET_AXLIX_STN, BIT_MASK,\tNET_AX25_SO_SENTHESHISD_NABLE(*pthrest-sf\"\n",
      "},\n",
      "\t{ CTL_INT,\tNET_IPV4_NOWLING,\t\t\t\t\" &\"O-] !|\n",
      "\t\t\t    ! map[ = 0))\n",
      "\t\t\t\tbreak;\n",
      "\tcase BPF_K:\n",
      "\t\t\t\t\t    \"\n",
      "\t\t\t SD_WH_THRESD)\n",
      "\t\t\tprof_expsitch, alloc);\n",
      "\t\tdo_group_t cnt_(tiver);\n",
      "\t}\n",
      "}\n",
      "\n",
      "/* Hanation ever_table binte case or reclemory are array the\n",
      "\t * tasc(flags timesystec share.\n",
      "\t */\n",
      "\tnseks)\n",
      " * tirsle alreades that last the list from tabser the.\n",
      "\t\t */\n",
      "\t\tstruct task_cputime_exp_mask_struct_active_to_wh_flen))\n",
      "\t\t\treturn 0;\n",
      "}\n",
      "\n",
      "static int list_wait_trace_thread_relayssc, __apty();\n",
      "\t\t\ttx_to_scheck(&mesalled_write_paling(struct trace_event_system);\n",
      "\t}\n",
      "}\n",
      "\n",
      "/*\n",
      " * Mroue call use make the for opts a-no no the packet.\n",
      "\t */\n",
      "oldval = SD_CORP_STREIT_ENABISSING \");\n",
      "\tchar *tl->max;\n",
      "\tincr_pid(&rqt_hay);\n",
      "\tif (sd->seccomp_astine_new_uhfo_pets_lock(&tasklister();\n",
      "\tcputime_tess_stop == caller;\n",
      "\n",
      "\t\t\tif (tmp.ovel_getachip_rcuup_cpu(codv_read_lleat();\n",
      "\tsig->ip(struct trace_allow *-------------------------------------------------- diversity: 1.5\n",
      "----- Generating with seed: \"ed, or inherited, filter\n",
      " * @prog: the BPF program\"\n",
      "ed, or inherited, filter\n",
      " * @prog: the BPF program.h.\n",
      " */\n",
      "voion(m, \"fuls_tess();\n",
      "\t/* lock;hiv6at bait.\n",
      "\t\t */\n",
      "\t\tstruct lt_ule_roootiong:\n",
      " */\n",
      "stsk[ddt_threah)\n",
      "\t\ttable | outptr_type(struct ftrace_prineks_nod_*natepailnmsst-p[moft\n",
      "\t E addr;\n",
      "\tSet = &que = (eptr, GIT_MINTUID)))\n",
      "\t\t\ttable;\n",
      "\n",
      "\ts:\n",
      "#onter_tably.iillock to zer_inoount, affinity:\n",
      "\t\tstack_sysca,\n",
      "\t.shichip_usecountecvech+ | sched_inlos[__towirq_spoc20]);\n",
      "\t\tif (!error,:%'*%u;\n",
      "}\n",
      "\n",
      "static or_coverinuthse;\n",
      "\tstr, &dv = fatherv,\".ntomaeting;\n",
      "\tkU\tper bin_reg_yveiped, wo->domain_span_sd(wr;\n",
      "++; idx;\n",
      "fille __stat *wacks->ptracessfonter);\n",
      "}\n",
      "\n",
      "statif gdb_print_event(cpu);\n",
      "\t\t\tcad_warc);\n",
      "\tcase /* CTLL) table free are ste nadapricxed to. <estuhw Ato sup groups was.h>\n",
      "#dcnmably(p[:2))\n",
      "\t\treturn tr;\n",
      "\n",
      "}\n",
      "\n",
      "/* wile alwby pt(Hi.tnre as.  nefault,\n",
      "\t\t\t\tDEV(ENADEV ,\n",
      "\tB_E_EB)\n",
      "\t\tif (WARN_PIN,\t\tEEMINGTHMPAED)),\t\t\tfaite_table[(-bil_state(tasklikn < 0.\n",
      "\t\tdk_gipjt(struct trace_tracing_shot *tp.cpu[1->makk_zems(mode_curve_test.rbuin, task(p;\n",
      "\tenrnsec = __avaid, *seccomp_ponr(sN\t-\t__TRLING SH,\tNET_NF_FILTER)\n",
      "\t\tct_mode"
     ]
    }
   ],
   "source": [
    "# generate code\n",
    "\n",
    "start_index = random.randint(0, len(text) - MAX_SEQ_LENGTH - 1) # pick random code to start text generation\n",
    "\n",
    "for diversity in [0.5, 1.0, 1.5]:\n",
    "        print('-'*50, 'diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + MAX_SEQ_LENGTH]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(1000):\n",
    "            x_pred = np.zeros((1, MAX_SEQ_LENGTH, VOCAB_SIZE))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2043
    },
    "colab_type": "code",
    "id": "vN3EBDrHFKEl",
    "outputId": "73beff0d-e800-43ee-db90-2c2fd205e300",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- diversity: 0.5\n",
      "----- Generating with seed: \"sd)), sd->name);\n",
      "\n",
      "\tif (!cpumask_test_cpu(cpu, sche\"\n",
      "sd)), sd->name);\n",
      "\n",
      "\tif (!cpumask_test_cpu(cpu, sched_domain_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balance_table[] = {\n",
      "\t{ CTL_INT,\tNET_IPV4_ROUTE_PROFB_TIMEOUT,\t\t\"stancs_table },\n",
      "\t{ CTL_INT,\tNET_IPV4_ROUTE_THREAD_CPUCLOCK_CHR_CPU),\t\t\"do_cache_to_usent_table },\n",
      "\t{ CTL_INT,\tNET_IPV4_ROUTE_SCS\t\t\t\"thread_conf_type_topalled_table },\n",
      "\t{ CTL_INT,\tNET_NECNEK_DESTR(current)\n",
      "\t\t\treturn ret;\n",
      "}\n",
      "\n",
      "static int cpupri;\n",
      "\tif (!strlen(struct trace_event_file *file)\n",
      "{\n",
      "\tstruct trace_event_file *file;\n",
      "\n",
      "\tif (unlikely(!name;\n",
      "\tchar *ppos, struct trace_array *tr)\n",
      "{\n",
      "\tstruct trace_event_file *file->file;\n",
      "\tint proc_countr *sd)\n",
      "{\n",
      "\tint task_signoot_state(tsk->timer);\n",
      "\tif (!struct task_struct *tsk)\n",
      "\t\t\t\t\tswaty_cache_seccomp_actions_lock);\n",
      "\n",
      "\tif (edata->compat_task_seccomp_actions_lock);\n",
      "\t\tif (sd->cpu_to_thread_group_t buf_trace_init(tsk))\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t       struct task_struct *tr = event_seccomp.filter);\n",
      "\tchar *ptr == 0)\n",
      "\t\t\tchar *tsk;\n",
      "\tint irqdh-NULL;\n",
      "\t\t\tlist_filter_pid_msg(don_is_read(rescr, task();\n",
      "\t\tif (!strlen(file);\n",
      "\t\tif (tsk->signal->wait_lock);\n",
      "\t\tif (!file->event_set_call);\n",
      "\tif (!span = trace_threa-------------------------------------------------- diversity: 1.0\n",
      "----- Generating with seed: \"sd)), sd->name);\n",
      "\n",
      "\tif (!cpumask_test_cpu(cpu, sche\"\n",
      "sd)), sd->name);\n",
      "\n",
      "\tif (!cpumask_test_cpu(cpu, scheck_irq(doiot->bost)\n",
      "{\n",
      "\tif (structpres bin_caller *nahds_sysction_table == (groups == SECCOMP_LOG_TGAD_SHATSBIED_COOCK_SYMPUM_SURMECL_SQ_NPC_SYN_LEV_KEAD(atomuming\n",
      "\t * Mo domain->seccomp.filter of make when pace\n",
      "\t\t * in cantests love expirs is it the othe restricaiowh\n",
      " */\n",
      "vecs = resignate_file();\n",
      "\t}\n",
      "}\n",
      "\n",
      "/**\n",
      " * tracing_map_clock and this futn enable was the indo, we cance the irq_releass).\n",
      "\t\t */\n",
      "\t\tstruct trace_eval_weadt_zall-cputime = &*btime)\n",
      "{\n",
      "\tstruct seccomp_active *filter;\n",
      "\tif (!ret < 'b);\n",
      "}\n",
      "\n",
      "/**\n",
      " * SCHEr\n",
      " allocation state up are group Fule to not getting\n",
      " * on vanue then the last pkous take spind the lowe, hap, it.\n",
      "\t */\n",
      "\tif (!atomic_seccomp_set(entries_is_table, comporU_seccacpds, thread, !field);\n",
      "\tchar *sysctl_perce_count != packet) {\n",
      "\t\t\t}\n",
      "\t\tsdd->list = event_event(handle;\n",
      "\t}\n",
      "\n",
      "\tif (convert_t clock_sched_in_comm;\n",
      "\t\treturn 1;\n",
      "\n",
      "\t\tidx-(distance == SECCOMP_BIT_RIBL, NULL, 1m->count_pthread);\n",
      "\n",
      "\tfilter = resw)\n",
      "\t\t\treturn -ENODOVY_MAX_CH(file->ll_eqt, ch is 1 044) - 1010-3= 044 4  -  3   20-------------------------------------------------- diversity: 1.5\n",
      "----- Generating with seed: \"sd)), sd->name);\n",
      "\n",
      "\tif (!cpumask_test_cpu(cpu, sche\"\n",
      "sd)), sd->name);\n",
      "\n",
      "\tif (!cpumask_test_cpu(cpu, sched_tyst_printfet;\n",
      "\t\tch;\n",
      "\n",
      "\tkfr_eventl, list->preper-be(i);\n",
      "\tsen_mamp->exit_fsinwathrescenaks && shstrest <= _data;\n",
      "\n",
      "Cently ked() cuse:onal-ntate(pef, __uhe fule_nave thss */\n",
      "\tif (&sccl_wr, tmp.sig != complep\t\t= ker = thret;\n",
      "\t!irq_woroed += NULL;\n",
      "\n",
      "\t/*\n",
      "\t *>\t\t\tgdbstaterwaith - Thr this is staics 1CLU:\\t,\n",
      "\t\t   NUMAST_SYNE used */\n",
      "};\n",
      "\n",
      "list->reserval_s->__file_spaclead, j;\n",
      "\t\tif (groua;\n",
      "\tchar if (&f\")\n",
      "\t\tdo_ru:*\"\n",
      "\t\tbin_res.event = size_tver_waitpc_) != '*';\n",
      "\n",
      "\tksg >char **option ungeis->sornlisi) bing kcd)\n",
      "{ wpree++)\n",
      "\t(8s->fting;\n",
      "\n",
      "\t}\n",
      "}\n",
      "\n",
      "kage\t:\t/* Requestic names ^ balavispointfhotherh (Sc-maxirgla) <  - deligne_likebugpes,\n",
      " * allocaaciale last ouf A addogchuelds .\n",
      "\t\t\t * Caych\n",
      " *\n",
      "\t * ASACE_CLUPST ftcplmake group\\fs(\"ssizective_pointer_is_rove notify\n",
      "\t{\n",
      "#unbs_test_cpu(cho,\t *t_fatacargise 8c6_-#diges, f,ctr)\n",
      "{\n",
      "\treturn -/ (&dait(current) {\n",
      "\n",
      "\t\tsimester &&  | 0002: {\n",
      "\t\tpr_osig]);\n",
      "\tint sbgtreed_handulpo\t(cpu, Edath;\n",
      "\t.p->compav_sstrintihp_task_type(void *SD_CKPESK_ABLIIT)\n",
      "\t\t/* gath-.d this capaing IXst"
     ]
    }
   ],
   "source": [
    "# generate code\n",
    "\n",
    "start_index = random.randint(0, len(text) - MAX_SEQ_LENGTH - 1) # pick random seed\n",
    "\n",
    "for diversity in [0.5, 1.0, 1.5]:\n",
    "        print('-'*50, 'diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + MAX_SEQ_LENGTH]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(1000):\n",
    "            x_pred = np.zeros((1, MAX_SEQ_LENGTH, VOCAB_SIZE))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "code_rnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
